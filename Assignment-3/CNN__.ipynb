{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import os\n",
    "from sklearn import preprocessing, metrics\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Tuple\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MnistDataLoader:\n",
    "    def __init__(self, data_folder_path):\n",
    "        self.data_folder_path = data_folder_path\n",
    "        self.train_file_name = 'train-images-idx3-ubyte.gz'\n",
    "        self.train_label_file_name = 'train-labels-idx1-ubyte.gz'\n",
    "        self.test_file_name = 't10k-images-idx3-ubyte.gz'\n",
    "        self.test_label_file_name = 't10k-labels-idx1-ubyte.gz'\n",
    "        self.data = dict()\n",
    "        self.size = 28\n",
    "        self.color_channel = 1\n",
    "        self.data_list = [\n",
    "            'train_images',\n",
    "            'train_labels',\n",
    "            'test_images',\n",
    "            'test_labels'\n",
    "        ]\n",
    "\n",
    "    def load_images(self, data_list_index, file_name):\n",
    "        images = gzip.open(os.path.join(self.data_folder_path, file_name), 'rb')\n",
    "        self.data[self.data_list[data_list_index]] = np.frombuffer(images.read(), dtype=np.uint8, offset=16).reshape(-1, self.size, self.size)\n",
    "        self.data[self.data_list[data_list_index]] = self.data[self.data_list[data_list_index]].reshape(self.data[self.data_list[data_list_index]].shape[0], self.size, self.size, self.color_channel).astype(np.float32)\n",
    "\n",
    "    def load_labels(self, data_list_index, file_name):\n",
    "        labels = gzip.open(os.path.join(self.data_folder_path, file_name), 'rb')\n",
    "        self.data[self.data_list[data_list_index]] = np.frombuffer(labels.read(), dtype=np.uint8, offset=8)\n",
    "        self.data[self.data_list[data_list_index]].resize(self.data[self.data_list[data_list_index]].shape[0],1)\n",
    "\n",
    "    def load_mnist(self):\n",
    "        self.load_images(data_list_index=0, file_name=self.train_file_name)\n",
    "        self.load_labels(data_list_index=1, file_name=self.train_label_file_name)\n",
    "        self.load_images(data_list_index=2, file_name=self.test_file_name)\n",
    "        self.load_labels(data_list_index=3, file_name=self.test_label_file_name)\n",
    "\n",
    "        self.assert_data_shape()\n",
    "\n",
    "    def assert_data_shape(self):\n",
    "        assert self.data[self.data_list[0]].shape == (60000, 28, 28, 1)\n",
    "        assert self.data[self.data_list[1]].shape == (60000, 1)\n",
    "        assert self.data[self.data_list[2]].shape == (10000, 28, 28, 1)\n",
    "        assert self.data[self.data_list[3]].shape == (10000, 1)\n",
    "\n",
    "    def preprocess_data(self):\n",
    "\n",
    "        self.data[self.data_list[0]] /= 255\n",
    "        self.data[self.data_list[2]] /= 255\n",
    "\n",
    "        self.data[self.data_list[1]] = Utility.one_hot_encode(self.data[self.data_list[1]])\n",
    "        self.data[self.data_list[3]] = Utility.one_hot_encode(self.data[self.data_list[3]])\n",
    "\n",
    "        assert self.data[self.data_list[1]].shape == (60000, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Cifer10DataLoader:\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.size = 32\n",
    "        self.color_channel = 3\n",
    "        self.per_batch_data_size = 10000\n",
    "        self.data = dict()\n",
    "\n",
    "    def load_data(self, file_name):\n",
    "        with open(os.path.join(self.data_path, file_name), 'rb') as f:\n",
    "\n",
    "            data_dict=pickle.load(f, encoding='latin1')\n",
    "\n",
    "            images = data_dict['data']\n",
    "            labels = data_dict['labels']\n",
    "\n",
    "            images = images.reshape(self.per_batch_data_size, self.color_channel, self.size, self.size).transpose(0,2,3,1).astype(\"float\")\n",
    "            labels = np.array(labels)\n",
    "            print(labels.shape)\n",
    "\n",
    "            return images, labels\n",
    "\n",
    "    def concatenate_data(self):\n",
    "        X1, Y1 = self.load_data('data_batch_1')\n",
    "        X2, Y2 = self.load_data('data_batch_2')\n",
    "        X3, Y3 = self.load_data('data_batch_3')\n",
    "        X4, Y4 = self.load_data('data_batch_4')\n",
    "        X5, Y5 = self.load_data('data_batch_5')\n",
    "\n",
    "        self.data['train_images'] = np.concatenate(\n",
    "            (\n",
    "                X1, X2, X3, X4, X5\n",
    "            ),\n",
    "            axis=0\n",
    "        )\n",
    "\n",
    "        self.data['train_labels'] = np.concatenate(\n",
    "            (\n",
    "                Y1.reshape(self.per_batch_data_size, 1),\n",
    "                Y2.reshape(self.per_batch_data_size, 1),\n",
    "                Y3.reshape(self.per_batch_data_size, 1),\n",
    "                Y4.reshape(self.per_batch_data_size, 1),\n",
    "                Y5.reshape(self.per_batch_data_size, 1)\n",
    "            ),\n",
    "            axis=0\n",
    "        )\n",
    "\n",
    "        X_test, Y_test = self.load_data('test_batch')\n",
    "\n",
    "        self.data['test_images'] = X_test\n",
    "        self.data['test_labels'] = Y_test.reshape(Y_test.shape[0], 1)\n",
    "\n",
    "        self.assert_data_shape()\n",
    "\n",
    "        for key, data in self.data.items():\n",
    "            print(f'Shape: {data.shape}')\n",
    "\n",
    "    def assert_data_shape(self):\n",
    "        assert self.data['train_images'].shape == (50000, 32, 32, 3)\n",
    "        assert self.data['train_labels'].shape == (50000, 1)\n",
    "        assert self.data['test_images'].shape  == (10000, 32, 32, 3)\n",
    "        assert self.data['test_labels'].shape  == (10000, 1)\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        self.data['train_images'] /= 255\n",
    "        self.data['test_images'] /= 255\n",
    "\n",
    "        self.data['train_labels'] = Utility.one_hot_encode(self.data['train_labels'])\n",
    "        self.data['test_labels'] = Utility.one_hot_encode(self.data['test_labels'])\n",
    "\n",
    "        assert self.data['train_labels'].shape == (50000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Convolution2D:\n",
    "\n",
    "    # class variable\n",
    "    layer_num = 1\n",
    "    file_name = 'weights.txt'\n",
    "\n",
    "    def __init__(self, num_out_channel, filter_size, stride, padding_size):\n",
    "        self.num_out_channel = num_out_channel\n",
    "        self.filter_size = filter_size\n",
    "        self.stride = stride\n",
    "        self.padding_size = padding_size\n",
    "        self.h_prev, self.w_prev, self.num_channel_prev = None, None, None\n",
    "        self.h_new, self.w_new = None, None\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.output_tensor = None\n",
    "        self.cache = {}\n",
    "        self.layer_name = 'Conv2D__' + str(self.__class__.layer_num)\n",
    "        self.__class__.layer_num += 1\n",
    "        self.is_trainable = True\n",
    "        self.training_mode = False\n",
    "        self.activation_prev_current_layer_cache = None\n",
    "\n",
    "        print('The layer name: ', self.layer_name, self.layer_num)\n",
    "\n",
    "    def toggle_training_mode(self):\n",
    "        if not self.training_mode:\n",
    "            self.training_mode = True\n",
    "        else:\n",
    "            self.training_mode = False\n",
    "\n",
    "    def initialize_output_dimensions(self, prev_layer_output_dim):\n",
    "        \"\"\"\n",
    "        Initializes output dimensions with the dimension of the previous layers\n",
    "        :param prev_layer_output_dim: output dimension of the layer immediately before this layer\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.h_prev, self.w_prev , self.num_channel_prev = prev_layer_output_dim\n",
    "        self.h_new = (self.h_prev - self.filter_size + 2 * self.padding_size) // self.stride + 1\n",
    "        self.w_new = (self.w_prev - self.filter_size + 2 * self.padding_size) // self.stride + 1\n",
    "\n",
    "    def initialize_weights_biases(self):\n",
    "        \"\"\"\n",
    "        Initializes weights with the proper dimensions\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        #print('he: parameters: ', (self.h_prev * self.w_prev * self.num_channel_prev))\n",
    "        self.W = np.random.randn(self.filter_size, self.filter_size, self.num_channel_prev, self.num_out_channel) * np.sqrt(2/(self.h_prev * self.w_prev * self.num_channel_prev))\n",
    "        self.b = np.zeros((1, 1, 1, self.num_out_channel))\n",
    "\n",
    "    def forward_wob(self, Z_prev, is_training):\n",
    "        \"\"\"\n",
    "        Performs a forward operation of the convolution layer\n",
    "        :param Z_prev: The activation of the previous layer\n",
    "        :param is_training: whether we are in training mode or not\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        assert Z_prev.shape == (self.h_prev, self.w_prev, self.num_channel_prev)\n",
    "        Z_prev = np.array(Z_prev, copy=True)\n",
    "\n",
    "        # create zero padded Z_prev\n",
    "        Z_prev_padded = Utility.zero_pad_without_batch(Z_prev, self.padding_size)\n",
    "        self.output_tensor = np.zeros((self.h_new, self.w_new, self.num_out_channel))\n",
    "\n",
    "        # Apply convolution operation over this zero padded previous activation\n",
    "        for row in range(self.h_new):\n",
    "\n",
    "            row_start = row * self.stride\n",
    "\n",
    "            for col in range(self.w_new):\n",
    "\n",
    "                col_start = col *  self.stride\n",
    "\n",
    "                for output_channel_index in range(self.num_out_channel):\n",
    "\n",
    "                    Z_prev_windowed = Z_prev_padded[\n",
    "                        row_start : row_start + self.filter_size,\n",
    "                        col_start : col_start + self.filter_size,\n",
    "                        :\n",
    "                    ]\n",
    "\n",
    "                    conv_step_W = self.W[:, :, :, output_channel_index]\n",
    "                    conv_step_b = self.b[:, :, :, output_channel_index]\n",
    "\n",
    "                    self.output_tensor[row, col, output_channel_index] = Utility.convolve_single_step(Z_prev_windowed, conv_step_W, conv_step_b)\n",
    "\n",
    "        # asserting output shape\n",
    "        assert(self.output_tensor.shape == (self.h_new, self.w_new, self.num_out_channel))\n",
    "\n",
    "        if is_training:\n",
    "            # cache some values\n",
    "            pass\n",
    "\n",
    "        # perform activation element wise in this case\n",
    "        print(f'In forward of Convolution output tensor shape before relu {self.output_tensor.shape}')\n",
    "        self.output_tensor = self.relu_activation.activation_f(self.output_tensor)\n",
    "\n",
    "        # asserting output shape\n",
    "        #assert(self.output_tensor.shape == (self.h_new, self.w_new, self.num_out_channel))\n",
    "        print(f'In forward of CNN output tensor shape after relu {self.output_tensor.shape}')\n",
    "\n",
    "    def forward_batch(self, Z_prev, is_training=True):\n",
    "        \"\"\"\n",
    "        Performs a forward operation of the convolution layer\n",
    "        :param Z_prev: The activation of the previous layer\n",
    "        :param is_training: whether we are in training mode or not\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        batch_size = Z_prev.shape[0]\n",
    "        assert Z_prev.shape == (batch_size, self.h_prev, self.w_prev, self.num_channel_prev)\n",
    "\n",
    "        self.output_tensor = np.zeros((batch_size, self.h_new, self.w_new, self.num_out_channel))\n",
    "        Z_prev = np.array(Z_prev, copy=True)\n",
    "\n",
    "        # create zero padded Z_prev\n",
    "        print(f'Z prev shape:{Z_prev.shape}')\n",
    "        Z_prev_padded = Utility.zero_pad(Z_prev, self.padding_size)\n",
    "\n",
    "        # Apply convolution operation over this zero padded previous activation\n",
    "        for image_index in range(batch_size):\n",
    "            current_Z_prev_padded = Z_prev_padded[image_index] # choosing a single tensor from the batch\n",
    "            for row in range(self.h_new):\n",
    "\n",
    "                row_start = row * self.stride\n",
    "\n",
    "                for col in range(self.w_new):\n",
    "\n",
    "                    col_start = col *  self.stride\n",
    "\n",
    "                    for output_channel_index in range(self.num_out_channel):\n",
    "\n",
    "                        Z_prev_windowed = current_Z_prev_padded[\n",
    "                                        row_start : row_start + self.filter_size,\n",
    "                                        col_start : col_start + self.filter_size,\n",
    "                                        :\n",
    "                                        ]\n",
    "\n",
    "                        conv_step_W = self.W[:, :, :, output_channel_index]\n",
    "                        conv_step_b = self.b[:, :, :, output_channel_index]\n",
    "\n",
    "                        self.output_tensor[image_index, row, col, output_channel_index] = Utility.convolve_single_step(Z_prev_windowed, conv_step_W, conv_step_b)\n",
    "\n",
    "        # asserting output shape\n",
    "        assert(self.output_tensor.shape == (batch_size, self.h_new, self.w_new, self.num_out_channel))\n",
    "        print(self.output_tensor.shape)\n",
    "\n",
    "        if is_training:\n",
    "            # cache some values\n",
    "            pass\n",
    "\n",
    "        # perform activation element wise in this case\n",
    "        #self.output_tensor = self.relu_activation.activation_f(self.output_tensor)\n",
    "\n",
    "    def forward(self, Z_prev):\n",
    "        \"\"\"\n",
    "        Performs a forward operation of the convolution layer\n",
    "        :param Z_prev: The activation of the previous layer\n",
    "        :param is_training: whether we are in training mode or not\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        #print(Z_prev.shape)\n",
    "        batch_size = Z_prev.shape[0]\n",
    "        assert Z_prev.shape == (batch_size, self.h_prev, self.w_prev, self.num_channel_prev)\n",
    "\n",
    "        self.output_tensor = np.zeros((batch_size, self.h_new, self.w_new, self.num_out_channel))\n",
    "\n",
    "        # storing the activation for the bac propagation\n",
    "        self.activation_prev_current_layer_cache = Z_prev\n",
    "\n",
    "        Z_prev = np.array(Z_prev, copy=True)\n",
    "\n",
    "        # create zero padded Z_prev\n",
    "        #print(f'Z prev shape:{Z_prev.shape}')\n",
    "        Z_prev_padded = Utility.zero_pad(Z_prev, self.padding_size)\n",
    "\n",
    "        # Apply convolution operation over this zero padded previous activation\n",
    "        for row in range(self.h_new):\n",
    "\n",
    "            row_start = row * self.stride\n",
    "\n",
    "            for col in range(self.w_new):\n",
    "\n",
    "                col_start = col *  self.stride\n",
    "\n",
    "                for output_channel_index in range(self.num_out_channel):\n",
    "\n",
    "                    Z_prev_windowed = Z_prev_padded[:, row_start : row_start + self.filter_size,\n",
    "                                            col_start : col_start + self.filter_size, :]\n",
    "\n",
    "                    conv_step_W = self.W[:, :, :, output_channel_index]\n",
    "                    conv_step_b = self.b[:, :, :, output_channel_index]\n",
    "\n",
    "                    # print('z shape', Z_prev_windowed.shape)\n",
    "                    # print('w shape', conv_step_W.shape)\n",
    "\n",
    "                    # self.output_tensor[:, row, col, output_channel_index] = np.sum(\n",
    "                    #     Z_prev_windowed * conv_step_W,\n",
    "                    #     axis=(1,2,3)\n",
    "                    # ) + conv_step_b\n",
    "\n",
    "                    self.output_tensor[:, row, col, output_channel_index] = Utility.convolve_single_step_over_batch(\n",
    "                        Z_prev_windowed, conv_step_W, conv_step_b\n",
    "                    )\n",
    "\n",
    "            # asserting output shape\n",
    "            assert(self.output_tensor.shape == (batch_size, self.h_new, self.w_new, self.num_out_channel))\n",
    "            #print('output tensor shape:', self.output_tensor.shape)\n",
    "\n",
    "            if self.training_mode:\n",
    "                # cache some values\n",
    "                pass\n",
    "            # # perform activation element wise in this case\n",
    "            # self.output_tensor = self.relu_activation.activation_f(self.output_tensor)\n",
    "\n",
    "    def forward_(self, Z_prev):\n",
    "        \"\"\"\n",
    "        Performs a forward operation of the convolution layer\n",
    "        :param Z_prev: The activation of the previous layer\n",
    "        :param is_training: whether we are in training mode or not\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        print(Z_prev.shape)\n",
    "        batch_size = Z_prev.shape[0]\n",
    "        assert Z_prev.shape == (batch_size, self.h_prev, self.w_prev, self.num_channel_prev)\n",
    "\n",
    "        self.output_tensor = np.zeros((batch_size, self.h_new, self.w_new, self.num_out_channel))\n",
    "        Z_prev = np.array(Z_prev, copy=True)\n",
    "\n",
    "        # create zero padded Z_prev\n",
    "        #print(f'Z prev shape:{Z_prev.shape}')\n",
    "        Z_prev_padded = Utility.zero_pad(Z_prev, self.padding_size)\n",
    "\n",
    "        # Apply convolution operation over this zero padded previous activation\n",
    "        for row in range(self.h_new):\n",
    "\n",
    "            row_start = row * self.stride\n",
    "\n",
    "            for col in range(self.w_new):\n",
    "\n",
    "                col_start = col *  self.stride\n",
    "\n",
    "                Z_prev_windowed = Z_prev_padded[:,row_start : row_start + self.filter_size,\n",
    "                                  col_start : col_start + self.filter_size,:, np.newaxis]\n",
    "\n",
    "                conv_step_W = self.W[np.newaxis, :, :, :, :]\n",
    "                conv_step_b = self.b[:, :, :, :]\n",
    "\n",
    "                # print('z shape', Z_prev_windowed.shape)\n",
    "                # print('w shape', conv_step_W.shape)\n",
    "\n",
    "                # self.output_tensor[:, row, col, output_channel_index] = np.sum(\n",
    "                #     Z_prev_windowed * conv_step_W,\n",
    "                #     axis=(1,2,3)\n",
    "                # ) + conv_step_b\n",
    "\n",
    "                self.output_tensor[:, row, col, :] = Utility.convolve_single_step_over_batch(\n",
    "                    Z_prev_windowed, conv_step_W, conv_step_b\n",
    "                )\n",
    "\n",
    "            # asserting output shape\n",
    "            assert(self.output_tensor.shape == (batch_size, self.h_new, self.w_new, self.num_out_channel))\n",
    "            #print('output tensor shape:', self.output_tensor.shape)\n",
    "\n",
    "            if self.training_mode:\n",
    "                # cache some values\n",
    "                pass\n",
    "\n",
    "            return self.output_tensor\n",
    "\n",
    "            # # perform activation element wise in this case\n",
    "            # self.output_tensor = self.relu_activation.activation_f(self.output_tensor)\n",
    "\n",
    "    def get_output_tensor(self):\n",
    "        return self.output_tensor\n",
    "\n",
    "    def backward(self, dZ: np.array, learning_rate):\n",
    "        mini_batch_size = dZ.shape[0]\n",
    "\n",
    "        # initialize gradient shape\n",
    "        dActivation_prev, dW, db = self.initialize_gradients(mini_batch_size=mini_batch_size)\n",
    "\n",
    "        # do required padding\n",
    "        activation_prev_padded = Utility.zero_pad(self.activation_prev_current_layer_cache, self.padding_size)\n",
    "        dActivation_prev_padded = Utility.zero_pad(dActivation_prev, self.padding_size)\n",
    "\n",
    "        for row in range(self.h_new):\n",
    "\n",
    "            row_start = row * self.stride\n",
    "\n",
    "            for col in range(self.w_new):\n",
    "\n",
    "                col_start = col * self.stride\n",
    "\n",
    "                for output_channel_index in range(self.num_out_channel):\n",
    "\n",
    "                    # Use the corners to define the slice from a_prev_pad\n",
    "                    activation_slice = activation_prev_padded[:, row_start:row_start + self.filter_size, col_start:col_start + self.filter_size, :]\n",
    "\n",
    "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n",
    "                    dActivation_prev_padded[:, row_start:row_start + self.filter_size, col_start:col_start + self.filter_size, :] += self.W[np.newaxis, :,:,:,output_channel_index] * dZ[: , row:row+1, col:col+1, np.newaxis,  output_channel_index] # done works\n",
    "\n",
    "                    # print('a_slice:', a_slice.shape)\n",
    "                    # print('dZ:', dZ[:, h:h+1, w:w+1, np.newaxis, c].shape)\n",
    "                    # s = a_slice * dZ[:, h:h+1, w:w+1, np.newaxis, c]\n",
    "                    # print('s shape:', s.shape)\n",
    "\n",
    "                    dW[:,:,:,output_channel_index] += np.sum(activation_slice * dZ[:, row:row+1, col:col+1, np.newaxis, output_channel_index], axis=0)\n",
    "\n",
    "                    db[:,:,:,output_channel_index] += np.sum(dZ[:, row:row+1, col:col+1, output_channel_index], axis=(0,1,2))\n",
    "\n",
    "        # unpad the dActivation_padded\n",
    "        if self.padding_size != 0:\n",
    "            dActivation_prev[:, :, :, :] = dActivation_prev_padded[:,\n",
    "                                       self.padding_size:-self.padding_size,\n",
    "                                       self.padding_size:-self.padding_size,\n",
    "                                       :]\n",
    "        else:\n",
    "            dActivation_prev = dActivation_prev_padded\n",
    "\n",
    "        # careful here!!!\n",
    "        dW = dW / mini_batch_size\n",
    "        db = db / mini_batch_size\n",
    "\n",
    "        assert dActivation_prev.shape == (mini_batch_size, self.h_prev, self.w_prev, self.num_channel_prev)\n",
    "\n",
    "        assert dW.shape == (self.filter_size, self.filter_size, self.num_channel_prev, self.num_out_channel)\n",
    "\n",
    "        assert db.shape == (1,1,1,self.num_out_channel)\n",
    "\n",
    "        self.update_CNN_parameters(dW, db, learning_rate=learning_rate)\n",
    "\n",
    "        #print('output of cnn backward:', dActivation_prev)\n",
    "        #print('shape of output cnn backward: ', dActivation_prev.shape)\n",
    "        return dActivation_prev\n",
    "\n",
    "    def initialize_gradients(self, mini_batch_size):\n",
    "        dActivation_prev = np.zeros((mini_batch_size, self.h_prev, self.w_prev, self.num_channel_prev))\n",
    "\n",
    "        dW = np.zeros((self.filter_size, self.filter_size, self.num_channel_prev, self.num_out_channel))\n",
    "\n",
    "        db = np.zeros((1,1,1, self.num_out_channel))\n",
    "\n",
    "        return dActivation_prev, dW, db\n",
    "\n",
    "    def update_CNN_parameters(self, dW : np.array, db: np.array, learning_rate: float):\n",
    "        #print(f'learning rate: {learning_rate}')\n",
    "        self.W = self.W - learning_rate * dW\n",
    "        self.b = self.b - learning_rate * db\n",
    "\n",
    "        # print('------------------ccccccccccccccccccccc----------------------------')\n",
    "        # print(f'layer {self.layer_name} W updated: {self.W}')\n",
    "        # print(f'layer {self.layer_name} b updated: {self.b}')\n",
    "\n",
    "        f = open(self.file_name, 'a+')\n",
    "\n",
    "        f.write(f'W of {self.layer_name} layer: \\n')\n",
    "        f.write(str(self.W))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        f.write(f'b of {self.layer_name} layer: \\n')\n",
    "        f.write(str(self.b))\n",
    "        f.write('\\n')\n",
    "\n",
    "        f.close()\n",
    "\n",
    "\n",
    "    def print_layer_dimensions(self):\n",
    "        print(f'Output Tensor Dimensions: {self.output_tensor.shape}')\n",
    "        print(f'Weight Dimension: {self.W.shape}')\n",
    "        print(f'Bias Dimension: {self.b.shape}')\n",
    "\n",
    "    def get_output_dimension(self) -> Tuple:\n",
    "        return self.h_new, self.w_new, self.num_out_channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    # class variable\n",
    "    layer_num = 1\n",
    "\n",
    "    def __init__(self):\n",
    "        self.input_dim = None\n",
    "        self.output_tensor = None\n",
    "        self.output_dim = None\n",
    "        self.h_prev, self.w_prev, self.num_channel_prev = None, None, None\n",
    "        self.layer_name = 'Flatten__' + str(self.layer_num)\n",
    "        self.__class__.layer_num += 1\n",
    "        self.is_trainable = False\n",
    "        self.shape_input_tensor = None\n",
    "\n",
    "    def initialize_flatten_layer_dimensions(self, prev_layer_output_dim):\n",
    "        \"\"\"\n",
    "        :param prev_layer_output_dim: prev layer output of shape (new_h, new_w, new_channel)\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.h_prev, self.w_prev, self.num_channel_prev  = prev_layer_output_dim\n",
    "        self.output_dim = self.h_prev * self.w_prev * self.num_channel_prev\n",
    "\n",
    "    def forward(self, Z_prev: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "\n",
    "        :param Z_prev: Z_prev of shape (1/batch size, prev_h, prev_w, prev_color_channel)\n",
    "        :return: tensor of shape (1/batch size, prev_h * prev_w * prev_color_channel)\n",
    "        \"\"\"\n",
    "\n",
    "        self.shape_input_tensor = Z_prev.shape\n",
    "        self.output_tensor = Z_prev.reshape(Z_prev.shape[0], Z_prev.shape[1] * Z_prev.shape[2] * Z_prev.shape[3])\n",
    "\n",
    "        assert self.output_tensor.shape[1] == self.output_dim\n",
    "\n",
    "    def forward_wob(self, Z_prev: np.array) -> np.array:\n",
    "        self.output_tensor = Z_prev.reshape(1, Z_prev.shape[0] * Z_prev.shape[1] * Z_prev.shape[2])\n",
    "\n",
    "    def get_output_dimension(self):\n",
    "        return self.output_dim\n",
    "\n",
    "    def get_output_tensor(self) -> np.array:\n",
    "        return self.output_tensor\n",
    "\n",
    "    def backward(self, dZ: np.array):\n",
    "        \"\"\"\n",
    "\n",
    "        :param dZ: dZ shape: (batch size/1, prev_h * prev_w * prev_color_channel)\n",
    "        :return: tensor of shape: (batch size/1, prev_h, prev_w, prev_color_channel\n",
    "        \"\"\"\n",
    "\n",
    "        assert dZ.shape == self.output_tensor.shape\n",
    "\n",
    "        dZ_reshaped = dZ.reshape(self.shape_input_tensor)\n",
    "\n",
    "        return dZ_reshaped\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## MaxPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class MaxPool:\n",
    "    # class variable\n",
    "    layer_num = 1\n",
    "\n",
    "    def __init__(self, filter_size, stride):\n",
    "        self.filter_size = filter_size\n",
    "        self.stride = stride\n",
    "        self.h_prev, self.w_prev, self.num_channel_prev = None, None, None\n",
    "        self.h_new, self.w_new, self.num_out_channel = None, None, None\n",
    "        self.output_tensor = None\n",
    "        self.cache = {}\n",
    "        self.layer_name = 'MaxPool__'+ str(self.layer_num)\n",
    "        self.__class__.layer_num += 1\n",
    "        self.is_trainable = False\n",
    "        self.training_mode = False\n",
    "        self.activation_prev_cached = None\n",
    "\n",
    "    def toggle_training_mode(self):\n",
    "        if not self.training_mode:\n",
    "            self.training_mode = True\n",
    "        else:\n",
    "            self.training_mode = False\n",
    "\n",
    "    def initialize_max_pool_params(self, prev_layer_output_dim):\n",
    "        \"\"\"\n",
    "        Initializes output dimensions with the dimension of the previous layers\n",
    "        :param prev_layer_output_dim: output dimension of the layer immediately before this layer\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.h_prev, self.w_prev , self.num_channel_prev = prev_layer_output_dim\n",
    "        self.h_new = int((self.h_prev - self.filter_size) / self.stride + 1)\n",
    "        self.w_new = int((self.w_prev - self.filter_size) / self.stride + 1)\n",
    "        self.num_out_channel = self.num_channel_prev\n",
    "\n",
    "    def forward_wob(self, Z_prev, is_training):\n",
    "\n",
    "        #print('prev z shape in maxpool:', Z_prev.shape)\n",
    "        assert Z_prev.shape == (self.h_prev, self.w_prev, self.num_channel_prev)\n",
    "        #print('here')\n",
    "        Z_prev = np.array(Z_prev, copy=True)\n",
    "        #print('there')\n",
    "        self.output_tensor = np.zeros((self.h_new, self.w_new, self.num_out_channel))\n",
    "\n",
    "        #print('Going for max pooling')\n",
    "        # Apply convolution operation over this zero padded previous activation\n",
    "        for row in range(self.h_new):\n",
    "\n",
    "            row_start = row * self.stride\n",
    "\n",
    "            for col in range(self.w_new):\n",
    "\n",
    "                col_start = col *  self.stride\n",
    "\n",
    "                for output_channel_index in range(self.num_out_channel):\n",
    "\n",
    "                    Z_prev_windowed = Z_prev[\n",
    "                                      row_start : row_start + self.filter_size,\n",
    "                                      col_start : col_start + self.filter_size,\n",
    "                                      output_channel_index\n",
    "                                      ]\n",
    "\n",
    "                    self.output_tensor[row, col, output_channel_index] = Utility.get_max_pool_window(Z_prev_windowed)\n",
    "\n",
    "        assert self.output_tensor.shape == (self.h_new, self.w_new, self.num_out_channel)\n",
    "        if is_training:\n",
    "            pass\n",
    "        print('Max pool forward done')\n",
    "\n",
    "    def forward(self, Z_prev):\n",
    "\n",
    "        batch_size = Z_prev.shape[0]\n",
    "        assert Z_prev.shape == (batch_size, self.h_prev, self.w_prev, self.num_channel_prev)\n",
    "\n",
    "        self.activation_prev_cached = Z_prev\n",
    "\n",
    "        self.output_tensor = np.zeros((batch_size, self.h_new, self.w_new, self.num_out_channel))\n",
    "        Z_prev = np.array(Z_prev, copy=True)\n",
    "\n",
    "        # Apply convolution operation over this zero padded previous activation\n",
    "        for row in range(self.h_new):\n",
    "\n",
    "            row_start = row * self.stride\n",
    "\n",
    "            for col in range(self.w_new):\n",
    "\n",
    "                col_start = col *  self.stride\n",
    "\n",
    "                for output_channel_index in range(self.num_out_channel):\n",
    "\n",
    "                    Z_prev_windowed = Z_prev[:,\n",
    "                                    row_start : row_start + self.filter_size,\n",
    "                                    col_start : col_start + self.filter_size,\n",
    "                                    output_channel_index\n",
    "                                      ]\n",
    "\n",
    "                    self.output_tensor[:, row, col, output_channel_index] = Utility.get_max_pool_window_over_batch(Z_prev_windowed)\n",
    "\n",
    "        assert self.output_tensor.shape == (batch_size, self.h_new, self.w_new, self.num_out_channel)\n",
    "        if self.training_mode:\n",
    "            pass\n",
    "\n",
    "    def forward_(self, Z_prev):\n",
    "\n",
    "        batch_size = Z_prev.shape[0]\n",
    "        assert Z_prev.shape == (batch_size, self.h_prev, self.w_prev, self.num_channel_prev)\n",
    "        self.activation_prev_cached = Z_prev\n",
    "\n",
    "        self.output_tensor = np.zeros((batch_size, self.h_new, self.w_new, self.num_out_channel))\n",
    "        Z_prev = np.array(Z_prev, copy=True)\n",
    "\n",
    "        # Apply convolution operation over this zero padded previous activation\n",
    "        for row in range(self.h_new):\n",
    "\n",
    "            row_start = row * self.stride\n",
    "\n",
    "            for col in range(self.w_new):\n",
    "\n",
    "                col_start = col *  self.stride\n",
    "\n",
    "\n",
    "                Z_prev_windowed = Z_prev[:,\n",
    "                                    row_start : row_start + self.filter_size,\n",
    "                                    col_start : col_start + self.filter_size,\n",
    "                                      :\n",
    "                                    ]\n",
    "\n",
    "                self.output_tensor[:, row, col, :] = Utility.get_max_pool_window_over_batch(Z_prev_windowed)\n",
    "\n",
    "        assert self.output_tensor.shape == (batch_size, self.h_new, self.w_new, self.num_out_channel)\n",
    "        if self.training_mode:\n",
    "            pass\n",
    "\n",
    "    def print_layer_dimensions(self):\n",
    "        print(f'Output Tensor Dimensions: {self.output_tensor.shape}')\n",
    "\n",
    "    def get_output_tensor(self):\n",
    "        return self.output_tensor\n",
    "\n",
    "    # WOB\n",
    "    def backward_(self, dActivation_pool: np.array):\n",
    "\n",
    "        dActivation_pool_prev = np.zeros_like(self.activation_prev_cached)\n",
    "        print(dActivation_pool_prev.shape)\n",
    "        mini_batch_size = self.activation_prev_cached.shape[0]\n",
    "        print('mini batch size:', mini_batch_size)\n",
    "\n",
    "        for batch_index in range(mini_batch_size):\n",
    "            # first select a fixed tensor from the batch\n",
    "            activation_prev = self.activation_prev_cached[batch_index, :, :, :]\n",
    "\n",
    "            # print('dA shape:', dA.shape)\n",
    "            # print('h:', self.h_new)\n",
    "            # print('w:', self.w_new)\n",
    "            # print('c:', self.num_out_channel)\n",
    "\n",
    "            for row in range(self.h_new):\n",
    "\n",
    "                for col in range(self.w_new):\n",
    "\n",
    "                    for output_channel_index in range(self.num_out_channel):\n",
    "\n",
    "                        row_start = row * self.stride\n",
    "                        row_end = row * self.stride + self.filter_size\n",
    "                        col_start = col * self.stride\n",
    "                        col_end = col * self.stride + self.filter_size\n",
    "\n",
    "                        activation_prev_window = activation_prev[\n",
    "                            row_start: row_end,\n",
    "                            col_start: col_end,\n",
    "                            output_channel_index\n",
    "                        ]\n",
    "\n",
    "                        # print('before the mask:', activation_prev_window)\n",
    "                        mask = Utility.get_mask_from_tensor(activation_prev_window)\n",
    "                        # print('the mashk:', mask)\n",
    "                        # print(dActivation_pool.shape)\n",
    "                        # print('batch index: ', batch_index)\n",
    "                        # print('row: ', row)\n",
    "                        # print('col: ', col)\n",
    "                        # print('output channel index: ', output_channel_index)\n",
    "                        # s = dActivation_pool[batch_index, row, col, output_channel_index]\n",
    "                        dActivation_pool_prev[batch_index, row_start: row_end, col_start : col_end,output_channel_index] +=  dActivation_pool[batch_index, row, col, output_channel_index] * mask\n",
    "\n",
    "                        #print(f'for batch {batch_index} {dActivation_pool_prev[batch_index,row: row + self.filter_size,col: col + self.filter_size,output_channel_index]}')\n",
    "\n",
    "        # print(dActivation_pool.shape)\n",
    "        #print(self.activation_prev_cached.shape)\n",
    "        assert dActivation_pool_prev.shape == self.activation_prev_cached.shape\n",
    "        print('-----------------------------------------------------------------------')\n",
    "        print('Output of backward pooling non batched: ', dActivation_pool_prev)\n",
    "        return dActivation_pool_prev\n",
    "\n",
    "    # With Batching\n",
    "    def backward(self, dActivation_pool: np.array):\n",
    "\n",
    "        last_mask = None\n",
    "        last_dA = None\n",
    "\n",
    "        # print('sssssssssssssssssssssssssssssssssssssssssssss')\n",
    "        # print('dA from previous:', dActivation_pool)\n",
    "        # print()\n",
    "\n",
    "        dActivation_pool_prev = np.zeros_like(self.activation_prev_cached)\n",
    "        #print(self.activation_prev_cached.shape)\n",
    "        mini_batch_size = self.activation_prev_cached.shape[0]\n",
    "        #print('mini batch size:', mini_batch_size)\n",
    "\n",
    "        for row in range(self.h_new):\n",
    "            row_start = row * self.stride\n",
    "            for col in range(self.w_new):\n",
    "                col_start = col * self.stride\n",
    "                for output_channel_index in range(self.num_out_channel):\n",
    "\n",
    "\n",
    "                    activation_prev_window = self.activation_prev_cached[:, row_start: row_start + self.filter_size, col_start: col_start + self.filter_size, output_channel_index]\n",
    "                    #print('before the mask:', activation_prev_window)\n",
    "                    mask = Utility.get_mask_from_tensor_batch(activation_prev_window)\n",
    "                    # last_mask = mask\n",
    "                    # last_dA = dActivation_pool[:,  row: row + 1,  col: col + 1, output_channel_index]\n",
    "\n",
    "                    dActivation_pool_prev[:, row_start: row_start + self.filter_size, col_start: col_start + self.filter_size, output_channel_index] += dActivation_pool[:,  row: row + 1,  col: col + 1, output_channel_index] * mask\n",
    "\n",
    "\n",
    "        assert dActivation_pool_prev.shape == self.activation_prev_cached.shape\n",
    "        # print('xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx')\n",
    "        # print('mask shape:', last_mask.shape)\n",
    "        # print('the mask:', last_mask)\n",
    "        #\n",
    "        # print('the dA:', last_dA)\n",
    "        # print('------------------------------------------------')\n",
    "        # print('In pooling output custom:  -- dA', dActivation_pool_prev)\n",
    "        return dActivation_pool_prev\n",
    "\n",
    "    def get_output_dimension(self):\n",
    "        return self.h_new, self.w_new, self.num_out_channel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Denselayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "\n",
    "    # class variable\n",
    "    layer_num = 1\n",
    "    file_name = './weights.txt'\n",
    "\n",
    "    def __init__(self, num_units):\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.num_units = num_units\n",
    "        self.A_prev_layer_cached = None\n",
    "        self.output_tensor = None\n",
    "        self.layer_name = 'Dense__' + str(self.__class__.layer_num)\n",
    "        self.__class__.layer_num += 1\n",
    "        self.is_trainable = True\n",
    "        self.training_mode = False\n",
    "\n",
    "    def toggle_training_mode(self):\n",
    "        if not self.training_mode:\n",
    "            self.training_mode = True\n",
    "        else:\n",
    "            self.training_mode = False\n",
    "\n",
    "\n",
    "    def initialize_dense_layer_weights_biases(self, prev_layer_output_dim):\n",
    "        self.W = np.random.randn(self.num_units, prev_layer_output_dim) * np.sqrt(2/prev_layer_output_dim)\n",
    "        self.b = np.zeros((self.num_units, 1)) # will be broadcast to (hidden_units, batch_size) before addition\n",
    "\n",
    "        #print('W dense: ', self.W)\n",
    "\n",
    "    def forward(self, A_prev_layer):\n",
    "        \"\"\"\n",
    "        :param A_prev_layer: tensor of shape (batch, prev_flattened_shape)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # print('starting of dense: ', A_prev_layer)\n",
    "        # print(f'A_prev_layer shape: {A_prev_layer.shape}')\n",
    "        assert A_prev_layer.shape[1] == self.W.shape[1]\n",
    "\n",
    "        self.A_prev_layer_cached = A_prev_layer\n",
    "\n",
    "        A_prev_layer = np.array(A_prev_layer, copy=True)\n",
    "        A_prev_layer_reshaped = A_prev_layer.T\n",
    "\n",
    "        Z = np.dot(self.W, A_prev_layer_reshaped) + self.b\n",
    "\n",
    "        self.output_tensor = Z.T # converting to (batch_size, num_units)\n",
    "        #print('dense layer output: ',Z)\n",
    "\n",
    "        # assert the output tensor shape should be (num_hidden_units, batch size)\n",
    "        assert self.output_tensor.shape == (A_prev_layer_reshaped.shape[1], self.num_units)\n",
    "\n",
    "        if self.training_mode:\n",
    "            pass\n",
    "\n",
    "    def get_output_tensor(self):\n",
    "        return self.output_tensor\n",
    "\n",
    "    def backward(self, dZ : np.array, learning_rate):\n",
    "        \"\"\"\n",
    "\n",
    "        :param dZ: the gradient of loss with respect to this layers output Z. dZ = dL/dZ. Shape: (batch size , num_units)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        #\n",
    "        # print('Start of dense backward: ', dZ)\n",
    "        #print(learning_rate)\n",
    "\n",
    "        # for softmax activation after this, the value of dZ = y_pred - y\n",
    "        A_prev_layer = self.A_prev_layer_cached\n",
    "        mini_batch_size = dZ.shape[0]\n",
    "        dW = (1/mini_batch_size) * np.dot(dZ.T, A_prev_layer) # (num unit, 1) * (1, x) --> (num unit, x)\n",
    "        db = (1/mini_batch_size) * np.sum(dZ.T, axis=1, keepdims=True)\n",
    "        #dW =  np.dot(dZ.T, A_prev_layer) # (num unit, 1) * (1, x) --> (num unit, x)\n",
    "        dA_prev_layer = np.dot(dZ, self.W)\n",
    "        #db = np.sum(dZ.T, axis=1, keepdims=True)\n",
    "\n",
    "        # print('dW', dW)\n",
    "        # print('dB', db)\n",
    "\n",
    "        #print(dW.shape)\n",
    "        #print(db.shape)\n",
    "        #print(self.b.shape)\n",
    "        assert dW.shape == self.W.shape\n",
    "        assert db.shape == self.b.shape\n",
    "        assert dA_prev_layer.shape == A_prev_layer.shape\n",
    "\n",
    "        self.update_parameters(dW, db, learning_rate=learning_rate)\n",
    "        return dA_prev_layer\n",
    "\n",
    "    def get_output_dimension(self):\n",
    "        return self.num_units\n",
    "\n",
    "    def print_layer_dimensions(self):\n",
    "        print(f'Output Tensor Dimensions: {self.output_tensor.shape}')\n",
    "        print(f'Weight Dimension: {self.W.shape}')\n",
    "        print(f'Bias Dimension: {self.b.shape}')\n",
    "\n",
    "    def update_parameters(self, dW: np.array, db: np.array, learning_rate: float):\n",
    "        #print('updating parameters', learning_rate)\n",
    "        self.W = self.W -   (dW * learning_rate)\n",
    "        self.b = self.b -   (db * learning_rate)\n",
    "\n",
    "        #print(f'Weight of {self.layer_name} : {self.W}')\n",
    "        #print(f'Bias of {self.layer_name} : {self.b}')\n",
    "\n",
    "        f = open(self.file_name, 'w+')\n",
    "\n",
    "        f.write(f'W of {self.layer_name} layer: \\n')\n",
    "        f.write(str(self.W))\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        f.write(f'b of {self.layer_name} layer: \\n')\n",
    "        f.write(str(self.b))\n",
    "        f.write('\\n')\n",
    "\n",
    "        f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Utility:\n",
    "\n",
    "    @staticmethod\n",
    "    def one_hot_encode(y_true):\n",
    "        # Define the One-hot Encoder\n",
    "        ohe = preprocessing.OneHotEncoder()\n",
    "        ohe.fit(y_true)\n",
    "        y_true = ohe.transform(y_true).toarray()\n",
    "        return y_true\n",
    "\n",
    "    @staticmethod\n",
    "    def zero_pad(tensor, pad_size):\n",
    "        \"\"\"\n",
    "        :param tensor: tensor of shape (batch_size, h, w, num_channel)\n",
    "        :return: padded tensor of shape (h + 2 * pad_size, w + 2 * pad_size, num_channel)\n",
    "        \"\"\"\n",
    "        return np.pad(tensor, ((0,0), (pad_size, pad_size), (pad_size, pad_size), (0,0)), mode='constant', constant_values=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def zero_pad_without_batch(tensor, pad_size):\n",
    "        \"\"\"\n",
    "        :param tensor: tensor of shape (h, w, num_channel)\n",
    "        :return: padded tensor of shape (h + 2 * pad_size, w + 2 * pad_size, num_channel)\n",
    "        \"\"\"\n",
    "        return np.pad(tensor, ((pad_size, pad_size), (pad_size, pad_size), (0,0)), mode='constant', constant_values=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def convolve_single_step(Z_prev_windowed, W, b):\n",
    "        \"\"\"\n",
    "        :param Z_prev_windowed: window of shape (F, F, num_channel_Z_prev)\n",
    "        :param W: kernel/filter/weight of shape (F, F, num_channel_Z_prev)\n",
    "        :param b: bias term of shape (1, 1, 1)\n",
    "        :return: scaler convolved value\n",
    "        \"\"\"\n",
    "        return np.multiply(Z_prev_windowed, W).sum() + float(b)\n",
    "\n",
    "    @staticmethod\n",
    "    def convolve_single_step_over_batch(tensor, W, b):\n",
    "        # print('slice shape:', tensor.shape)\n",
    "        # print('W shape:', W.shape)\n",
    "        # print('b shape:', b.shape)\n",
    "        return np.sum(tensor * W, axis=(1,2,3)) + b\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_max_pool_window(Z_prev_windowed):\n",
    "        return Z_prev_windowed.max()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_max_pool_window_over_batch(Z_prev_windowed: np.array):\n",
    "        #print('In max pool', Z_prev_windowed.shape)\n",
    "        #print(np.max(Z_prev_windowed, axis=(1,2)))\n",
    "        return np.max(Z_prev_windowed, axis=(1,2))\n",
    "\n",
    "    @staticmethod\n",
    "    def create_mini_batches(X: np.array, Y: np.array, mini_batch_size: int):\n",
    "        total_data = X.shape[0]\n",
    "        for index in range(0, total_data, mini_batch_size):\n",
    "            start_index = index\n",
    "            end_index = min(start_index + mini_batch_size, total_data)\n",
    "            yield X[start_index: end_index,...], Y[start_index: end_index, ...]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_mask_from_tensor(tensor: np.array):\n",
    "        mask = (tensor == np.max(tensor))\n",
    "        return mask\n",
    "\n",
    "    @staticmethod\n",
    "    def get_mask_from_tensor_batch(tensor: np.array):\n",
    "        mask = (tensor == tensor.max(axis=(1,2)).reshape(tensor.shape[0], 1, 1))\n",
    "        return mask\n",
    "\n",
    "    @staticmethod\n",
    "    def read_model_config(file_path, data_loader):\n",
    "\n",
    "        with open(file_path) as f:\n",
    "            file_lines = [line.strip() for line in f.readlines()]\n",
    "            print(file_lines)\n",
    "\n",
    "            first_dense_layer_index = -1\n",
    "            for index in range(len(file_lines)):\n",
    "                if file_lines[index].startswith(\"FC\") and not file_lines[index - 1].startswith(\"Flatten\"):\n",
    "                    first_dense_layer_index = index\n",
    "                    break\n",
    "\n",
    "            # print(first_dense_layer_index)\n",
    "\n",
    "            if first_dense_layer_index != -1:\n",
    "                file_lines.insert(first_dense_layer_index, 'Flatten')\n",
    "\n",
    "            # print(file_lines)\n",
    "\n",
    "            layer_list = [InputLayer(input_dimension=(data_loader.size, data_loader.size, data_loader.color_channel), is_trainable=False, layer_name='Input')]\n",
    "\n",
    "\n",
    "            for line in file_lines:\n",
    "                if line.startswith(\"Conv\"):\n",
    "                    line_splitted = line.split()\n",
    "                    layer_list.append( Convolution2D(num_out_channel=int(line_splitted[1]), filter_size=int(line_splitted[2]), stride=int(line_splitted[3]), padding_size=int(line_splitted[4])))\n",
    "                elif line.startswith(\"ReLU\"):\n",
    "                    layer_list.append(ReLUActivation())\n",
    "                elif line.startswith(\"Pool\"):\n",
    "                    line_splitted = line.split()\n",
    "                    layer_list.append(MaxPool(filter_size=int(line_splitted[1]), stride=int(line_splitted[2])))\n",
    "                elif line.startswith(\"Flatten\"):\n",
    "                    layer_list.append(Flatten())\n",
    "                elif line.startswith(\"FC\"):\n",
    "                    line_splitted = line.split()\n",
    "                    layer_list.append(DenseLayer(int(line_splitted[1])))\n",
    "                elif line.startswith(\"Softmax\"):\n",
    "                    layer_list.append(SoftmaxActivation())\n",
    "\n",
    "            return layer_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            # layer_list=[\n",
    "            #     InputLayer(input_dimension=(mnist.size, mnist.size, mnist.color_channel), is_trainable=False, layer_name='Input'),\n",
    "            #     Convolution2D(num_out_channel=6, filter_size=5, stride=1, padding_size=2),\n",
    "            #     ReLUActivation(),\n",
    "            #     MaxPool(filter_size=2, stride=2),\n",
    "            #     Convolution2D(num_out_channel=12, filter_size=5, stride=1, padding_size=0),\n",
    "            #     ReLUActivation(),\n",
    "            #     MaxPool(filter_size=2, stride=2),\n",
    "            #     Convolution2D(num_out_channel=100, filter_size=5, stride=1, padding_size=0),\n",
    "            #     ReLUActivation(),\n",
    "            #     Flatten(),\n",
    "            #     DenseLayer(10),\n",
    "            #     SoftmaxActivation()\n",
    "            # ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class ReLUActivation:\n",
    "\n",
    "    # class variable\n",
    "    layer_num = 1\n",
    "    def __init__(self):\n",
    "        self.layer_name = 'ReLU__' + str(self.layer_num)\n",
    "        self.__class__.layer_num += 1\n",
    "        self.input_tensor_dimension = None\n",
    "        self.output_tensor = None\n",
    "        self.is_trainable = False\n",
    "        self.input_tensor_cached = None\n",
    "\n",
    "    def set_input_tensor_dimension(self, prev_layer_tensor_dimension):\n",
    "        self.input_tensor_dimension = prev_layer_tensor_dimension\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        #print('Relu Input Tensor Shape: ', tensor.shape)\n",
    "        # print('in relu:', tensor)\n",
    "        # print('After: ', np.maximum(tensor, 0))\n",
    "        self.input_tensor_cached = tensor\n",
    "        self.output_tensor = np.maximum(tensor, 0)\n",
    "\n",
    "    def get_output_dimension(self):\n",
    "        return self.input_tensor_dimension\n",
    "\n",
    "\n",
    "    def backward(self, dA: np.array):\n",
    "        dA = np.array(dA, copy=True)\n",
    "        dA[self.input_tensor_cached <= 0] = 0\n",
    "\n",
    "        # print('In relu: dA', dA)\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "@dataclass(unsafe_hash=True)\n",
    "class InputLayer:\n",
    "    \"\"\"\n",
    "    Class for saving input dimension\n",
    "    \"\"\"\n",
    "    input_dimension: np.array\n",
    "    is_trainable: bool\n",
    "    layer_name: str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SoftmaxActivation:\n",
    "\n",
    "    # class variable\n",
    "    layer_num = 1\n",
    "    def __init__(self):\n",
    "        self.input_tensor_dimension = None\n",
    "        self.layer_name = 'Softmax__' + str(self.layer_num)\n",
    "        self.__class__.layer_num += 1\n",
    "        self.output_tensor = None\n",
    "        self.is_trainable = False\n",
    "\n",
    "    def set_input_tensor_dimension(self, prev_layer_tensor_dimension):\n",
    "        self.input_tensor_dimension = prev_layer_tensor_dimension\n",
    "\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        #print('softmax tensor shape:', tensor.shape) # expected tensor shape: (1/batch-size, classes)\n",
    "        exponent = np.exp(tensor - np.max(tensor, axis=1, keepdims=True))\n",
    "\n",
    "        #print('softmax exp shape',exponent.shape)\n",
    "        summation_along_batch = np.sum(exponent, axis=1, keepdims=True)\n",
    "\n",
    "        #print('softmax sum shape:', summation_along_batch.shape)\n",
    "\n",
    "        self.output_tensor = exponent/summation_along_batch\n",
    "\n",
    "        return self.output_tensor\n",
    "\n",
    "\n",
    "    def get_output_dimension(self):\n",
    "        return self.input_tensor_dimension\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(dA: np.array):\n",
    "        \"\"\" passes the backward gradient\n",
    "        :param dA: the gradient wrt to the activation softmax\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "    epsilon = 1e-50\n",
    "\n",
    "    def compute_cost(self, y_pred, y):\n",
    "        mini_batch_size = y.shape[0]\n",
    "        clipped_y_pred = np.clip(y_pred, self.epsilon, 1.0)\n",
    "        cross_entropy_loss = -(1/mini_batch_size) * np.sum(np.multiply(y, np.log(clipped_y_pred)))\n",
    "        return cross_entropy_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = None # a list of layer object according to input\n",
    "        self.layer_w_gradients = dict() # {'layer name': dw}\n",
    "        self.layer_b_gradients = dict() # {'layer name': db}\n",
    "        self.epochs = None\n",
    "        self.cost_function = None\n",
    "        self.history = {\n",
    "            'train_loss': [],\n",
    "            'train_acc': [],\n",
    "            'validation_loss': [],\n",
    "            'validation_acc': []\n",
    "        }\n",
    "\n",
    "    def add(self, layer_list):\n",
    "        self.layers = layer_list\n",
    "\n",
    "    def initializer_layer_params(self):\n",
    "        \"\"\"\n",
    "        This method initializes the layers in the model providing the input dimension that the layers expect to get\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        #InputLayer(input_dimension=input_dimension, is_trainable=False, layer_name='Input')\n",
    "\n",
    "        assert self.layers[0].layer_name == 'Input'\n",
    "\n",
    "        for previous_layer, current_layer in zip(self.layers, self.layers[1:]):\n",
    "            prev_output_dim = None\n",
    "            if previous_layer.layer_name == 'Input':\n",
    "                prev_output_dim = previous_layer.input_dimension # H, W, Color Channel\n",
    "            else:\n",
    "                prev_output_dim = previous_layer.get_output_dimension()\n",
    "\n",
    "\n",
    "            if current_layer.layer_name.startswith('Conv2D__'):\n",
    "                current_layer.initialize_output_dimensions(prev_output_dim) # H, W, Color Channel\n",
    "                current_layer.initialize_weights_biases()\n",
    "            elif current_layer.layer_name.startswith('MaxPool__'):\n",
    "                print(prev_output_dim)\n",
    "                current_layer.initialize_max_pool_params(prev_output_dim) # H, W, Color Channel\n",
    "            elif current_layer.layer_name.startswith('Dense__'):\n",
    "                current_layer.initialize_dense_layer_weights_biases(prev_output_dim) # flatten layer dimension\n",
    "            elif current_layer.layer_name.startswith('Flatten__'):\n",
    "                current_layer.initialize_flatten_layer_dimensions(prev_output_dim) # (new_h, new_w, new_channel)\n",
    "            elif current_layer.layer_name.startswith(\"ReLU\") or current_layer.layer_name.startswith(\"Softmax\") :\n",
    "                current_layer.set_input_tensor_dimension(prev_output_dim) # for the activation layers\n",
    "\n",
    "\n",
    "    def compile(self, cost_function):\n",
    "        self.cost_function = cost_function\n",
    "\n",
    "    def train(self, training_data, test_data, epochs=5, learning_rate=0.001, mini_batch_size=32):\n",
    "\n",
    "        self.epochs = epochs\n",
    "        X_test, Y_test = test_data\n",
    "        # Split data into 50% train and 50% test subsets\n",
    "        X_validation, X_test, Y_validation, Y_test = train_test_split(\n",
    "            X_test, Y_test, test_size=0.5, shuffle=False, random_state=1\n",
    "        )\n",
    "        # print(X_validation.shape)\n",
    "        # print(Y_validation.shape)\n",
    "        # print(Y_validation[0:5])\n",
    "\n",
    "        for e in range(epochs):\n",
    "            # each epoch will run through a training once and update weights\n",
    "            print(f'Running Epoch: {e+1}')\n",
    "            X_train, Y_train = training_data\n",
    "            total_data = X_train.shape[0]\n",
    "            num_of_mini_batches = total_data//mini_batch_size\n",
    "\n",
    "\n",
    "            # first we create the mini batches and then run training step through it\n",
    "            i = 1\n",
    "            batch_correct = 0\n",
    "            train_loss = 0\n",
    "            for X, Y in Utility.create_mini_batches(X=X_train, Y=Y_train, mini_batch_size=mini_batch_size):\n",
    "                \"\"\"\n",
    "                X shape --> (mini_batch_size, h, w, color_channel)\n",
    "                Y shape --> (mini_batch_size, num_of_class) (one hot encoded vector)\n",
    "                \"\"\"\n",
    "                Y_pred = self.forward_propagation(X)\n",
    "                # print('final input shape', Y_pred.shape)\n",
    "                # print('final output:', Y_pred)\n",
    "                y_pred_one_hot_encoded = Y_pred == Y_pred.max(axis=1).reshape(Y_pred.shape[0], 1)\n",
    "\n",
    "                #print('Y_pred shape: {}')\n",
    "                #print(metrics.classification_report(Y.argmax(axis=1), y_pred_one_hot_encoded.argmax(axis=1)))\n",
    "                # batch_correct = np.sum(Y.argmax(axis=1) == y_pred_one_hot_encoded.argmax(axis=1))\n",
    "                # batch_acc = batch_correct / mini_batch_size\n",
    "                # print(f'Batch correct: {batch_correct} out of {mini_batch_size}')\n",
    "                # print(f'Batch Acc: {batch_acc * 100} %')\n",
    "                self.backward_propagation(Y_pred,Y, learning_rate, mini_batch_size)\n",
    "                #train_loss = self.cost_function.compute_cost(Y_pred, Y)\n",
    "                #print(f'Loss per batch: {train_loss * 100} %')\n",
    "                print(\"\\rProgress {:1.1%}\".format(i / num_of_mini_batches), end=\"\")\n",
    "                i += 1\n",
    "\n",
    "            print()\n",
    "            self.calculate_metrics(X_train=X_train, Y_train=Y_train, X_validation=X_validation, Y_validation=Y_validation, epoch=e)\n",
    "\n",
    "            # Y_train_pred_all = self.forward_propagation(X_train)\n",
    "            # Y_train_pred_all_one_hot_encoded = Y_train_pred_all == Y_train_pred_all.max(axis=1).reshape(Y_train_pred_all.shape[0], 1)\n",
    "            #\n",
    "            # print(metrics.classification_report(Y_train.argmax(axis=1), Y_train_pred_all_one_hot_encoded.argmax(axis=1)))\n",
    "            # print('macro f1 train set: ')\n",
    "            # print(metrics.f1_score(Y_train.argmax(axis=1), Y_train_pred_all_one_hot_encoded.argmax(axis=1), average='macro'))\n",
    "            #\n",
    "            # train_loss = self.cost_function.compute_cost(Y_train_pred_all, Y_train)\n",
    "            # print(f'Train Loss After a Epoch {e+1}: {train_loss * 100} %')\n",
    "            # self.history['train_loss'].append((train_loss * 100))\n",
    "            #\n",
    "            # train_correct = np.sum(Y_train.argmax(axis=1) == Y_train_pred_all_one_hot_encoded.argmax(axis=1))\n",
    "            # train_acc = train_correct / Y_train.shape[0]\n",
    "            # print(f'Train Acc: {train_acc * 100} %')\n",
    "            # self.history['train_acc'].append((train_acc * 100))\n",
    "            #\n",
    "            # # calculate validation accuracy and loss here\n",
    "            # print('--------------Running On validation data-----------------')\n",
    "            # Y_validation_pred = self.forward_propagation(X_validation)\n",
    "            # Y__validation_pred_one_hot_encoded = Y_validation_pred == Y_validation_pred.max(axis=1).reshape(Y_validation_pred.shape[0], 1)\n",
    "            #\n",
    "            # print(metrics.classification_report(Y_validation.argmax(axis=1), Y__validation_pred_one_hot_encoded.argmax(axis=1)))\n",
    "            # print('macro f1 validation set: ')\n",
    "            # print(metrics.f1_score(Y_validation.argmax(axis=1), Y__validation_pred_one_hot_encoded.argmax(axis=1), average='macro'))\n",
    "            #\n",
    "            # validation_loss = self.cost_function.compute_cost(Y_validation_pred, Y_validation)\n",
    "            # print(f'Validation Loss After a Epoch {e+1}: {validation_loss * 100} %')\n",
    "            # self.history['validation_loss'].append((validation_loss * 100))\n",
    "            #\n",
    "            # val_correct = np.sum(Y_validation.argmax(axis=1) == Y__validation_pred_one_hot_encoded.argmax(axis=1))\n",
    "            # val_acc = val_correct / Y_validation.shape[0]\n",
    "            # print(f'Validation Acc: {val_acc * 100} %')\n",
    "            # self.history['validation_acc'].append((val_acc * 100))\n",
    "\n",
    "        print('Finished Training!')\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    def forward_propagation(self, X_train) -> np.array:\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the network\n",
    "        :param X_train: nd training tensor (batch_size, h, w, color_channel)\n",
    "        :param is_training: whether we are training or not\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        # print(f'In forward propagation: {self.layers[1].W}')\n",
    "\n",
    "        input = X_train\n",
    "        for layer in self.layers[1:]:\n",
    "            # skipping the input layer\n",
    "            #print(f'Forward for layer {layer.layer_name}')\n",
    "            if not layer.layer_name.startswith('ReLU') and not layer.layer_name.startswith('Softmax') and not layer.layer_name.startswith('Flatten'):\n",
    "                layer.toggle_training_mode() # toggling training mode for a layer\n",
    "            layer.forward(input)\n",
    "            input = layer.output_tensor # getting the output tensor of the layer to be the input tensor to the next\n",
    "            #print(f'Output of layer {layer.layer_name}\\n: {input}')\n",
    "\n",
    "            if not layer.layer_name.startswith('ReLU') and not layer.layer_name.startswith('Softmax') and not layer.layer_name.startswith('Flatten'):\n",
    "                layer.toggle_training_mode() # turning the training mode off here\n",
    "\n",
    "\n",
    "\n",
    "        return input\n",
    "\n",
    "    def backward_propagation(self, Y_out, Y, learning_rate, mini_batch_size):\n",
    "        #print('here')\n",
    "\n",
    "        # dA = (Y_out - Y) / mini_batch_size\n",
    "        # print(f'In backpropagation: {self.layers[1].W}')\n",
    "        dA = Y_out - Y\n",
    "        #print('yhat - Y', dA)\n",
    "        for layer in reversed(self.layers[1:]):\n",
    "            #print(f'Running backward for layer {layer.layer_name}')\n",
    "            if layer.is_trainable:\n",
    "                dA = layer.backward(dA, learning_rate)\n",
    "            else:\n",
    "                dA = layer.backward(dA)\n",
    "\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        epochs = range(0,self.epochs)\n",
    "        plt.plot(epochs, self.history['train_acc'])\n",
    "        plt.plot(epochs, self.history['validation_acc'])\n",
    "        plt.title('model accuracy')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'validation'], loc='upper left')\n",
    "        plt.show()\n",
    "        # summarize history for loss\n",
    "        plt.plot(epochs, self.history['train_loss'])\n",
    "        plt.plot(epochs, self.history['validation_loss'])\n",
    "        plt.title('model loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(['train', 'validation'], loc='upper left')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def calculate_metrics(self, X_train, Y_train, X_validation, Y_validation, epoch):\n",
    "        print('--------------Running On all training data-----------------')\n",
    "        Y_train_pred_all = self.forward_propagation(X_train)\n",
    "        Y_train_pred_all_one_hot_encoded = Y_train_pred_all == Y_train_pred_all.max(axis=1).reshape(Y_train_pred_all.shape[0], 1)\n",
    "\n",
    "        print(metrics.classification_report(Y_train.argmax(axis=1), Y_train_pred_all_one_hot_encoded.argmax(axis=1)))\n",
    "        print('macro f1 train set: ')\n",
    "        print(metrics.f1_score(Y_train.argmax(axis=1), Y_train_pred_all_one_hot_encoded.argmax(axis=1), average='macro'))\n",
    "\n",
    "        train_loss = self.cost_function.compute_cost(Y_train_pred_all, Y_train)\n",
    "        print(f'Train Loss After a Epoch {epoch+1}: {train_loss * 100} %')\n",
    "        self.history['train_loss'].append((train_loss * 100))\n",
    "\n",
    "        train_correct = np.sum(Y_train.argmax(axis=1) == Y_train_pred_all_one_hot_encoded.argmax(axis=1))\n",
    "        train_acc = train_correct / Y_train.shape[0]\n",
    "        print(f'Train Acc: {train_acc * 100} %')\n",
    "        self.history['train_acc'].append((train_acc * 100))\n",
    "\n",
    "        # calculate validation accuracy and loss here\n",
    "        print('--------------Running On validation data-----------------')\n",
    "        Y_validation_pred = self.forward_propagation(X_validation)\n",
    "        Y__validation_pred_one_hot_encoded = Y_validation_pred == Y_validation_pred.max(axis=1).reshape(Y_validation_pred.shape[0], 1)\n",
    "\n",
    "        print(metrics.classification_report(Y_validation.argmax(axis=1), Y__validation_pred_one_hot_encoded.argmax(axis=1)))\n",
    "        print('macro f1 validation set: ')\n",
    "        print(metrics.f1_score(Y_validation.argmax(axis=1), Y__validation_pred_one_hot_encoded.argmax(axis=1), average='macro'))\n",
    "\n",
    "        validation_loss = self.cost_function.compute_cost(Y_validation_pred, Y_validation)\n",
    "        print(f'Validation Loss After a Epoch {epoch+1}: {validation_loss * 100} %')\n",
    "        self.history['validation_loss'].append((validation_loss * 100))\n",
    "\n",
    "        val_correct = np.sum(Y_validation.argmax(axis=1) == Y__validation_pred_one_hot_encoded.argmax(axis=1))\n",
    "        val_acc = val_correct / Y_validation.shape[0]\n",
    "        print(f'Validation Acc: {val_acc * 100} %')\n",
    "        self.history['validation_acc'].append((val_acc * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.62434536 -0.61175641 -0.52817175 -1.07296862  0.86540763 -2.3015387\n",
      "   1.74481176 -0.7612069   0.3190391  -0.24937038]\n",
      " [ 1.46210794 -2.06014071 -0.3224172  -0.38405435  1.13376944 -1.09989127\n",
      "  -0.17242821 -0.87785842  0.04221375  0.58281521]]\n",
      "a_m [1.74481176 1.46210794]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets, svm, metrics\n",
    "a = np.random.randn(2,10)\n",
    "print(a)\n",
    "a_m = a.max(axis=1)\n",
    "print('a_m', a_m)\n",
    "a = a == a.max(axis=1).reshape(a.shape[0], 1)\n",
    "v = a.argmax(axis=1)\n",
    "np.sum(v == v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# np.random.seed(1)\n",
    "# A_prev = np.random.randn(10,5,7,4) # h, w, c of previous layer\n",
    "# cnn = Convolution2D(num_out_channel=8, filter_size=3, stride=2, padding_size=1)\n",
    "# cnn.initialize_output_dimensions((A_prev.shape[1], A_prev.shape[2], A_prev.shape[3]))\n",
    "# cnn.initialize_weights_biases()\n",
    "# #cnn.print_layer_dimensions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# cnn.forward_(A_prev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# print(cnn.output_tensor.shape)\n",
    "# print(\"Z'as mean =\\n\", np.mean(cnn.output_tensor))\n",
    "# print(\"Z[3,2,1] =\\n\", cnn.output_tensor[3,2,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# # Case 1: stride of 1\n",
    "# np.random.seed(1)\n",
    "# A_prev = np.random.randn(2, 5, 5, 3)\n",
    "# maxpool = MaxPool(filter_size=3, stride=1)\n",
    "# maxpool.initialize_max_pool_params((A_prev.shape[1],A_prev.shape[2],A_prev.shape[3]))\n",
    "# maxpool.forward_(A_prev)\n",
    "# maxpool.print_layer_dimensions()\n",
    "# print(maxpool.output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "(10000,)\n",
      "(10000,)\n",
      "(10000,)\n",
      "(10000,)\n",
      "(10000,)\n",
      "Shape: (50000, 32, 32, 3)\n",
      "Shape: (50000, 1)\n",
      "Shape: (10000, 32, 32, 3)\n",
      "Shape: (10000, 1)\n",
      "(50000, 32, 32, 3)\n",
      "(50000, 10)\n"
     ]
    }
   ],
   "source": [
    "cifer_dataloader = Cifer10DataLoader('/home/akil/Work/Work/Academics/4-2/ML/Assignment-3/dataset/cifer-10/cifar-10-python/cifar-10-batches-py')\n",
    "cifer_dataloader.concatenate_data()\n",
    "cifer_dataloader.preprocess_data()\n",
    "print(cifer_dataloader.data['train_images'].shape)\n",
    "print(cifer_dataloader.data['train_labels'].shape)\n",
    "# total_train_data = cifer_dataloader.data['train_images'].shape[0]\n",
    "# mini_batch_size = 32\n",
    "# p = np.random.permutation(total_data)\n",
    "# cifer_dataloader.data['train_images'], cifer_dataloader.data['train_labels'] = cifer_dataloader.data['train_images'][p, :], cifer_dataloader.data['train_labels'][p, :]\n",
    "X_train = cifer_dataloader.data['train_images']\n",
    "Y_train = cifer_dataloader.data['train_labels']\n",
    "X_test = cifer_dataloader.data['test_images']\n",
    "Y_test = cifer_dataloader.data['test_labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Testing MNIST lr:0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(60000, 10)\n",
      "(10000, 28, 28, 1)\n",
      "(10000, 10)\n"
     ]
    }
   ],
   "source": [
    "mnist = MnistDataLoader('./dataset/mnist')\n",
    "mnist.load_mnist()\n",
    "mnist.preprocess_data()\n",
    "\n",
    "X_train = mnist.data[mnist.data_list[0]]\n",
    "Y_train = mnist.data[mnist.data_list[1]]\n",
    "X_test = mnist.data[mnist.data_list[2]]\n",
    "Y_test = mnist.data[mnist.data_list[3]]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Conv 6 5 1 2', 'ReLU', 'Pool 2 2', 'Conv 12 5 1 0', 'ReLU', 'Pool 2 2', 'Conv 100 5 1 0', 'ReLU', 'FC 10', 'Softmax']\n",
      "The layer name:  Conv2D__1 2\n",
      "The layer name:  Conv2D__2 3\n",
      "The layer name:  Conv2D__3 4\n",
      "(28, 28, 6)\n",
      "(10, 10, 12)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "cnn = Model()\n",
    "cnn.add(Utility.read_model_config('./input.txt', mnist))\n",
    "cnn.compile(cost_function=CrossEntropyLoss())\n",
    "cnn.initializer_layer_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cnn.train(training_data=(X_train, Y_train), epochs=15, learning_rate=0.02, test_data=(X_test, Y_test), mini_batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Output of backward pooling non batched:  [[[[ 0.         -0.029709    0.05225134 ...  0.          0.\n",
    "    -0.03361794]\n",
    "   [-0.08076787 -0.029709    0.05225134 ... -0.09097289 -0.09927129\n",
    "    -0.03361794]\n",
    "   [ 0.          0.         -0.12472123 ...  0.          0.\n",
    "     0.        ]\n",
    "   ...\n",
    "   [ 0.          0.          0.         ...  0.          0.\n",
    "     0.        ]\n",
    "   [ 0.          0.          0.05685159 ...  0.          0.\n",
    "     0.        ]\n",
    "   [-0.19879527  0.          0.         ...  0.          0.07716001\n",
    "     0.        ]]\n",
    "\n",
    "  [[ 0.         -0.029709    0.05225134 ...  0.          0.\n",
    "    -0.03361794]\n",
    "   [ 0.         -0.029709    0.05225134 ...  0.          0.\n",
    "    -0.03361794]\n",
    "   [ 0.          0.         -0.12472123 ... -0.06503445  0.\n",
    "     0.        ]\n",
    "   ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Testing for cifer 10 data lr: 0.01 e:10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "(10000,)\n",
      "(10000,)\n",
      "(10000,)\n",
      "(10000,)\n",
      "(10000,)\n",
      "Shape: (50000, 32, 32, 3)\n",
      "Shape: (50000, 1)\n",
      "Shape: (10000, 32, 32, 3)\n",
      "Shape: (10000, 1)\n",
      "(50000, 32, 32, 3)\n",
      "(50000, 10)\n"
     ]
    }
   ],
   "source": [
    "cifer_dataloader = Cifer10DataLoader('/home/akil/Work/Work/Academics/4-2/ML/Assignment-3/dataset/cifer-10/cifar-10-python/cifar-10-batches-py')\n",
    "cifer_dataloader.concatenate_data()\n",
    "cifer_dataloader.preprocess_data()\n",
    "print(cifer_dataloader.data['train_images'].shape)\n",
    "print(cifer_dataloader.data['train_labels'].shape)\n",
    "# total_train_data = cifer_dataloader.data['train_images'].shape[0]\n",
    "# mini_batch_size = 32\n",
    "# p = np.random.permutation(total_data)\n",
    "# cifer_dataloader.data['train_images'], cifer_dataloader.data['train_labels'] = cifer_dataloader.data['train_images'][p, :], cifer_dataloader.data['train_labels'][p, :]\n",
    "X_train = cifer_dataloader.data['train_images']\n",
    "Y_train = cifer_dataloader.data['train_labels']\n",
    "X_test = cifer_dataloader.data['test_images']\n",
    "Y_test = cifer_dataloader.data['test_labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Conv 6 5 1 2', 'ReLU', 'Pool 2 2', 'Conv 12 5 1 0', 'ReLU', 'Pool 2 2', 'Conv 100 5 1 0', 'ReLU', 'FC 10', 'Softmax']\n",
      "The layer name:  Conv2D__4 5\n",
      "The layer name:  Conv2D__5 6\n",
      "The layer name:  Conv2D__6 7\n",
      "(32, 32, 6)\n",
      "(12, 12, 12)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "cnn = Model()\n",
    "cnn.add(Utility.read_model_config('./input.txt', cifer_dataloader))\n",
    "cnn.compile(cost_function=CrossEntropyLoss())\n",
    "cnn.initializer_layer_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Epoch: 1\n",
      "Progress 100.1%\n",
      "--------------Running On all training data-----------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_259067/2475788208.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_259067/282631891.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_data, test_data, epochs, learning_rate, mini_batch_size)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_validation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_validation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mY_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;31m# Y_train_pred_all = self.forward_propagation(X_train)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_259067/282631891.py\u001b[0m in \u001b[0;36mcalculate_metrics\u001b[0;34m(self, X_train, Y_train, X_validation, Y_validation, epoch)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcalculate_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_validation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'--------------Running On all training data-----------------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0mY_train_pred_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_propagation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m         \u001b[0mY_train_pred_all_one_hot_encoded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_train_pred_all\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mY_train_pred_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_train_pred_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_259067/282631891.py\u001b[0m in \u001b[0;36mforward_propagation\u001b[0;34m(self, X_train)\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ReLU'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Softmax'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Flatten'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                 \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoggle_training_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# toggling training mode for a layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m             \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensor\u001b[0m \u001b[0;31m# getting the output tensor of the layer to be the input tensor to the next\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m             \u001b[0;31m#print(f'Output of layer {layer.layer_name}\\n: {input}')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_259067/1611157823.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, Z_prev)\u001b[0m\n\u001b[1;32m     94\u001b[0m                                       ]\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_channel_index\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUtility\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_max_pool_window_over_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ_prev_windowed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_out_channel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_259067/3660619026.py\u001b[0m in \u001b[0;36mget_max_pool_window_over_batch\u001b[0;34m(Z_prev_windowed)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;31m#print('In max pool', Z_prev_windowed.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;31m#print(np.max(Z_prev_windowed, axis=(1,2)))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ_prev_windowed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.9/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2703\u001b[0m     \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2704\u001b[0m     \"\"\"\n\u001b[0;32m-> 2705\u001b[0;31m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0m\u001b[1;32m   2706\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[1;32m   2707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ai/lib/python3.9/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cnn.train(training_data=(X_train, Y_train),  epochs=10, learning_rate=0.01, test_data=(X_test, Y_test),mini_batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dense = DenseLayer(num_units=4)\n",
    "dense.W = np.array( [[ 1.4401747,   0.72498046, -0.05727674],\n",
    "                     [-1.15246919, -0.39990891,  0.44136903],\n",
    "                     [ 1.14171484, -1.41891945,  0.73059128],\n",
    "                     [ 0.60664542, -0.08249916, -1.05893566]])\n",
    "dense.W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([ [0.83351854,  -0.55429203,   0.0702855 ]])\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense.b = np.array( [[-0.64243089], [0.51146315], [-0.17120088], [1.10775354]])\n",
    "dense.b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_prev_layer shape: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "dense.forward(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3)\n",
      "(4, 1)\n",
      "(4, 1)\n",
      "[[ 0.12678218 -0.08431048  0.01069076]\n",
      " [-0.16374732  0.1088924  -0.01380781]\n",
      " [ 1.34887082 -0.89700266  0.11374199]\n",
      " [ 1.32088062 -0.8783891   0.11138175]]\n",
      "[[ 0.15210481]\n",
      " [-0.19645312]\n",
      " [ 1.61828532]\n",
      " [ 1.58470455]]\n"
     ]
    }
   ],
   "source": [
    "dz = dense.output_tensor\n",
    "dense.backward(dz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "s = np.array([[-0.39692968,  1.78981015, -0.54303206,  0.14530002,  1.42375341]])\n",
    "s = s.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax tensor shape: (1, 5)\n",
      "softmax exp shape (1, 5)\n",
      "softmax sum shape: (1, 1)\n",
      "[[0.05357302 0.47712828 0.0462908  0.09213688 0.33087103]]\n"
     ]
    }
   ],
   "source": [
    "soft = SoftmaxActivation()\n",
    "print(soft.forward(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he: paramaters:  784\n",
      "(28, 28, 6)\n",
      "he: paramaters:  1176\n",
      "(10, 10, 12)\n",
      "he: paramaters:  300\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X = np.random.randn(1,28,28,1)\n",
    "Y = np.random.randn(1,10)\n",
    "cnn = Model()\n",
    "np.random.seed(1)\n",
    "\n",
    "cnn = Model()\n",
    "cnn.add(layer_list=[\n",
    "    InputLayer(input_dimension=(mnist.size, mnist.size, mnist.color_channel), is_trainable=False, layer_name='Input'),\n",
    "    Convolution2D(num_out_channel=6, filter_size=5, stride=1, padding_size=2),\n",
    "    ReLUActivation(),\n",
    "    MaxPool(filter_size=2, stride=2),\n",
    "    Convolution2D(num_out_channel=12, filter_size=5, stride=1, padding_size=0),\n",
    "    ReLUActivation(),\n",
    "    MaxPool(filter_size=2, stride=2),\n",
    "    Convolution2D(num_out_channel=100, filter_size=5, stride=1, padding_size=0),\n",
    "    ReLUActivation(),\n",
    "    Flatten(),\n",
    "    DenseLayer(10),\n",
    "    SoftmaxActivation()\n",
    "])\n",
    "cnn.compile(optimizer=GradientDescent(), cost_function=CrossEntropyLoss())\n",
    "cnn.initializer_layer_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Epoch: 0\n",
      "Forward for layer Conv2D__1\n",
      "(1, 28, 28, 1)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (1, 28, 28, 6)\n",
      "Forward for layer MaxPool__1\n",
      "Forward for layer Conv2D__1\n",
      "(1, 14, 14, 6)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (1, 10, 10, 12)\n",
      "Forward for layer MaxPool__1\n",
      "Forward for layer Conv2D__1\n",
      "(1, 5, 5, 12)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (1, 1, 1, 100)\n",
      "Forward for layer Flatten__1\n",
      "Forward for layer Dense__1\n",
      "A_prev_layer shape: (1, 100)\n",
      "Forward for layer Softmax__1\n",
      "softmax tensor shape: (1, 10)\n",
      "softmax exp shape (1, 10)\n",
      "softmax sum shape: (1, 1)\n",
      "final input shape (1, 10)\n",
      "final output: [[0.09049115 0.06765897 0.1529529  0.08609433 0.04750961 0.10952845\n",
      "  0.10272049 0.08826287 0.12400152 0.13077972]]\n",
      "Loss is :  -6.485603025596568\n",
      "Progress 100.0%\n",
      "\n",
      "Cost After a Epoch 1: -648.5603025596567\n",
      "Running Epoch: 1\n",
      "Forward for layer Conv2D__1\n",
      "(1, 28, 28, 1)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (1, 28, 28, 6)\n",
      "Forward for layer MaxPool__1\n",
      "Forward for layer Conv2D__1\n",
      "(1, 14, 14, 6)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (1, 10, 10, 12)\n",
      "Forward for layer MaxPool__1\n",
      "Forward for layer Conv2D__1\n",
      "(1, 5, 5, 12)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (1, 1, 1, 100)\n",
      "Forward for layer Flatten__1\n",
      "Forward for layer Dense__1\n",
      "A_prev_layer shape: (1, 100)\n",
      "Forward for layer Softmax__1\n",
      "softmax tensor shape: (1, 10)\n",
      "softmax exp shape (1, 10)\n",
      "softmax sum shape: (1, 1)\n",
      "final input shape (1, 10)\n",
      "final output: [[0.09049115 0.06765897 0.1529529  0.08609433 0.04750961 0.10952845\n",
      "  0.10272049 0.08826287 0.12400152 0.13077972]]\n",
      "Loss is :  -6.485603025596568\n",
      "Progress 100.0%\n",
      "\n",
      "Cost After a Epoch 2: -648.5603025596567\n",
      "Running Epoch: 2\n",
      "Forward for layer Conv2D__1\n",
      "(1, 28, 28, 1)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (1, 28, 28, 6)\n",
      "Forward for layer MaxPool__1\n",
      "Forward for layer Conv2D__1\n",
      "(1, 14, 14, 6)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (1, 10, 10, 12)\n",
      "Forward for layer MaxPool__1\n",
      "Forward for layer Conv2D__1\n",
      "(1, 5, 5, 12)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (1, 1, 1, 100)\n",
      "Forward for layer Flatten__1\n",
      "Forward for layer Dense__1\n",
      "A_prev_layer shape: (1, 100)\n",
      "Forward for layer Softmax__1\n",
      "softmax tensor shape: (1, 10)\n",
      "softmax exp shape (1, 10)\n",
      "softmax sum shape: (1, 1)\n",
      "final input shape (1, 10)\n",
      "final output: [[0.09049115 0.06765897 0.1529529  0.08609433 0.04750961 0.10952845\n",
      "  0.10272049 0.08826287 0.12400152 0.13077972]]\n",
      "Loss is :  -6.485603025596568\n",
      "Progress 100.0%\n",
      "\n",
      "Cost After a Epoch 3: -648.5603025596567\n",
      "Running Epoch: 3\n",
      "Forward for layer Conv2D__1\n",
      "(1, 28, 28, 1)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (1, 28, 28, 6)\n",
      "Forward for layer MaxPool__1\n",
      "Forward for layer Conv2D__1\n",
      "(1, 14, 14, 6)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (1, 10, 10, 12)\n",
      "Forward for layer MaxPool__1\n",
      "Forward for layer Conv2D__1\n",
      "(1, 5, 5, 12)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (1, 1, 1, 100)\n",
      "Forward for layer Flatten__1\n",
      "Forward for layer Dense__1\n",
      "A_prev_layer shape: (1, 100)\n",
      "Forward for layer Softmax__1\n",
      "softmax tensor shape: (1, 10)\n",
      "softmax exp shape (1, 10)\n",
      "softmax sum shape: (1, 1)\n",
      "final input shape (1, 10)\n",
      "final output: [[0.09049115 0.06765897 0.1529529  0.08609433 0.04750961 0.10952845\n",
      "  0.10272049 0.08826287 0.12400152 0.13077972]]\n",
      "Loss is :  -6.485603025596568\n",
      "Progress 100.0%\n",
      "\n",
      "Cost After a Epoch 4: -648.5603025596567\n",
      "Running Epoch: 4\n",
      "Forward for layer Conv2D__1\n",
      "(1, 28, 28, 1)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (1, 28, 28, 6)\n",
      "Forward for layer MaxPool__1\n",
      "Forward for layer Conv2D__1\n",
      "(1, 14, 14, 6)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (1, 10, 10, 12)\n",
      "Forward for layer MaxPool__1\n",
      "Forward for layer Conv2D__1\n",
      "(1, 5, 5, 12)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (1, 1, 1, 100)\n",
      "Forward for layer Flatten__1\n",
      "Forward for layer Dense__1\n",
      "A_prev_layer shape: (1, 100)\n",
      "Forward for layer Softmax__1\n",
      "softmax tensor shape: (1, 10)\n",
      "softmax exp shape (1, 10)\n",
      "softmax sum shape: (1, 1)\n",
      "final input shape (1, 10)\n",
      "final output: [[0.09049115 0.06765897 0.1529529  0.08609433 0.04750961 0.10952845\n",
      "  0.10272049 0.08826287 0.12400152 0.13077972]]\n",
      "Loss is :  -6.485603025596568\n",
      "Progress 100.0%\n",
      "\n",
      "Cost After a Epoch 5: -648.5603025596567\n",
      "Finish Training!\n"
     ]
    }
   ],
   "source": [
    "cnn.train(training_data=(X, Y), validation_data=(None, None),mini_batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conv_forward' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_102397/2488773407.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m hparameters = {\"pad\" : 2,\n\u001b[1;32m      8\u001b[0m                \"stride\": 2}\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_conv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Test conv_backward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'conv_forward' is not defined"
     ]
    }
   ],
   "source": [
    "# We'll run conv_forward to initialize the 'Z' and 'cache_conv\",\n",
    "# which we'll use to test the conv_backward function\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10,4,4,3)\n",
    "W = np.random.randn(2,2,3,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "hparameters = {\"pad\" : 2,\n",
    "               \"stride\": 2}\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "\n",
    "# Test conv_backward\n",
    "dA, dW, db = conv_backward_(Z, cache_conv)\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10,4,4,3) # h, w, c of previous layer\n",
    "cnn = Convolution2D(num_out_channel=8, filter_size=2, stride=2, padding_size=2)\n",
    "cnn.initialize_output_dimensions((A_prev.shape[1], A_prev.shape[2], A_prev.shape[3]))\n",
    "cnn.initialize_weights_biases()\n",
    "cnn.forward(A_prev)\n",
    "Z = cnn.get_output_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Test conv_backward\n",
    "dA, dW, db = cnn.backward(Z)\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Case 1: stride of 1\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "maxpool = MaxPool(filter_size=2, stride=1)\n",
    "maxpool.initialize_max_pool_params((A_prev.shape[1],A_prev.shape[2],A_prev.shape[3]))\n",
    "maxpool.forward(A_prev)\n",
    "A = maxpool.get_output_tensor()\n",
    "print('shape of A',A.shape)\n",
    "\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "print(dA[0,0,0,0])\n",
    "out = maxpool.backward(dA)\n",
    "print(\"mode = max\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', out[1,1])\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Case 1: stride of 1\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "maxpool = MaxPool(filter_size=2, stride=1)\n",
    "maxpool.initialize_max_pool_params((A_prev.shape[1],A_prev.shape[2],A_prev.shape[3]))\n",
    "maxpool.forward(A_prev)\n",
    "A = maxpool.get_output_tensor()\n",
    "print('shape of A',A.shape)\n",
    "\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "print(dA[0,0,0,0])\n",
    "out = maxpool.backward(dA)\n",
    "\n",
    "print(out.shape)\n",
    "print(\"mode = max\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', out[1,1])\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
