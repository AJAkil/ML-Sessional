{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import os\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "class MnistDataLoader:\n",
    "    def __init__(self, data_folder_path):\n",
    "        self.data_folder_path = data_folder_path\n",
    "        self.train_file_name = 'train-images-idx3-ubyte.gz'\n",
    "        self.train_label_file_name = 'train-labels-idx1-ubyte.gz'\n",
    "        self.test_file_name = 't10k-images-idx3-ubyte.gz'\n",
    "        self.test_label_file_name = 't10k-labels-idx1-ubyte.gz'\n",
    "        self.data = dict()\n",
    "        self.size = 28\n",
    "        self.color_channel = 1\n",
    "        self.data_list = [\n",
    "            'train_images',\n",
    "            'train_labels',\n",
    "            'test_images',\n",
    "            'test_labels'\n",
    "        ]\n",
    "\n",
    "    def load_images(self, data_list_index, file_name):\n",
    "        images = gzip.open(os.path.join(self.data_folder_path, file_name), 'rb')\n",
    "        self.data[self.data_list[data_list_index]] = np.frombuffer(images.read(), dtype=np.uint8, offset=16).reshape(-1, self.size, self.size)\n",
    "        self.data[self.data_list[data_list_index]] = self.data[self.data_list[data_list_index]].reshape(self.data[self.data_list[data_list_index]].shape[0], self.size, self.size, self.color_channel).astype(np.float32)\n",
    "\n",
    "    def load_labels(self, data_list_index, file_name):\n",
    "        labels = gzip.open(os.path.join(self.data_folder_path, file_name), 'rb')\n",
    "        self.data[self.data_list[data_list_index]] = np.frombuffer(labels.read(), dtype=np.uint8, offset=8)\n",
    "        self.data[self.data_list[data_list_index]].resize(self.data[self.data_list[data_list_index]].shape[0],1)\n",
    "\n",
    "    def load_mnist(self):\n",
    "        self.load_images(data_list_index=0, file_name=self.train_file_name)\n",
    "        self.load_labels(data_list_index=1, file_name=self.train_label_file_name)\n",
    "        self.load_images(data_list_index=2, file_name=self.test_file_name)\n",
    "        self.load_labels(data_list_index=3, file_name=self.test_label_file_name)\n",
    "\n",
    "        self.assert_data_shape()\n",
    "\n",
    "    def assert_data_shape(self):\n",
    "        assert self.data[self.data_list[0]].shape == (60000, 28, 28, 1)\n",
    "        assert self.data[self.data_list[1]].shape == (60000, 1)\n",
    "        assert self.data[self.data_list[2]].shape == (10000, 28, 28, 1)\n",
    "        assert self.data[self.data_list[3]].shape == (10000, 1)\n",
    "\n",
    "    def preprocess_data(self):\n",
    "\n",
    "        self.data[self.data_list[0]] /= 255\n",
    "        self.data[self.data_list[2]] /= 255\n",
    "\n",
    "        self.data[self.data_list[1]] = Utility.one_hot_encode(self.data[self.data_list[1]])\n",
    "\n",
    "        assert self.data[self.data_list[1]].shape == (60000, 10)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "class Cifer10DataLoader:\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "class Convolution2D:\n",
    "    def __init__(self, num_out_channel, filter_size, stride, padding_size):\n",
    "        self.num_out_channel = num_out_channel\n",
    "        self.filter_size = filter_size\n",
    "        self.stride = stride\n",
    "        self.padding_size = padding_size\n",
    "        self.h_prev, self.w_prev, self.num_channel_prev = None, None, None\n",
    "        self.h_new, self.w_new = None, None\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.output_tensor = None\n",
    "        self.cache = {}\n",
    "        self.relu_activation = ReLUActivation()\n",
    "\n",
    "    def initialize_output_dimensions(self, prev_layer_output_dim):\n",
    "        \"\"\"\n",
    "        Initializes output dimensions with the dimension of the previous layers\n",
    "        :param prev_layer_output_dim: output dimension of the layer immediately before this layer\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.h_prev, self.w_prev , self.num_channel_prev = prev_layer_output_dim\n",
    "        self.h_new = (self.h_prev - self.filter_size + 2 * self.padding_size) // self.stride + 1\n",
    "        self.w_new = (self.w_prev - self.filter_size + 2 * self.padding_size) // self.stride + 1\n",
    "        self.output_tensor = np.zeros((self.h_new, self.w_new, self.num_out_channel))\n",
    "\n",
    "    def initialize_weights_biases(self):\n",
    "        \"\"\"\n",
    "        Initializes weights with the proper dimensions\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.W = np.random.randn(self.filter_size, self.filter_size, self.num_channel_prev, self.num_out_channel)\n",
    "        self.b = np.random.randn(1, 1, 1, self.num_out_channel)\n",
    "\n",
    "    def forward(self, Z_prev, is_training):\n",
    "        \"\"\"\n",
    "        Performs a forward operation of the convolution layer\n",
    "        :param Z_prev: The activation of the previous layer\n",
    "        :param is_training: whether we are in training mode or not\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        assert Z_prev.shape == (self.h_prev, self.w_prev, self.num_channel_prev)\n",
    "        Z_prev = np.array(Z_prev, copy=True)\n",
    "\n",
    "        # create zero padded Z_prev\n",
    "        Z_prev_padded = Utility.zero_pad(Z_prev, self.padding_size)\n",
    "\n",
    "        # Apply convolution operation over this zero padded previous activation\n",
    "        for row in range(self.h_new):\n",
    "\n",
    "            row_start = row * self.stride\n",
    "\n",
    "            for col in range(self.w_new):\n",
    "\n",
    "                col_start = col *  self.stride\n",
    "\n",
    "                for output_channel_index in range(self.num_out_channel):\n",
    "\n",
    "                    Z_prev_windowed = Z_prev_padded[\n",
    "                        row_start : row_start + self.filter_size,\n",
    "                        col_start : col_start + self.filter_size,\n",
    "                        :\n",
    "                    ]\n",
    "\n",
    "                    conv_step_W = self.W[:, :, :, output_channel_index]\n",
    "                    conv_step_b = self.b[:, :, :, output_channel_index]\n",
    "\n",
    "                    self.output_tensor[row, col, output_channel_index] = Utility.convolve_single_step(Z_prev_windowed, conv_step_W, conv_step_b)\n",
    "\n",
    "        # asserting output shape\n",
    "        assert(self.output_tensor.shape == (self.h_new, self.w_new, self.num_out_channel))\n",
    "\n",
    "        if is_training:\n",
    "            # cache some values\n",
    "            pass\n",
    "\n",
    "        # perform activation element wise in this case\n",
    "        self.output_tensor = self.relu_activation.activation_f(self.output_tensor)\n",
    "\n",
    "    def backward(self):\n",
    "        pass\n",
    "\n",
    "    def update_CNN_parameters(self, dW, db):\n",
    "        self.W = self.W - dW\n",
    "        self.b = self.b - db\n",
    "\n",
    "    def print_layer_dimensions(self):\n",
    "        print(f'Output Tensor Dimensions: {self.output_tensor.shape}')\n",
    "        print(f'Weight Dimension: {self.W.shape}')\n",
    "        print(f'Bias Dimension: {self.b.shape}')\n",
    "\n",
    "    def get_output_dimension(self):\n",
    "        return self.output_tensor.shape\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "class MaxPool:\n",
    "    def __init__(self, filter_size, stride):\n",
    "        self.filter_size = filter_size\n",
    "        self.stride = stride\n",
    "        self.h_prev, self.w_prev, self.num_channel_prev = None, None, None\n",
    "        self.h_new, self.w_new, self.num_out_channel = None, None, None\n",
    "        self.output_tensor = None\n",
    "        self.cache = {}\n",
    "\n",
    "    def initialize_max_pool_params(self, prev_layer_output_dim):\n",
    "        \"\"\"\n",
    "        Initializes output dimensions with the dimension of the previous layers\n",
    "        :param prev_layer_output_dim: output dimension of the layer immediately before this layer\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.h_prev, self.w_prev , self.num_channel_prev = prev_layer_output_dim\n",
    "        self.h_new = int((self.h_prev - self.filter_size) / self.stride + 1)\n",
    "        self.w_new = int((self.w_prev - self.filter_size) / self.stride + 1)\n",
    "        self.num_out_channel = self.num_channel_prev\n",
    "        print(self.h_new)\n",
    "        print(self.w_new)\n",
    "        print(self.num_out_channel)\n",
    "        self.output_tensor = np.zeros((self.h_new, self.w_new, self.num_out_channel))\n",
    "\n",
    "    def forward(self, Z_prev, is_training):\n",
    "\n",
    "        assert Z_prev.shape == (self.h_prev, self.w_prev, self.num_channel_prev)\n",
    "        Z_prev = np.array(Z_prev, copy=True)\n",
    "\n",
    "        # Apply convolution operation over this zero padded previous activation\n",
    "        for row in range(self.h_new):\n",
    "\n",
    "            row_start = row * self.stride\n",
    "\n",
    "            for col in range(self.w_new):\n",
    "\n",
    "                col_start = col *  self.stride\n",
    "\n",
    "                for output_channel_index in range(self.num_out_channel):\n",
    "\n",
    "                    Z_prev_windowed = Z_prev[\n",
    "                                      row_start : row_start + self.filter_size,\n",
    "                                      col_start : col_start + self.filter_size,\n",
    "                                      output_channel_index\n",
    "                                      ]\n",
    "\n",
    "                    self.output_tensor[row, col, output_channel_index] = Utility.get_max_pool_window(Z_prev_windowed)\n",
    "\n",
    "        assert self.output_tensor.shape == (self.h_new, self.w_new, self.num_out_channel)\n",
    "        if is_training:\n",
    "            pass\n",
    "\n",
    "    def print_layer_dimensions(self):\n",
    "        print(f'Output Tensor Dimensions: {self.output_tensor.shape}')\n",
    "\n",
    "    def backward(self):\n",
    "        pass\n",
    "\n",
    "    def get_output_dimension(self):\n",
    "        return self.output_tensor.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "    def __init__(self, num_units, activation_obj):\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.num_units = num_units\n",
    "        self.cache = {}\n",
    "        self.output_tensor = None\n",
    "        self.activation_obj = activation_obj\n",
    "\n",
    "    def initialize_weights_biases(self, prev_flat_layer_output_dim):\n",
    "        self.W = np.random.randn(self.num_units, prev_flat_layer_output_dim)\n",
    "        self.b = np.random.randn(1, self.num_units)\n",
    "\n",
    "    def forward(self, Z_prev):\n",
    "        \"\"\"\n",
    "\n",
    "        :param Z_prev: tensor of shape (1, prev_flattened_shape)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        assert Z_prev.shape[1] == self.W.shape[1]\n",
    "\n",
    "        Z_prev = np.array(Z_prev, copy=True)\n",
    "        self.output_tensor = self.activation_obj.activation_f(np.dot(Z_prev, np.transpose(self.W)) + self.b)\n",
    "\n",
    "        assert self.output_tensor.shape == (1, self.num_units)\n",
    "\n",
    "    def backward(self):\n",
    "        pass\n",
    "\n",
    "    def get_output_dimension(self):\n",
    "        return self.output_tensor.shape\n",
    "\n",
    "    def print_layer_dimensions(self):\n",
    "        print(f'Output Tensor Dimensions: {self.output_tensor.shape}')\n",
    "        print(f'Weight Dimension: {self.W.shape}')\n",
    "        print(f'Bias Dimension: {self.b.shape}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "class Utility:\n",
    "\n",
    "    @staticmethod\n",
    "    def one_hot_encode(y_true):\n",
    "        # Define the One-hot Encoder\n",
    "        ohe = preprocessing.OneHotEncoder()\n",
    "        ohe.fit(y_true)\n",
    "        y_true = ohe.transform(y_true).toarray()\n",
    "        return y_true\n",
    "\n",
    "    @staticmethod\n",
    "    def zero_pad(tensor, pad_size):\n",
    "        \"\"\"\n",
    "        :param tensor: tensor of shape (h, w, num_channel)\n",
    "        :return: padded tensor of shape (h + 2 * pad_size, w + 2 * pad_size, num_channel)\n",
    "        \"\"\"\n",
    "        return np.pad(tensor, ((pad_size, pad_size), (pad_size, pad_size), (0,0)), mode='constant', constant_values=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def convolve_single_step(Z_prev_windowed, W, b):\n",
    "        \"\"\"\n",
    "        :param Z_prev_windowed: window of shape (F, F, num_channel_Z_prev)\n",
    "        :param W: kernel/filter/weight of shape (F, F, num_channel_Z_prev)\n",
    "        :param b: bias term of shape (1, 1, 1)\n",
    "        :return: scaler convolved value\n",
    "        \"\"\"\n",
    "        return np.multiply(Z_prev_windowed, W).sum() + float(b)\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_max_pool_window(Z_prev_windowed):\n",
    "        return Z_prev_windowed.max()\n",
    "\n",
    "    def create_mini_batches(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "class ReLUActivation:\n",
    "    @staticmethod\n",
    "    def activation_f(tensor):\n",
    "        return np.max(tensor, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def d_relu(tensor):\n",
    "        return np.where(tensor > 0, 1, 0)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def compile(self):\n",
    "        pass\n",
    "\n",
    "    def train(self):\n",
    "        pass\n",
    "\n",
    "    def predict(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "mnist = MnistDataLoader('./dataset/mnist')\n",
    "mnist.load_mnist()\n",
    "mnist.preprocess_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.data[mnist.data_list[1]][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "(3, 3, 3)"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.random.randn(3,3,3,5)\n",
    "a[:,:,:,1].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Tensor Dimensions: (3, 4, 8)\n",
      "Weight Dimension: (3, 3, 4, 8)\n",
      "Bias Dimension: (1, 1, 1, 8)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5,7,4) # h, w, c of previous layer\n",
    "cnn = Convolution2D(num_out_channel=8, filter_size=3, stride=2, padding_size=1)\n",
    "cnn.initialize_output_dimensions(A_prev.shape)\n",
    "cnn.initialize_weights_biases()\n",
    "cnn.print_layer_dimensions()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "cnn.forward(A_prev, is_training=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z's mean =\n",
      " 0.7671068812283951\n",
      "Z[3,2,1] =\n",
      " [-2.05981919  0.8841739   2.44456907 -4.05359906  5.20914997  1.40909525\n",
      "  5.19845828  0.4675558 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Z's mean =\\n\", np.mean(cnn.output_tensor))\n",
    "print(\"Z[3,2,1] =\\n\", cnn.output_tensor[2,1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "3\n",
      "3\n",
      "Output Tensor Dimensions: (3, 3, 3)\n",
      "[[[1.74481176 0.90159072 1.65980218]\n",
      "  [1.74481176 1.46210794 1.65980218]\n",
      "  [1.74481176 1.6924546  1.65980218]]\n",
      "\n",
      " [[1.14472371 0.90159072 2.10025514]\n",
      "  [1.14472371 0.90159072 1.65980218]\n",
      "  [1.14472371 1.6924546  1.65980218]]\n",
      "\n",
      " [[1.13162939 1.51981682 2.18557541]\n",
      "  [1.13162939 1.51981682 2.18557541]\n",
      "  [1.13162939 1.6924546  2.18557541]]]\n"
     ]
    }
   ],
   "source": [
    "# Case 1: stride of 1\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3)\n",
    "maxpool = MaxPool(filter_size=3, stride=1)\n",
    "maxpool.initialize_max_pool_params(A_prev.shape)\n",
    "maxpool.forward(A_prev, True)\n",
    "maxpool.print_layer_dimensions()\n",
    "print(maxpool.output_tensor)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "hparameters = {\"stride\" : 1, \"f\": 3}\n",
    "\n",
    "A, cache = pool_forward(A_prev, hparameters)\n",
    "print(\"mode = max\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A =\\n\", A)\n",
    "print()\n",
    "A, cache = pool_forward(A_prev, hparameters, mode = \"average\")\n",
    "print(\"mode = average\")\n",
    "print(\"A.shape = \" + str(A.shape))\n",
    "print(\"A =\\n\", A)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ True,  True])"
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,23])\n",
    "b = np.array(a, copy=True)\n",
    "b == a"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}