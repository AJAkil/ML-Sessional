{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "from typing import Tuple\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "outputs": [],
   "source": [
    "class MnistDataLoader:\n",
    "    def __init__(self, data_folder_path):\n",
    "        self.data_folder_path = data_folder_path\n",
    "        self.train_file_name = 'train-images-idx3-ubyte.gz'\n",
    "        self.train_label_file_name = 'train-labels-idx1-ubyte.gz'\n",
    "        self.test_file_name = 't10k-images-idx3-ubyte.gz'\n",
    "        self.test_label_file_name = 't10k-labels-idx1-ubyte.gz'\n",
    "        self.data = dict()\n",
    "        self.size = 28\n",
    "        self.color_channel = 1\n",
    "        self.data_list = [\n",
    "            'train_images',\n",
    "            'train_labels',\n",
    "            'test_images',\n",
    "            'test_labels'\n",
    "        ]\n",
    "\n",
    "    def load_images(self, data_list_index, file_name):\n",
    "        images = gzip.open(os.path.join(self.data_folder_path, file_name), 'rb')\n",
    "        self.data[self.data_list[data_list_index]] = np.frombuffer(images.read(), dtype=np.uint8, offset=16).reshape(-1, self.size, self.size)\n",
    "        self.data[self.data_list[data_list_index]] = self.data[self.data_list[data_list_index]].reshape(self.data[self.data_list[data_list_index]].shape[0], self.size, self.size, self.color_channel).astype(np.float32)\n",
    "\n",
    "    def load_labels(self, data_list_index, file_name):\n",
    "        labels = gzip.open(os.path.join(self.data_folder_path, file_name), 'rb')\n",
    "        self.data[self.data_list[data_list_index]] = np.frombuffer(labels.read(), dtype=np.uint8, offset=8)\n",
    "        self.data[self.data_list[data_list_index]].resize(self.data[self.data_list[data_list_index]].shape[0],1)\n",
    "\n",
    "    def load_mnist(self):\n",
    "        self.load_images(data_list_index=0, file_name=self.train_file_name)\n",
    "        self.load_labels(data_list_index=1, file_name=self.train_label_file_name)\n",
    "        self.load_images(data_list_index=2, file_name=self.test_file_name)\n",
    "        self.load_labels(data_list_index=3, file_name=self.test_label_file_name)\n",
    "\n",
    "        self.assert_data_shape()\n",
    "\n",
    "    def assert_data_shape(self):\n",
    "        assert self.data[self.data_list[0]].shape == (60000, 28, 28, 1)\n",
    "        assert self.data[self.data_list[1]].shape == (60000, 1)\n",
    "        assert self.data[self.data_list[2]].shape == (10000, 28, 28, 1)\n",
    "        assert self.data[self.data_list[3]].shape == (10000, 1)\n",
    "\n",
    "    def preprocess_data(self):\n",
    "\n",
    "        self.data[self.data_list[0]] /= 255\n",
    "        self.data[self.data_list[2]] /= 255\n",
    "\n",
    "        self.data[self.data_list[1]] = Utility.one_hot_encode(self.data[self.data_list[1]])\n",
    "\n",
    "        assert self.data[self.data_list[1]].shape == (60000, 10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "outputs": [],
   "source": [
    "class Cifer10DataLoader:\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.size = 32\n",
    "        self.color_channel = 3\n",
    "        self.per_batch_data_size = 10000\n",
    "        self.data = dict()\n",
    "\n",
    "    def load_data(self, file_name):\n",
    "        with open(os.path.join(self.data_path, file_name), 'rb') as f:\n",
    "\n",
    "            data_dict=pickle.load(f, encoding='latin1')\n",
    "\n",
    "            images = data_dict['data']\n",
    "            labels = data_dict['labels']\n",
    "\n",
    "            images = images.reshape(self.per_batch_data_size, self.color_channel, self.size, self.size).transpose(0,2,3,1).astype(\"float\")\n",
    "            labels = np.array(labels)\n",
    "            print(labels.shape)\n",
    "\n",
    "            return images, labels\n",
    "\n",
    "    def concatenate_data(self):\n",
    "        X1, Y1 = self.load_data('data_batch_1')\n",
    "        X2, Y2 = self.load_data('data_batch_2')\n",
    "        X3, Y3 = self.load_data('data_batch_3')\n",
    "        X4, Y4 = self.load_data('data_batch_4')\n",
    "        X5, Y5 = self.load_data('data_batch_5')\n",
    "\n",
    "        self.data['train_images'] = np.concatenate(\n",
    "            (\n",
    "                X1, X2, X3, X4, X5\n",
    "            ),\n",
    "            axis=0\n",
    "        )\n",
    "\n",
    "        self.data['train_labels'] = np.concatenate(\n",
    "            (\n",
    "                Y1.reshape(self.per_batch_data_size, 1),\n",
    "                Y2.reshape(self.per_batch_data_size, 1),\n",
    "                Y3.reshape(self.per_batch_data_size, 1),\n",
    "                Y4.reshape(self.per_batch_data_size, 1),\n",
    "                Y5.reshape(self.per_batch_data_size, 1)\n",
    "            ),\n",
    "            axis=0\n",
    "        )\n",
    "\n",
    "        X_test, Y_test = self.load_data('test_batch')\n",
    "\n",
    "        self.data['test_images'] = X_test\n",
    "        self.data['test_labels'] = Y_test.reshape(Y_test.shape[0], 1)\n",
    "\n",
    "        self.assert_data_shape()\n",
    "\n",
    "        for key, data in self.data.items():\n",
    "            print(f'Shape: {data.shape}')\n",
    "\n",
    "    def assert_data_shape(self):\n",
    "        assert self.data['train_images'].shape == (50000, 32, 32, 3)\n",
    "        assert self.data['train_labels'].shape == (50000, 1)\n",
    "        assert self.data['test_images'].shape  == (10000, 32, 32, 3)\n",
    "        assert self.data['test_labels'].shape  == (10000, 1)\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        self.data['train_images'] /= 255\n",
    "        self.data['test_images'] /= 255\n",
    "\n",
    "        self.data['train_labels'] = Utility.one_hot_encode(self.data['train_labels'])\n",
    "\n",
    "        assert self.data['train_labels'].shape == (50000, 10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "outputs": [],
   "source": [
    "class Convolution2D:\n",
    "\n",
    "    # class variable\n",
    "    layer_num = 1\n",
    "\n",
    "    def __init__(self, num_out_channel, filter_size, stride, padding_size, activation: \"ReLUActivation\"):\n",
    "        self.num_out_channel = num_out_channel\n",
    "        self.filter_size = filter_size\n",
    "        self.stride = stride\n",
    "        self.padding_size = padding_size\n",
    "        self.h_prev, self.w_prev, self.num_channel_prev = None, None, None\n",
    "        self.h_new, self.w_new = None, None\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.output_tensor = None\n",
    "        self.cache = {}\n",
    "        self.relu_activation = activation\n",
    "        self.layer_name = 'Conv2D__' + str(self.layer_num + 1)\n",
    "        self.layer_num += 1\n",
    "        self.is_trainable = True\n",
    "\n",
    "    def initialize_output_dimensions(self, prev_layer_output_dim):\n",
    "        \"\"\"\n",
    "        Initializes output dimensions with the dimension of the previous layers\n",
    "        :param prev_layer_output_dim: output dimension of the layer immediately before this layer\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.h_prev, self.w_prev , self.num_channel_prev = prev_layer_output_dim\n",
    "        self.h_new = (self.h_prev - self.filter_size + 2 * self.padding_size) // self.stride + 1\n",
    "        self.w_new = (self.w_prev - self.filter_size + 2 * self.padding_size) // self.stride + 1\n",
    "\n",
    "    def initialize_weights_biases(self):\n",
    "        \"\"\"\n",
    "        Initializes weights with the proper dimensions\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        self.W = np.random.randn(self.filter_size, self.filter_size, self.num_channel_prev, self.num_out_channel)\n",
    "        self.b = np.random.randn(1, 1, 1, self.num_out_channel)\n",
    "\n",
    "    def forward(self, Z_prev, is_training):\n",
    "        \"\"\"\n",
    "        Performs a forward operation of the convolution layer\n",
    "        :param Z_prev: The activation of the previous layer\n",
    "        :param is_training: whether we are in training mode or not\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        assert Z_prev.shape == (self.h_prev, self.w_prev, self.num_channel_prev)\n",
    "        Z_prev = np.array(Z_prev, copy=True)\n",
    "\n",
    "        # create zero padded Z_prev\n",
    "        Z_prev_padded = Utility.zero_pad_without_batch(Z_prev, self.padding_size)\n",
    "        self.output_tensor = np.zeros((self.h_new, self.w_new, self.num_out_channel))\n",
    "\n",
    "        # Apply convolution operation over this zero padded previous activation\n",
    "        for row in range(self.h_new):\n",
    "\n",
    "            row_start = row * self.stride\n",
    "\n",
    "            for col in range(self.w_new):\n",
    "\n",
    "                col_start = col *  self.stride\n",
    "\n",
    "                for output_channel_index in range(self.num_out_channel):\n",
    "\n",
    "                    Z_prev_windowed = Z_prev_padded[\n",
    "                        row_start : row_start + self.filter_size,\n",
    "                        col_start : col_start + self.filter_size,\n",
    "                        :\n",
    "                    ]\n",
    "\n",
    "                    conv_step_W = self.W[:, :, :, output_channel_index]\n",
    "                    conv_step_b = self.b[:, :, :, output_channel_index]\n",
    "\n",
    "                    self.output_tensor[row, col, output_channel_index] = Utility.convolve_single_step(Z_prev_windowed, conv_step_W, conv_step_b)\n",
    "\n",
    "        # asserting output shape\n",
    "        assert(self.output_tensor.shape == (self.h_new, self.w_new, self.num_out_channel))\n",
    "\n",
    "        if is_training:\n",
    "            # cache some values\n",
    "            pass\n",
    "\n",
    "        # perform activation element wise in this case\n",
    "        print(f'In forward of Convolution output tensor shape before relu {self.output_tensor.shape}')\n",
    "        self.output_tensor = self.relu_activation.activation_f(self.output_tensor)\n",
    "\n",
    "        # asserting output shape\n",
    "        #assert(self.output_tensor.shape == (self.h_new, self.w_new, self.num_out_channel))\n",
    "        print(f'In forward of CNN output tensor shape after relu {self.output_tensor.shape}')\n",
    "\n",
    "    def forward_batch(self, Z_prev, is_training=True):\n",
    "        \"\"\"\n",
    "        Performs a forward operation of the convolution layer\n",
    "        :param Z_prev: The activation of the previous layer\n",
    "        :param is_training: whether we are in training mode or not\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        batch_size = Z_prev.shape[0]\n",
    "        assert Z_prev.shape == (batch_size, self.h_prev, self.w_prev, self.num_channel_prev)\n",
    "\n",
    "        self.output_tensor = np.zeros((batch_size, self.h_new, self.w_new, self.num_out_channel))\n",
    "        Z_prev = np.array(Z_prev, copy=True)\n",
    "\n",
    "        # create zero padded Z_prev\n",
    "        print(f'Z prev shape:{Z_prev.shape}')\n",
    "        Z_prev_padded = Utility.zero_pad(Z_prev, self.padding_size)\n",
    "\n",
    "        # Apply convolution operation over this zero padded previous activation\n",
    "        for image_index in range(batch_size):\n",
    "            current_Z_prev_padded = Z_prev_padded[image_index] # choosing a single tensor from the batch\n",
    "            for row in range(self.h_new):\n",
    "\n",
    "                row_start = row * self.stride\n",
    "\n",
    "                for col in range(self.w_new):\n",
    "\n",
    "                    col_start = col *  self.stride\n",
    "\n",
    "                    for output_channel_index in range(self.num_out_channel):\n",
    "\n",
    "                        Z_prev_windowed = current_Z_prev_padded[\n",
    "                                        row_start : row_start + self.filter_size,\n",
    "                                        col_start : col_start + self.filter_size,\n",
    "                                        :\n",
    "                                        ]\n",
    "\n",
    "                        conv_step_W = self.W[:, :, :, output_channel_index]\n",
    "                        conv_step_b = self.b[:, :, :, output_channel_index]\n",
    "\n",
    "                        self.output_tensor[image_index, row, col, output_channel_index] = Utility.convolve_single_step(Z_prev_windowed, conv_step_W, conv_step_b)\n",
    "\n",
    "        # asserting output shape\n",
    "        assert(self.output_tensor.shape == (batch_size, self.h_new, self.w_new, self.num_out_channel))\n",
    "        print(self.output_tensor.shape)\n",
    "\n",
    "        if is_training:\n",
    "            # cache some values\n",
    "            pass\n",
    "\n",
    "        # perform activation element wise in this case\n",
    "        self.output_tensor = self.relu_activation.activation_f(self.output_tensor)\n",
    "\n",
    "    def forward_batch_i(self, Z_prev, is_training=True):\n",
    "        \"\"\"\n",
    "        Performs a forward operation of the convolution layer\n",
    "        :param Z_prev: The activation of the previous layer\n",
    "        :param is_training: whether we are in training mode or not\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        batch_size = Z_prev.shape[0]\n",
    "        assert Z_prev.shape == (batch_size, self.h_prev, self.w_prev, self.num_channel_prev)\n",
    "\n",
    "        self.output_tensor = np.zeros((batch_size, self.h_new, self.w_new, self.num_out_channel))\n",
    "        Z_prev = np.array(Z_prev, copy=True)\n",
    "\n",
    "        # create zero padded Z_prev\n",
    "        print(f'Z prev shape:{Z_prev.shape}')\n",
    "        Z_prev_padded = Utility.zero_pad(Z_prev, self.padding_size)\n",
    "\n",
    "        # Apply convolution operation over this zero padded previous activation\n",
    "        for row in range(self.h_new):\n",
    "\n",
    "            row_start = row * self.stride\n",
    "\n",
    "            for col in range(self.w_new):\n",
    "\n",
    "                col_start = col *  self.stride\n",
    "\n",
    "                for output_channel_index in range(self.num_out_channel):\n",
    "\n",
    "                    Z_prev_windowed = Z_prev_padded[:,row_start : row_start + self.filter_size,\n",
    "                                            col_start : col_start + self.filter_size,:]\n",
    "\n",
    "                    conv_step_W = self.W[:, :, :, output_channel_index]\n",
    "                    conv_step_b = self.b[:, :, :, output_channel_index]\n",
    "\n",
    "                    # print('z shape', Z_prev_windowed.shape)\n",
    "                    # print('w shape', conv_step_W.shape)\n",
    "\n",
    "                    # self.output_tensor[:, row, col, output_channel_index] = np.sum(\n",
    "                    #     Z_prev_windowed * conv_step_W,\n",
    "                    #     axis=(1,2,3)\n",
    "                    # ) + conv_step_b\n",
    "\n",
    "                    self.output_tensor[:, row, col, output_channel_index] = Utility.convolve_single_step_over_batch(\n",
    "                        Z_prev_windowed, conv_step_W, conv_step_b\n",
    "                    )\n",
    "\n",
    "            # asserting output shape\n",
    "            assert(self.output_tensor.shape == (batch_size, self.h_new, self.w_new, self.num_out_channel))\n",
    "            print('output tensor shape:', self.output_tensor.shape)\n",
    "\n",
    "            if is_training:\n",
    "                # cache some values\n",
    "                pass\n",
    "\n",
    "            # perform activation element wise in this case\n",
    "            self.output_tensor = self.relu_activation.activation_f(self.output_tensor)\n",
    "\n",
    "    def backward(self):\n",
    "        pass\n",
    "\n",
    "    def update_CNN_parameters(self, dW : np.array, db: np.array):\n",
    "        self.W = self.W - dW\n",
    "        self.b = self.b - db\n",
    "\n",
    "    def print_layer_dimensions(self):\n",
    "        print(f'Output Tensor Dimensions: {self.output_tensor.shape}')\n",
    "        print(f'Weight Dimension: {self.W.shape}')\n",
    "        print(f'Bias Dimension: {self.b.shape}')\n",
    "\n",
    "    def get_output_dimension(self) -> Tuple:\n",
    "        return self.h_new, self.w_new, self.num_out_channel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    # class variable\n",
    "    layer_num = 1\n",
    "\n",
    "    def __init__(self):\n",
    "        self.input_dim = None\n",
    "        self.output_tensor = None\n",
    "        self.output_dim = None\n",
    "        self.h_prev, self.w_prev, self.num_channel_prev = None, None, None\n",
    "        self.layer_name = 'Flatten__' + str(self.layer_num + 1)\n",
    "        self.layer_num += 1\n",
    "        self.is_trainable = False\n",
    "\n",
    "    def initialize_flatten_layer_dimensions(self, prev_layer_output_dim):\n",
    "        \"\"\"\n",
    "        :param prev_layer_output_dim: prev layer output of shape (new_h, new_w, new_channel)\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.h_prev, self.w_prev, self.num_channel_prev  = prev_layer_output_dim\n",
    "        self.output_dim = self.h_prev * self.w_prev * self.num_channel_prev\n",
    "\n",
    "    def forward_batch(self, Z_prev: np.array, is_training) -> np.array:\n",
    "        self.output_tensor = Z_prev.reshape(Z_prev.shape[0], Z_prev.shape[1] * Z_prev.shape[2] * Z_prev.shape[3])\n",
    "\n",
    "        assert self.output_tensor.shape[1] == self.output_dim\n",
    "\n",
    "    def forward(self, Z_prev: np.array, is_training) -> np.array:\n",
    "        self.output_tensor = Z_prev.reshape(1, Z_prev.shape[0] * Z_prev.shape[1] * Z_prev.shape[2])\n",
    "\n",
    "    def get_output_dimension(self):\n",
    "        return self.output_dim\n",
    "\n",
    "    def backward(self):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "outputs": [],
   "source": [
    "class MaxPool:\n",
    "    # class variable\n",
    "    layer_num = 1\n",
    "\n",
    "    def __init__(self, filter_size, stride):\n",
    "        self.filter_size = filter_size\n",
    "        self.stride = stride\n",
    "        self.h_prev, self.w_prev, self.num_channel_prev = None, None, None\n",
    "        self.h_new, self.w_new, self.num_out_channel = None, None, None\n",
    "        self.output_tensor = None\n",
    "        self.cache = {}\n",
    "        self.layer_name = 'MaxPool__'+ str(self.layer_num + 1)\n",
    "        self.layer_num += 1\n",
    "        self.is_trainable = False\n",
    "\n",
    "    def initialize_max_pool_params(self, prev_layer_output_dim):\n",
    "        \"\"\"\n",
    "        Initializes output dimensions with the dimension of the previous layers\n",
    "        :param prev_layer_output_dim: output dimension of the layer immediately before this layer\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.h_prev, self.w_prev , self.num_channel_prev = prev_layer_output_dim\n",
    "        self.h_new = int((self.h_prev - self.filter_size) / self.stride + 1)\n",
    "        self.w_new = int((self.w_prev - self.filter_size) / self.stride + 1)\n",
    "        self.num_out_channel = self.num_channel_prev\n",
    "\n",
    "    def forward(self, Z_prev, is_training):\n",
    "\n",
    "        print('prev z shape in maxpool:', Z_prev.shape)\n",
    "        assert Z_prev.shape == (self.h_prev, self.w_prev, self.num_channel_prev)\n",
    "        print('here')\n",
    "        Z_prev = np.array(Z_prev, copy=True)\n",
    "        print('there')\n",
    "        self.output_tensor = np.zeros((self.h_new, self.w_new, self.num_out_channel))\n",
    "\n",
    "        print('Going for max pooling')\n",
    "        # Apply convolution operation over this zero padded previous activation\n",
    "        for row in range(self.h_new):\n",
    "\n",
    "            row_start = row * self.stride\n",
    "\n",
    "            for col in range(self.w_new):\n",
    "\n",
    "                col_start = col *  self.stride\n",
    "\n",
    "                for output_channel_index in range(self.num_out_channel):\n",
    "\n",
    "                    Z_prev_windowed = Z_prev[\n",
    "                                      row_start : row_start + self.filter_size,\n",
    "                                      col_start : col_start + self.filter_size,\n",
    "                                      output_channel_index\n",
    "                                      ]\n",
    "\n",
    "                    self.output_tensor[row, col, output_channel_index] = Utility.get_max_pool_window(Z_prev_windowed)\n",
    "\n",
    "        assert self.output_tensor.shape == (self.h_new, self.w_new, self.num_out_channel)\n",
    "        if is_training:\n",
    "            pass\n",
    "        print('Max pool forward done')\n",
    "\n",
    "    def forward_batching(self, Z_prev, is_training):\n",
    "\n",
    "        batch_size = Z_prev.shape[0]\n",
    "        assert Z_prev.shape == (batch_size, self.h_prev, self.w_prev, self.num_channel_prev)\n",
    "\n",
    "        self.output_tensor = np.zeros((batch_size, self.h_new, self.w_new, self.num_out_channel))\n",
    "        Z_prev = np.array(Z_prev, copy=True)\n",
    "\n",
    "        # Apply convolution operation over this zero padded previous activation\n",
    "        for row in range(self.h_new):\n",
    "\n",
    "            row_start = row * self.stride\n",
    "\n",
    "            for col in range(self.w_new):\n",
    "\n",
    "                col_start = col *  self.stride\n",
    "\n",
    "                for output_channel_index in range(self.num_out_channel):\n",
    "\n",
    "                    Z_prev_windowed = Z_prev[:,\n",
    "                                    row_start : row_start + self.filter_size,\n",
    "                                    col_start : col_start + self.filter_size,\n",
    "                                    output_channel_index\n",
    "                                      ]\n",
    "\n",
    "                    self.output_tensor[:, row, col, output_channel_index] = Utility.get_max_pool_window_over_batch(Z_prev_windowed)\n",
    "\n",
    "        assert self.output_tensor.shape == (batch_size, self.h_new, self.w_new, self.num_out_channel)\n",
    "        if is_training:\n",
    "            pass\n",
    "\n",
    "    def print_layer_dimensions(self):\n",
    "        print(f'Output Tensor Dimensions: {self.output_tensor.shape}')\n",
    "\n",
    "    def backward(self):\n",
    "        pass\n",
    "\n",
    "    def get_output_dimension(self):\n",
    "        return self.h_new, self.w_new, self.num_out_channel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "\n",
    "    # class variable\n",
    "    layer_num = 1\n",
    "\n",
    "    def __init__(self, num_units, activation_obj):\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.num_units = num_units\n",
    "        self.cache = {}\n",
    "        self.output_tensor = None\n",
    "        self.activation_obj = activation_obj\n",
    "        self.layer_name = 'Dense__' + str(self.layer_num)\n",
    "        self.layer_num += 1\n",
    "        self.is_trainable = True\n",
    "\n",
    "    def initialize_dense_layer_weights_biases(self, prev_flat_layer_output_dim):\n",
    "        self.W = np.random.randn(self.num_units, prev_flat_layer_output_dim)\n",
    "        self.b = np.random.randn(self.num_units, 1) # will be broadcast to (hidden_units, batch_size) before addition\n",
    "\n",
    "    def forward(self, Z_prev, is_training):\n",
    "        \"\"\"\n",
    "        :param Z_prev: tensor of shape (batch, prev_flattened_shape)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        print(f'Z_prev shape: {Z_prev.shape}')\n",
    "        assert Z_prev.shape[1] == self.W.shape[1]\n",
    "\n",
    "        Z_prev = np.array(Z_prev, copy=True)\n",
    "        Z_prev = Z_prev.reshape(Z_prev.shape[1], Z_prev.shape[0])\n",
    "        self.output_tensor = self.activation_obj.activation_f(np.dot(self.W, Z_prev) + self.b)\n",
    "\n",
    "        # assert the output tensor shape should be (num_hidden_units, batch size)\n",
    "        assert self.output_tensor.shape == (self.num_units, Z_prev.shape[1])\n",
    "\n",
    "        if is_training:\n",
    "            pass\n",
    "\n",
    "\n",
    "    def backward(self):\n",
    "        pass\n",
    "\n",
    "    def get_output_dimension(self):\n",
    "        return self.output_tensor.shape[0]\n",
    "\n",
    "    def print_layer_dimensions(self):\n",
    "        print(f'Output Tensor Dimensions: {self.output_tensor.shape}')\n",
    "        print(f'Weight Dimension: {self.W.shape}')\n",
    "        print(f'Bias Dimension: {self.b.shape}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "outputs": [],
   "source": [
    "class Utility:\n",
    "\n",
    "    @staticmethod\n",
    "    def one_hot_encode(y_true):\n",
    "        # Define the One-hot Encoder\n",
    "        ohe = preprocessing.OneHotEncoder()\n",
    "        ohe.fit(y_true)\n",
    "        y_true = ohe.transform(y_true).toarray()\n",
    "        return y_true\n",
    "\n",
    "    @staticmethod\n",
    "    def zero_pad(tensor, pad_size):\n",
    "        \"\"\"\n",
    "        :param tensor: tensor of shape (batch_size, h, w, num_channel)\n",
    "        :return: padded tensor of shape (h + 2 * pad_size, w + 2 * pad_size, num_channel)\n",
    "        \"\"\"\n",
    "        return np.pad(tensor, ((0,0), (pad_size, pad_size), (pad_size, pad_size), (0,0)), mode='constant', constant_values=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def zero_pad_without_batch(tensor, pad_size):\n",
    "        \"\"\"\n",
    "        :param tensor: tensor of shape (h, w, num_channel)\n",
    "        :return: padded tensor of shape (h + 2 * pad_size, w + 2 * pad_size, num_channel)\n",
    "        \"\"\"\n",
    "        return np.pad(tensor, ((pad_size, pad_size), (pad_size, pad_size), (0,0)), mode='constant', constant_values=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def convolve_single_step(Z_prev_windowed, W, b):\n",
    "        \"\"\"\n",
    "        :param Z_prev_windowed: window of shape (F, F, num_channel_Z_prev)\n",
    "        :param W: kernel/filter/weight of shape (F, F, num_channel_Z_prev)\n",
    "        :param b: bias term of shape (1, 1, 1)\n",
    "        :return: scaler convolved value\n",
    "        \"\"\"\n",
    "        return np.multiply(Z_prev_windowed, W).sum() + float(b)\n",
    "\n",
    "    @staticmethod\n",
    "    def convolve_single_step_over_batch(tensor, W, b):\n",
    "        return np.sum(tensor * W, axis=(1,2,3)) + b\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_max_pool_window(Z_prev_windowed):\n",
    "        return Z_prev_windowed.max()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_max_pool_window_over_batch(Z_prev_windowed: np.array):\n",
    "        print('In max pool', Z_prev_windowed.shape)\n",
    "        print(np.max(Z_prev_windowed, axis=(1,2)))\n",
    "        return np.max(Z_prev_windowed, axis=(1,2))\n",
    "\n",
    "    @staticmethod\n",
    "    def create_mini_batches(X: np.array, Y: np.array, mini_batch_size: int):\n",
    "        total_data = X.shape[0]\n",
    "        for index in range(0, total_data, mini_batch_size):\n",
    "            start_index = index\n",
    "            end_index = min(start_index + mini_batch_size, total_data)\n",
    "            yield X[start_index: end_index,...], Y[start_index: end_index, ...]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "outputs": [],
   "source": [
    "class ReLUActivation:\n",
    "\n",
    "    # class variable\n",
    "    layer_num = 1\n",
    "    def __init__(self):\n",
    "        self.layer_name = 'ReLU__' + str(self.layer_num)\n",
    "        self.layer_num += 1\n",
    "\n",
    "    @staticmethod\n",
    "    def activation_f(tensor):\n",
    "        print('in relu:', tensor)\n",
    "        print('After: ', np.maximum(tensor, 0))\n",
    "        return np.maximum(tensor, 0)\n",
    "\n",
    "    @staticmethod\n",
    "    def d_relu(tensor):\n",
    "        return np.where(tensor > 0, 1, 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "outputs": [],
   "source": [
    "@dataclass(unsafe_hash=True)\n",
    "class InputLayer:\n",
    "    \"\"\"\n",
    "    Class for saving input dimension\n",
    "    \"\"\"\n",
    "    input_dimension: np.array\n",
    "    is_trainable: bool\n",
    "    layer_name: str"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "outputs": [],
   "source": [
    "class SoftmaxActivation:\n",
    "\n",
    "    # class variable\n",
    "    layer_num = 1\n",
    "    def __init__(self):\n",
    "        self.layer_name = 'Softmax__' + str(self.layer_num)\n",
    "        self.layer_num += 1\n",
    "\n",
    "    @staticmethod\n",
    "    def activation_f(tensor):\n",
    "        print('softmax tensor shape:', tensor.shape) # expected tensor shape: (classes, batchsize/1)\n",
    "        exponent = np.exp(tensor)\n",
    "\n",
    "        # y = np.exp(tensor - np.max(tensor, axis=0, keepdims=True))\n",
    "        # print('alternative value: ')\n",
    "        # print(y / np.sum(tensor, axis=0, keepdims=True))\n",
    "        return tensor/np.sum(exponent, axis=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def d_softmax(tensor):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "outputs": [],
   "source": [
    "class crossEntropyLoss:\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = None # a list of layer object according to input\n",
    "        self.layer_w_gradients = dict() # {'layer name': dw}\n",
    "        self.layer_b_gradients = dict() # {'layer name': db}\n",
    "        self.optimizer = None\n",
    "        self.cost_function = None\n",
    "\n",
    "    def add(self, layer_list):\n",
    "        self.layers = layer_list\n",
    "\n",
    "    def initializer_layer_params(self):\n",
    "        \"\"\"\n",
    "        This method initializes the layers in the model providing the input dimension that the layers expect to get\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        #InputLayer(input_dimension=input_dimension, is_trainable=False, layer_name='Input')\n",
    "\n",
    "        assert self.layers[0].layer_name == 'Input'\n",
    "        input_dimension = self.layers[0].input_dimension\n",
    "\n",
    "        for previous_layer, current_layer in zip(self.layers, self.layers[1:]):\n",
    "            prev_output_dim = None\n",
    "            if previous_layer.layer_name == 'Input':\n",
    "                prev_output_dim = previous_layer.input_dimension # H, W, Color Channel\n",
    "            else:\n",
    "                prev_output_dim = previous_layer.get_output_dimension()\n",
    "\n",
    "            if current_layer.layer_name.startswith('Conv2D__'):\n",
    "                current_layer.initialize_output_dimensions(prev_output_dim) # H, W, Color Channel\n",
    "                current_layer.initialize_weights_biases()\n",
    "            elif current_layer.layer_name.startswith('MaxPool__'):\n",
    "                current_layer.initialize_max_pool_params(prev_output_dim) # H, W, Color Channel\n",
    "            elif current_layer.layer_name.startswith('Dense__'):\n",
    "                current_layer.initialize_dense_layer_weights_biases(prev_output_dim) # flatten layer dimension\n",
    "            elif current_layer.layer_name.startswith('Flatten__'):\n",
    "                current_layer.initialize_flatten_layer_dimensions(prev_output_dim) # (new_h, new_w, new_channel)\n",
    "\n",
    "    def compile(self, optimizer, cost_function):\n",
    "        self.optimizer = optimizer\n",
    "        self.cost_function = cost_function\n",
    "\n",
    "    def train(self, training_data, validation_data, epochs=5, learning_rate=0.01, mini_batch_size=32):\n",
    "\n",
    "        X_val, y_Val = validation_data\n",
    "\n",
    "        for e in range(epochs):\n",
    "            # each epoch will run through a training once and update weights\n",
    "            print(f'Running Epoch: {e}')\n",
    "            X_train, Y_train = training_data\n",
    "\n",
    "            # first we create the mini batches and then run training step through it\n",
    "            for X, Y in Utility.create_mini_batches(X=X_train, Y=Y_train, mini_batch_size=mini_batch_size):\n",
    "                \"\"\"\n",
    "                X shape --> (mini_batch_size, h, w, color_channel)\n",
    "                Y shape --> (mini_batch_size, num_of_class) (one hot encoded vector)\n",
    "                \"\"\"\n",
    "                Y_out = self.forward_propagation(X_train, is_training=True)\n",
    "                self.backward_propagation(Y_out,Y)\n",
    "                self.update_layer_parameters(learning_rate)\n",
    "                Loss = self.compute_cost(Y_out, Y)\n",
    "\n",
    "            print(f'Cost After a Epoch {e+1}: {Loss * 100}')\n",
    "\n",
    "            # perform validation step here\n",
    "        print('Finish Training!')\n",
    "\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    def forward_propagation(self, X_train, is_training) -> np.array:\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the network\n",
    "        :param X_train: nd training tensor (batch_size, h, w, color_channel)\n",
    "        :param is_training: whether we are training or not\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        input = X_train\n",
    "        for layer in self.layers[1:]:\n",
    "            # skipping the input layer\n",
    "            print(f'Forward for layer {layer.layer_name}')\n",
    "            layer.forward(input, is_training)\n",
    "            input = layer.output_tensor # getting the output tensor of the layer to be the input tensor to the next\n",
    "\n",
    "        print('final input', input)\n",
    "\n",
    "    def backward_propagation(self, Y_out, Y):\n",
    "        pass\n",
    "\n",
    "    def update_layer_parameters(self, learning_rate):\n",
    "        pass\n",
    "\n",
    "    def compute_cost(self, Y_out, Y):\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "outputs": [],
   "source": [
    "mnist = MnistDataLoader('./dataset/mnist')\n",
    "mnist.load_mnist()\n",
    "mnist.preprocess_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])"
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.data[mnist.data_list[1]][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10,5,7,4) # h, w, c of previous layer\n",
    "cnn = Convolution2D(num_out_channel=8, filter_size=3, stride=2, padding_size=1)\n",
    "cnn.initialize_output_dimensions((A_prev.shape[1], A_prev.shape[2], A_prev.shape[3]))\n",
    "cnn.initialize_weights_biases()\n",
    "#cnn.print_layer_dimensions()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z prev shape:(10, 5, 7, 4)\n",
      "output tensor shape: (10, 3, 4, 8)\n",
      "output tensor shape: (10, 3, 4, 8)\n",
      "output tensor shape: (10, 3, 4, 8)\n"
     ]
    }
   ],
   "source": [
    "cnn.forward_batch_i(A_prev, is_training=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 3, 4, 8)\n",
      "Z's mean =\n",
      " 0.6923608807576933\n",
      "Z[3,2,1] =\n",
      " [-1.28912231  2.27650251  6.61941931  0.95527176  8.25132576  2.31329639\n",
      " 13.00689405  2.34576051]\n"
     ]
    }
   ],
   "source": [
    "print(cnn.output_tensor.shape)\n",
    "print(\"Z's mean =\\n\", np.mean(cnn.output_tensor))\n",
    "print(\"Z[3,2,1] =\\n\", cnn.output_tensor[3,2,1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In max pool (2, 3, 3)\n",
      "[1.74481176 1.19891788]\n",
      "In max pool (2, 3, 3)\n",
      "[0.90159072 0.84616065]\n",
      "In max pool (2, 3, 3)\n",
      "[1.65980218 0.82797464]\n",
      "In max pool (2, 3, 3)\n",
      "[1.74481176 0.69803203]\n",
      "In max pool (2, 3, 3)\n",
      "[1.46210794 0.84616065]\n",
      "In max pool (2, 3, 3)\n",
      "[1.65980218 1.2245077 ]\n",
      "In max pool (2, 3, 3)\n",
      "[1.74481176 0.69803203]\n",
      "In max pool (2, 3, 3)\n",
      "[1.6924546  1.12141771]\n",
      "In max pool (2, 3, 3)\n",
      "[1.65980218 1.2245077 ]\n",
      "In max pool (2, 3, 3)\n",
      "[1.14472371 1.96710175]\n",
      "In max pool (2, 3, 3)\n",
      "[0.90159072 0.84616065]\n",
      "In max pool (2, 3, 3)\n",
      "[2.10025514 1.27375593]\n",
      "In max pool (2, 3, 3)\n",
      "[1.14472371 1.96710175]\n",
      "In max pool (2, 3, 3)\n",
      "[0.90159072 0.84616065]\n",
      "In max pool (2, 3, 3)\n",
      "[1.65980218 1.23616403]\n",
      "In max pool (2, 3, 3)\n",
      "[1.14472371 1.62765075]\n",
      "In max pool (2, 3, 3)\n",
      "[1.6924546  1.12141771]\n",
      "In max pool (2, 3, 3)\n",
      "[1.65980218 1.2245077 ]\n",
      "In max pool (2, 3, 3)\n",
      "[1.13162939 1.96710175]\n",
      "In max pool (2, 3, 3)\n",
      "[1.51981682 0.86888616]\n",
      "In max pool (2, 3, 3)\n",
      "[2.18557541 1.27375593]\n",
      "In max pool (2, 3, 3)\n",
      "[1.13162939 1.96710175]\n",
      "In max pool (2, 3, 3)\n",
      "[1.51981682 0.86888616]\n",
      "In max pool (2, 3, 3)\n",
      "[2.18557541 1.23616403]\n",
      "In max pool (2, 3, 3)\n",
      "[1.13162939 1.62765075]\n",
      "In max pool (2, 3, 3)\n",
      "[1.6924546  1.12141771]\n",
      "In max pool (2, 3, 3)\n",
      "[2.18557541 0.79280687]\n",
      "Output Tensor Dimensions: (2, 3, 3, 3)\n",
      "[[[[1.74481176 0.90159072 1.65980218]\n",
      "   [1.74481176 1.46210794 1.65980218]\n",
      "   [1.74481176 1.6924546  1.65980218]]\n",
      "\n",
      "  [[1.14472371 0.90159072 2.10025514]\n",
      "   [1.14472371 0.90159072 1.65980218]\n",
      "   [1.14472371 1.6924546  1.65980218]]\n",
      "\n",
      "  [[1.13162939 1.51981682 2.18557541]\n",
      "   [1.13162939 1.51981682 2.18557541]\n",
      "   [1.13162939 1.6924546  2.18557541]]]\n",
      "\n",
      "\n",
      " [[[1.19891788 0.84616065 0.82797464]\n",
      "   [0.69803203 0.84616065 1.2245077 ]\n",
      "   [0.69803203 1.12141771 1.2245077 ]]\n",
      "\n",
      "  [[1.96710175 0.84616065 1.27375593]\n",
      "   [1.96710175 0.84616065 1.23616403]\n",
      "   [1.62765075 1.12141771 1.2245077 ]]\n",
      "\n",
      "  [[1.96710175 0.86888616 1.27375593]\n",
      "   [1.96710175 0.86888616 1.23616403]\n",
      "   [1.62765075 1.12141771 0.79280687]]]]\n"
     ]
    }
   ],
   "source": [
    "# Case 1: stride of 1\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 5, 5, 3)\n",
    "maxpool = MaxPool(filter_size=3, stride=1)\n",
    "maxpool.initialize_max_pool_params((A_prev.shape[1],A_prev.shape[2],A_prev.shape[3]))\n",
    "maxpool.forward_batching(A_prev, True)\n",
    "maxpool.print_layer_dimensions()\n",
    "print(maxpool.output_tensor)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "(10000,)\n",
      "(10000,)\n",
      "(10000,)\n",
      "(10000,)\n",
      "(10000,)\n",
      "Shape: (50000, 32, 32, 3)\n",
      "Shape: (50000, 1)\n",
      "Shape: (10000, 32, 32, 3)\n",
      "Shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "cifer_dataloader = Cifer10DataLoader('/home/akil/Work/Work/Academics/4-2/ML/Assignment-3/dataset/cifer-10/cifar-10-python/cifar-10-batches-py')\n",
    "cifer_dataloader.concatenate_data()\n",
    "cifer_dataloader.preprocess_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifer_dataloader.data['train_labels'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "outputs": [
    {
     "data": {
      "text/plain": "(50000, 32, 32, 3)"
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifer_dataloader.data['train_images'].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [
    {
     "data": {
      "text/plain": "(50000, 10)"
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifer_dataloader.data['train_labels'].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "outputs": [],
   "source": [
    "total_data = cifer_dataloader.data['train_images'].shape[0]\n",
    "mini_batch_size = 32\n",
    "p = np.random.permutation(total_data)\n",
    "cifer_dataloader.data['train_images'], cifer_dataloader.data['train_labels'] = cifer_dataloader.data['train_images'][p, :], cifer_dataloader.data['train_labels'][p, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "outputs": [],
   "source": [
    "X = cifer_dataloader.data['train_images']\n",
    "Y = cifer_dataloader.data['train_labels']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing The forward pass of the Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "outputs": [
    {
     "data": {
      "text/plain": "array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image = mnist.data[mnist.data_list[0]][0]\n",
    "label = mnist.data[mnist.data_list[1]][1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "outputs": [],
   "source": [
    "cnn = Model()\n",
    "cnn.add(layer_list=[\n",
    "    InputLayer(input_dimension=(mnist.size, mnist.size, mnist.color_channel), is_trainable=False, layer_name='Input'),\n",
    "    Convolution2D(num_out_channel=8, filter_size=3, stride=2, padding_size=1, activation=ReLUActivation()),\n",
    "    MaxPool(filter_size=3, stride=1),\n",
    "    Flatten(),\n",
    "    DenseLayer(10, activation_obj=SoftmaxActivation())\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "outputs": [],
   "source": [
    "cnn.initializer_layer_params()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward for layer Conv2D__2\n",
      "In forward of Convolution output tensor shape before relu (14, 14, 8)\n",
      "in relu: [[[-1.36780311  1.67851907 -0.23367432 ... -1.06818604  2.00100227\n",
      "   -1.74383293]\n",
      "  [-1.36780311  1.67851907 -0.23367432 ... -1.06818604  2.00100227\n",
      "   -1.74383293]\n",
      "  [-1.36780311  1.67851907 -0.23367432 ... -1.06818604  2.00100227\n",
      "   -1.74383293]\n",
      "  ...\n",
      "  [-1.36780311  1.67851907 -0.23367432 ... -1.06818604  2.00100227\n",
      "   -1.74383293]\n",
      "  [-1.36780311  1.67851907 -0.23367432 ... -1.06818604  2.00100227\n",
      "   -1.74383293]\n",
      "  [-1.36780311  1.67851907 -0.23367432 ... -1.06818604  2.00100227\n",
      "   -1.74383293]]\n",
      "\n",
      " [[-1.36780311  1.67851907 -0.23367432 ... -1.06818604  2.00100227\n",
      "   -1.74383293]\n",
      "  [-1.36780311  1.67851907 -0.23367432 ... -1.06818604  2.00100227\n",
      "   -1.74383293]\n",
      "  [-1.36780311  1.67851907 -0.23367432 ... -1.06818604  2.00100227\n",
      "   -1.74383293]\n",
      "  ...\n",
      "  [-1.36780311  1.67851907 -0.23367432 ... -1.06818604  2.00100227\n",
      "   -1.74383293]\n",
      "  [-1.36780311  1.67851907 -0.23367432 ... -1.06818604  2.00100227\n",
      "   -1.74383293]\n",
      "  [-1.36780311  1.67851907 -0.23367432 ... -1.06818604  2.00100227\n",
      "   -1.74383293]]\n",
      "\n",
      " [[-1.36780311  1.67851907 -0.23367432 ... -1.06818604  2.00100227\n",
      "   -1.74383293]\n",
      "  [-1.36780311  1.67851907 -0.23367432 ... -1.06818604  2.00100227\n",
      "   -1.74383293]\n",
      "  [-1.36780311  1.67851907 -0.23367432 ... -1.06818604  2.00100227\n",
      "   -1.74383293]\n",
      "  ...\n",
      "  [ 0.29400225  1.30110478  1.61702241 ... -0.95887756  1.66962636\n",
      "   -0.7971717 ]\n",
      "  [-1.92174185  1.28114609 -0.20550027 ... -1.51101041  2.42538025\n",
      "   -1.79772184]\n",
      "  [-1.36780311  1.67851907 -0.23367432 ... -1.06818604  2.00100227\n",
      "   -1.74383293]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-1.36780311  1.67851907 -0.23367432 ... -1.06818604  2.00100227\n",
      "   -1.74383293]\n",
      "  [-1.36780311  1.67851907 -0.23367432 ... -1.06818604  2.00100227\n",
      "   -1.74383293]\n",
      "  [-0.38546867  1.19397036  0.99941435 ... -0.31989471  1.58882505\n",
      "   -1.21043578]\n",
      "  ...\n",
      "  [-1.36780311  1.67851907 -0.23367432 ... -1.06818604  2.00100227\n",
      "   -1.74383293]\n",
      "  [-1.36780311  1.67851907 -0.23367432 ... -1.06818604  2.00100227\n",
      "   -1.74383293]\n",
      "  [-1.36780311  1.67851907 -0.23367432 ... -1.06818604  2.00100227\n",
      "   -1.74383293]]\n",
      "\n",
      " [[-1.36780311  1.67851907 -0.23367432 ... -1.06818604  2.00100227\n",
      "   -1.74383293]\n",
      "  [-1.36780311  1.67851907 -0.23367432 ... -1.06818604  2.00100227\n",
      "   -1.74383293]\n",
      "  [-0.09624118  2.99759601 -1.36642963 ... -1.45901479  3.79787661\n",
      "   -2.26488173]\n",
      "  ...\n",
      "  [-1.36780311  1.67851907 -0.23367432 ... -1.06818604  2.00100227\n",
      "   -1.74383293]\n",
      "  [-1.36780311  1.67851907 -0.23367432 ... -1.06818604  2.00100227\n",
      "   -1.74383293]\n",
      "  [-1.36780311  1.67851907 -0.23367432 ... -1.06818604  2.00100227\n",
      "   -1.74383293]]\n",
      "\n",
      " [[-1.36780311  1.67851907 -0.23367432 ... -1.06818604  2.00100227\n",
      "   -1.74383293]\n",
      "  [-1.36780311  1.67851907 -0.23367432 ... -1.06818604  2.00100227\n",
      "   -1.74383293]\n",
      "  [-1.36780311  1.67851907 -0.23367432 ... -1.06818604  2.00100227\n",
      "   -1.74383293]\n",
      "  ...\n",
      "  [-1.36780311  1.67851907 -0.23367432 ... -1.06818604  2.00100227\n",
      "   -1.74383293]\n",
      "  [-1.36780311  1.67851907 -0.23367432 ... -1.06818604  2.00100227\n",
      "   -1.74383293]\n",
      "  [-1.36780311  1.67851907 -0.23367432 ... -1.06818604  2.00100227\n",
      "   -1.74383293]]]\n",
      "After:  [[[0.         1.67851907 0.         ... 0.         2.00100227 0.        ]\n",
      "  [0.         1.67851907 0.         ... 0.         2.00100227 0.        ]\n",
      "  [0.         1.67851907 0.         ... 0.         2.00100227 0.        ]\n",
      "  ...\n",
      "  [0.         1.67851907 0.         ... 0.         2.00100227 0.        ]\n",
      "  [0.         1.67851907 0.         ... 0.         2.00100227 0.        ]\n",
      "  [0.         1.67851907 0.         ... 0.         2.00100227 0.        ]]\n",
      "\n",
      " [[0.         1.67851907 0.         ... 0.         2.00100227 0.        ]\n",
      "  [0.         1.67851907 0.         ... 0.         2.00100227 0.        ]\n",
      "  [0.         1.67851907 0.         ... 0.         2.00100227 0.        ]\n",
      "  ...\n",
      "  [0.         1.67851907 0.         ... 0.         2.00100227 0.        ]\n",
      "  [0.         1.67851907 0.         ... 0.         2.00100227 0.        ]\n",
      "  [0.         1.67851907 0.         ... 0.         2.00100227 0.        ]]\n",
      "\n",
      " [[0.         1.67851907 0.         ... 0.         2.00100227 0.        ]\n",
      "  [0.         1.67851907 0.         ... 0.         2.00100227 0.        ]\n",
      "  [0.         1.67851907 0.         ... 0.         2.00100227 0.        ]\n",
      "  ...\n",
      "  [0.29400225 1.30110478 1.61702241 ... 0.         1.66962636 0.        ]\n",
      "  [0.         1.28114609 0.         ... 0.         2.42538025 0.        ]\n",
      "  [0.         1.67851907 0.         ... 0.         2.00100227 0.        ]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0.         1.67851907 0.         ... 0.         2.00100227 0.        ]\n",
      "  [0.         1.67851907 0.         ... 0.         2.00100227 0.        ]\n",
      "  [0.         1.19397036 0.99941435 ... 0.         1.58882505 0.        ]\n",
      "  ...\n",
      "  [0.         1.67851907 0.         ... 0.         2.00100227 0.        ]\n",
      "  [0.         1.67851907 0.         ... 0.         2.00100227 0.        ]\n",
      "  [0.         1.67851907 0.         ... 0.         2.00100227 0.        ]]\n",
      "\n",
      " [[0.         1.67851907 0.         ... 0.         2.00100227 0.        ]\n",
      "  [0.         1.67851907 0.         ... 0.         2.00100227 0.        ]\n",
      "  [0.         2.99759601 0.         ... 0.         3.79787661 0.        ]\n",
      "  ...\n",
      "  [0.         1.67851907 0.         ... 0.         2.00100227 0.        ]\n",
      "  [0.         1.67851907 0.         ... 0.         2.00100227 0.        ]\n",
      "  [0.         1.67851907 0.         ... 0.         2.00100227 0.        ]]\n",
      "\n",
      " [[0.         1.67851907 0.         ... 0.         2.00100227 0.        ]\n",
      "  [0.         1.67851907 0.         ... 0.         2.00100227 0.        ]\n",
      "  [0.         1.67851907 0.         ... 0.         2.00100227 0.        ]\n",
      "  ...\n",
      "  [0.         1.67851907 0.         ... 0.         2.00100227 0.        ]\n",
      "  [0.         1.67851907 0.         ... 0.         2.00100227 0.        ]\n",
      "  [0.         1.67851907 0.         ... 0.         2.00100227 0.        ]]]\n",
      "In forward of CNN output tensor shape after relu (14, 14, 8)\n",
      "Forward for layer MaxPool__2\n",
      "prev z shape in maxpool: (14, 14, 8)\n",
      "here\n",
      "there\n",
      "Going for max pooling\n",
      "Max pool forward done\n",
      "Forward for layer Flatten__2\n",
      "Forward for layer Dense__1\n",
      "Z_prev shape: (1, 1152)\n",
      "softmax tensor shape: (10, 1)\n",
      "final input [[-1.21131364e-31]\n",
      " [-7.03422023e-32]\n",
      " [ 1.03439870e-32]\n",
      " [-4.24969825e-32]\n",
      " [-6.97117852e-32]\n",
      " [ 2.34547006e-33]\n",
      " [-1.64061114e-32]\n",
      " [ 5.35855388e-32]\n",
      " [-1.03899319e-32]\n",
      " [-2.45569632e-32]]\n"
     ]
    }
   ],
   "source": [
    "out = cnn.forward_propagation(image, is_training=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "np.max(x, axis=1, keepdims=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}