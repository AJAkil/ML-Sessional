{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gzip\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "from typing import Tuple\n",
    "from dataclasses import dataclass\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "outputs": [],
   "source": [
    "class MnistDataLoader:\n",
    "    def __init__(self, data_folder_path):\n",
    "        self.data_folder_path = data_folder_path\n",
    "        self.train_file_name = 'train-images-idx3-ubyte.gz'\n",
    "        self.train_label_file_name = 'train-labels-idx1-ubyte.gz'\n",
    "        self.test_file_name = 't10k-images-idx3-ubyte.gz'\n",
    "        self.test_label_file_name = 't10k-labels-idx1-ubyte.gz'\n",
    "        self.data = dict()\n",
    "        self.size = 28\n",
    "        self.color_channel = 1\n",
    "        self.data_list = [\n",
    "            'train_images',\n",
    "            'train_labels',\n",
    "            'test_images',\n",
    "            'test_labels'\n",
    "        ]\n",
    "\n",
    "    def load_images(self, data_list_index, file_name):\n",
    "        images = gzip.open(os.path.join(self.data_folder_path, file_name), 'rb')\n",
    "        self.data[self.data_list[data_list_index]] = np.frombuffer(images.read(), dtype=np.uint8, offset=16).reshape(-1, self.size, self.size)\n",
    "        self.data[self.data_list[data_list_index]] = self.data[self.data_list[data_list_index]].reshape(self.data[self.data_list[data_list_index]].shape[0], self.size, self.size, self.color_channel).astype(np.float32)\n",
    "\n",
    "    def load_labels(self, data_list_index, file_name):\n",
    "        labels = gzip.open(os.path.join(self.data_folder_path, file_name), 'rb')\n",
    "        self.data[self.data_list[data_list_index]] = np.frombuffer(labels.read(), dtype=np.uint8, offset=8)\n",
    "        self.data[self.data_list[data_list_index]].resize(self.data[self.data_list[data_list_index]].shape[0],1)\n",
    "\n",
    "    def load_mnist(self):\n",
    "        self.load_images(data_list_index=0, file_name=self.train_file_name)\n",
    "        self.load_labels(data_list_index=1, file_name=self.train_label_file_name)\n",
    "        self.load_images(data_list_index=2, file_name=self.test_file_name)\n",
    "        self.load_labels(data_list_index=3, file_name=self.test_label_file_name)\n",
    "\n",
    "        self.assert_data_shape()\n",
    "\n",
    "    def assert_data_shape(self):\n",
    "        assert self.data[self.data_list[0]].shape == (60000, 28, 28, 1)\n",
    "        assert self.data[self.data_list[1]].shape == (60000, 1)\n",
    "        assert self.data[self.data_list[2]].shape == (10000, 28, 28, 1)\n",
    "        assert self.data[self.data_list[3]].shape == (10000, 1)\n",
    "\n",
    "    def preprocess_data(self):\n",
    "\n",
    "        self.data[self.data_list[0]] /= 255\n",
    "        self.data[self.data_list[2]] /= 255\n",
    "\n",
    "        self.data[self.data_list[1]] = Utility.one_hot_encode(self.data[self.data_list[1]])\n",
    "\n",
    "        assert self.data[self.data_list[1]].shape == (60000, 10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "outputs": [],
   "source": [
    "class Cifer10DataLoader:\n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.size = 32\n",
    "        self.color_channel = 3\n",
    "        self.per_batch_data_size = 10000\n",
    "        self.data = dict()\n",
    "\n",
    "    def load_data(self, file_name):\n",
    "        with open(os.path.join(self.data_path, file_name), 'rb') as f:\n",
    "\n",
    "            data_dict=pickle.load(f, encoding='latin1')\n",
    "\n",
    "            images = data_dict['data']\n",
    "            labels = data_dict['labels']\n",
    "\n",
    "            images = images.reshape(self.per_batch_data_size, self.color_channel, self.size, self.size).transpose(0,2,3,1).astype(\"float\")\n",
    "            labels = np.array(labels)\n",
    "            print(labels.shape)\n",
    "\n",
    "            return images, labels\n",
    "\n",
    "    def concatenate_data(self):\n",
    "        X1, Y1 = self.load_data('data_batch_1')\n",
    "        X2, Y2 = self.load_data('data_batch_2')\n",
    "        X3, Y3 = self.load_data('data_batch_3')\n",
    "        X4, Y4 = self.load_data('data_batch_4')\n",
    "        X5, Y5 = self.load_data('data_batch_5')\n",
    "\n",
    "        self.data['train_images'] = np.concatenate(\n",
    "            (\n",
    "                X1, X2, X3, X4, X5\n",
    "            ),\n",
    "            axis=0\n",
    "        )\n",
    "\n",
    "        self.data['train_labels'] = np.concatenate(\n",
    "            (\n",
    "                Y1.reshape(self.per_batch_data_size, 1),\n",
    "                Y2.reshape(self.per_batch_data_size, 1),\n",
    "                Y3.reshape(self.per_batch_data_size, 1),\n",
    "                Y4.reshape(self.per_batch_data_size, 1),\n",
    "                Y5.reshape(self.per_batch_data_size, 1)\n",
    "            ),\n",
    "            axis=0\n",
    "        )\n",
    "\n",
    "        X_test, Y_test = self.load_data('test_batch')\n",
    "\n",
    "        self.data['test_images'] = X_test\n",
    "        self.data['test_labels'] = Y_test.reshape(Y_test.shape[0], 1)\n",
    "\n",
    "        self.assert_data_shape()\n",
    "\n",
    "        for key, data in self.data.items():\n",
    "            print(f'Shape: {data.shape}')\n",
    "\n",
    "    def assert_data_shape(self):\n",
    "        assert self.data['train_images'].shape == (50000, 32, 32, 3)\n",
    "        assert self.data['train_labels'].shape == (50000, 1)\n",
    "        assert self.data['test_images'].shape  == (10000, 32, 32, 3)\n",
    "        assert self.data['test_labels'].shape  == (10000, 1)\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        self.data['train_images'] /= 255\n",
    "        self.data['test_images'] /= 255\n",
    "\n",
    "        self.data['train_labels'] = Utility.one_hot_encode(self.data['train_labels'])\n",
    "\n",
    "        assert self.data['train_labels'].shape == (50000, 10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "outputs": [],
   "source": [
    "class Convolution2D:\n",
    "\n",
    "    # class variable\n",
    "    layer_num = 1\n",
    "\n",
    "    def __init__(self, num_out_channel, filter_size, stride, padding_size):\n",
    "        self.num_out_channel = num_out_channel\n",
    "        self.filter_size = filter_size\n",
    "        self.stride = stride\n",
    "        self.padding_size = padding_size\n",
    "        self.h_prev, self.w_prev, self.num_channel_prev = None, None, None\n",
    "        self.h_new, self.w_new = None, None\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.output_tensor = None\n",
    "        self.cache = {}\n",
    "        self.layer_name = 'Conv2D__' + str(self.layer_num)\n",
    "        self.layer_num += 1\n",
    "        self.is_trainable = True\n",
    "        self.training_mode = False\n",
    "        self.activation_prev_current_layer_cache = None\n",
    "\n",
    "    def toggle_training_mode(self):\n",
    "        if not self.training_mode:\n",
    "            self.training_mode = True\n",
    "        else:\n",
    "            self.training_mode = False\n",
    "\n",
    "    def initialize_output_dimensions(self, prev_layer_output_dim):\n",
    "        \"\"\"\n",
    "        Initializes output dimensions with the dimension of the previous layers\n",
    "        :param prev_layer_output_dim: output dimension of the layer immediately before this layer\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.h_prev, self.w_prev , self.num_channel_prev = prev_layer_output_dim\n",
    "        self.h_new = (self.h_prev - self.filter_size + 2 * self.padding_size) // self.stride + 1\n",
    "        self.w_new = (self.w_prev - self.filter_size + 2 * self.padding_size) // self.stride + 1\n",
    "\n",
    "    def initialize_weights_biases(self):\n",
    "        \"\"\"\n",
    "        Initializes weights with the proper dimensions\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        print('he: parameters: ', (self.h_prev * self.w_prev * self.num_channel_prev))\n",
    "        self.W = np.random.randn(self.filter_size, self.filter_size, self.num_channel_prev, self.num_out_channel) * np.sqrt(2/(self.h_prev * self.w_prev * self.num_channel_prev))\n",
    "        self.b = np.zeros((1, 1, 1, self.num_out_channel))\n",
    "\n",
    "    def forward_wob(self, Z_prev, is_training):\n",
    "        \"\"\"\n",
    "        Performs a forward operation of the convolution layer\n",
    "        :param Z_prev: The activation of the previous layer\n",
    "        :param is_training: whether we are in training mode or not\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        assert Z_prev.shape == (self.h_prev, self.w_prev, self.num_channel_prev)\n",
    "        Z_prev = np.array(Z_prev, copy=True)\n",
    "\n",
    "        # create zero padded Z_prev\n",
    "        Z_prev_padded = Utility.zero_pad_without_batch(Z_prev, self.padding_size)\n",
    "        self.output_tensor = np.zeros((self.h_new, self.w_new, self.num_out_channel))\n",
    "\n",
    "        # Apply convolution operation over this zero padded previous activation\n",
    "        for row in range(self.h_new):\n",
    "\n",
    "            row_start = row * self.stride\n",
    "\n",
    "            for col in range(self.w_new):\n",
    "\n",
    "                col_start = col *  self.stride\n",
    "\n",
    "                for output_channel_index in range(self.num_out_channel):\n",
    "\n",
    "                    Z_prev_windowed = Z_prev_padded[\n",
    "                        row_start : row_start + self.filter_size,\n",
    "                        col_start : col_start + self.filter_size,\n",
    "                        :\n",
    "                    ]\n",
    "\n",
    "                    conv_step_W = self.W[:, :, :, output_channel_index]\n",
    "                    conv_step_b = self.b[:, :, :, output_channel_index]\n",
    "\n",
    "                    self.output_tensor[row, col, output_channel_index] = Utility.convolve_single_step(Z_prev_windowed, conv_step_W, conv_step_b)\n",
    "\n",
    "        # asserting output shape\n",
    "        assert(self.output_tensor.shape == (self.h_new, self.w_new, self.num_out_channel))\n",
    "\n",
    "        if is_training:\n",
    "            # cache some values\n",
    "            pass\n",
    "\n",
    "        # perform activation element wise in this case\n",
    "        print(f'In forward of Convolution output tensor shape before relu {self.output_tensor.shape}')\n",
    "        self.output_tensor = self.relu_activation.activation_f(self.output_tensor)\n",
    "\n",
    "        # asserting output shape\n",
    "        #assert(self.output_tensor.shape == (self.h_new, self.w_new, self.num_out_channel))\n",
    "        print(f'In forward of CNN output tensor shape after relu {self.output_tensor.shape}')\n",
    "\n",
    "    def forward_batch(self, Z_prev, is_training=True):\n",
    "        \"\"\"\n",
    "        Performs a forward operation of the convolution layer\n",
    "        :param Z_prev: The activation of the previous layer\n",
    "        :param is_training: whether we are in training mode or not\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        batch_size = Z_prev.shape[0]\n",
    "        assert Z_prev.shape == (batch_size, self.h_prev, self.w_prev, self.num_channel_prev)\n",
    "\n",
    "        self.output_tensor = np.zeros((batch_size, self.h_new, self.w_new, self.num_out_channel))\n",
    "        Z_prev = np.array(Z_prev, copy=True)\n",
    "\n",
    "        # create zero padded Z_prev\n",
    "        print(f'Z prev shape:{Z_prev.shape}')\n",
    "        Z_prev_padded = Utility.zero_pad(Z_prev, self.padding_size)\n",
    "\n",
    "        # Apply convolution operation over this zero padded previous activation\n",
    "        for image_index in range(batch_size):\n",
    "            current_Z_prev_padded = Z_prev_padded[image_index] # choosing a single tensor from the batch\n",
    "            for row in range(self.h_new):\n",
    "\n",
    "                row_start = row * self.stride\n",
    "\n",
    "                for col in range(self.w_new):\n",
    "\n",
    "                    col_start = col *  self.stride\n",
    "\n",
    "                    for output_channel_index in range(self.num_out_channel):\n",
    "\n",
    "                        Z_prev_windowed = current_Z_prev_padded[\n",
    "                                        row_start : row_start + self.filter_size,\n",
    "                                        col_start : col_start + self.filter_size,\n",
    "                                        :\n",
    "                                        ]\n",
    "\n",
    "                        conv_step_W = self.W[:, :, :, output_channel_index]\n",
    "                        conv_step_b = self.b[:, :, :, output_channel_index]\n",
    "\n",
    "                        self.output_tensor[image_index, row, col, output_channel_index] = Utility.convolve_single_step(Z_prev_windowed, conv_step_W, conv_step_b)\n",
    "\n",
    "        # asserting output shape\n",
    "        assert(self.output_tensor.shape == (batch_size, self.h_new, self.w_new, self.num_out_channel))\n",
    "        print(self.output_tensor.shape)\n",
    "\n",
    "        if is_training:\n",
    "            # cache some values\n",
    "            pass\n",
    "\n",
    "        # perform activation element wise in this case\n",
    "        #self.output_tensor = self.relu_activation.activation_f(self.output_tensor)\n",
    "\n",
    "    def forward(self, Z_prev):\n",
    "        \"\"\"\n",
    "        Performs a forward operation of the convolution layer\n",
    "        :param Z_prev: The activation of the previous layer\n",
    "        :param is_training: whether we are in training mode or not\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        print(Z_prev.shape)\n",
    "        batch_size = Z_prev.shape[0]\n",
    "        assert Z_prev.shape == (batch_size, self.h_prev, self.w_prev, self.num_channel_prev)\n",
    "\n",
    "        self.output_tensor = np.zeros((batch_size, self.h_new, self.w_new, self.num_out_channel))\n",
    "\n",
    "        # storing the activation for the bac propagation\n",
    "        self.activation_prev_current_layer_cache = Z_prev\n",
    "\n",
    "        Z_prev = np.array(Z_prev, copy=True)\n",
    "\n",
    "        # create zero padded Z_prev\n",
    "        #print(f'Z prev shape:{Z_prev.shape}')\n",
    "        Z_prev_padded = Utility.zero_pad(Z_prev, self.padding_size)\n",
    "\n",
    "        # Apply convolution operation over this zero padded previous activation\n",
    "        for row in range(self.h_new):\n",
    "\n",
    "            row_start = row * self.stride\n",
    "\n",
    "            for col in range(self.w_new):\n",
    "\n",
    "                col_start = col *  self.stride\n",
    "\n",
    "                for output_channel_index in range(self.num_out_channel):\n",
    "\n",
    "                    Z_prev_windowed = Z_prev_padded[:, row_start : row_start + self.filter_size,\n",
    "                                            col_start : col_start + self.filter_size, :]\n",
    "\n",
    "                    conv_step_W = self.W[:, :, :, output_channel_index]\n",
    "                    conv_step_b = self.b[:, :, :, output_channel_index]\n",
    "\n",
    "                    # print('z shape', Z_prev_windowed.shape)\n",
    "                    # print('w shape', conv_step_W.shape)\n",
    "\n",
    "                    # self.output_tensor[:, row, col, output_channel_index] = np.sum(\n",
    "                    #     Z_prev_windowed * conv_step_W,\n",
    "                    #     axis=(1,2,3)\n",
    "                    # ) + conv_step_b\n",
    "\n",
    "                    self.output_tensor[:, row, col, output_channel_index] = Utility.convolve_single_step_over_batch(\n",
    "                        Z_prev_windowed, conv_step_W, conv_step_b\n",
    "                    )\n",
    "\n",
    "            # asserting output shape\n",
    "            assert(self.output_tensor.shape == (batch_size, self.h_new, self.w_new, self.num_out_channel))\n",
    "            #print('output tensor shape:', self.output_tensor.shape)\n",
    "\n",
    "            if self.training_mode:\n",
    "                # cache some values\n",
    "                pass\n",
    "            # # perform activation element wise in this case\n",
    "            # self.output_tensor = self.relu_activation.activation_f(self.output_tensor)\n",
    "\n",
    "    def forward_(self, Z_prev):\n",
    "        \"\"\"\n",
    "        Performs a forward operation of the convolution layer\n",
    "        :param Z_prev: The activation of the previous layer\n",
    "        :param is_training: whether we are in training mode or not\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        print(Z_prev.shape)\n",
    "        batch_size = Z_prev.shape[0]\n",
    "        assert Z_prev.shape == (batch_size, self.h_prev, self.w_prev, self.num_channel_prev)\n",
    "\n",
    "        self.output_tensor = np.zeros((batch_size, self.h_new, self.w_new, self.num_out_channel))\n",
    "        Z_prev = np.array(Z_prev, copy=True)\n",
    "\n",
    "        # create zero padded Z_prev\n",
    "        #print(f'Z prev shape:{Z_prev.shape}')\n",
    "        Z_prev_padded = Utility.zero_pad(Z_prev, self.padding_size)\n",
    "\n",
    "        # Apply convolution operation over this zero padded previous activation\n",
    "        for row in range(self.h_new):\n",
    "\n",
    "            row_start = row * self.stride\n",
    "\n",
    "            for col in range(self.w_new):\n",
    "\n",
    "                col_start = col *  self.stride\n",
    "\n",
    "                Z_prev_windowed = Z_prev_padded[:,row_start : row_start + self.filter_size,\n",
    "                                  col_start : col_start + self.filter_size,:, np.newaxis]\n",
    "\n",
    "                conv_step_W = self.W[np.newaxis, :, :, :, :]\n",
    "                conv_step_b = self.b[:, :, :, :]\n",
    "\n",
    "                # print('z shape', Z_prev_windowed.shape)\n",
    "                # print('w shape', conv_step_W.shape)\n",
    "\n",
    "                # self.output_tensor[:, row, col, output_channel_index] = np.sum(\n",
    "                #     Z_prev_windowed * conv_step_W,\n",
    "                #     axis=(1,2,3)\n",
    "                # ) + conv_step_b\n",
    "\n",
    "                self.output_tensor[:, row, col, :] = Utility.convolve_single_step_over_batch(\n",
    "                    Z_prev_windowed, conv_step_W, conv_step_b\n",
    "                )\n",
    "\n",
    "            # asserting output shape\n",
    "            assert(self.output_tensor.shape == (batch_size, self.h_new, self.w_new, self.num_out_channel))\n",
    "            #print('output tensor shape:', self.output_tensor.shape)\n",
    "\n",
    "            if self.training_mode:\n",
    "                # cache some values\n",
    "                pass\n",
    "\n",
    "            return self.output_tensor\n",
    "\n",
    "            # # perform activation element wise in this case\n",
    "            # self.output_tensor = self.relu_activation.activation_f(self.output_tensor)\n",
    "\n",
    "    def get_output_tensor(self):\n",
    "        return self.output_tensor\n",
    "\n",
    "    def backward(self, dZ: np.array, learning_rate):\n",
    "        mini_batch_size = dZ.shape[0]\n",
    "\n",
    "        # initialize gradient shape\n",
    "        dActivation_prev, dW, db = self.initialize_gradients(mini_batch_size=mini_batch_size)\n",
    "\n",
    "        # do required padding\n",
    "        activation_prev_padded = Utility.zero_pad(self.activation_prev_current_layer_cache, self.padding_size)\n",
    "        dActivation_prev_padded = Utility.zero_pad(dActivation_prev, self.padding_size)\n",
    "\n",
    "        for row in range(self.h_new):\n",
    "\n",
    "            row_start = row * self.stride\n",
    "\n",
    "            for col in range(self.w_new):\n",
    "\n",
    "                col_start = col * self.stride\n",
    "\n",
    "                for output_channel_index in range(self.num_out_channel):\n",
    "\n",
    "                    # Use the corners to define the slice from a_prev_pad\n",
    "                    activation_slice = activation_prev_padded[:, row_start:row_start + self.filter_size, col_start:col_start + self.filter_size, :]\n",
    "\n",
    "                    # Update gradients for the window and the filter's parameters using the code formulas given above\n",
    "                    dActivation_prev_padded[:, row_start:row_start + self.filter_size, col_start:col_start + self.filter_size, :] += self.W[np.newaxis, :,:,:,output_channel_index] * dZ[: , row:row+1, col:col+1, np.newaxis,  output_channel_index] # done works\n",
    "\n",
    "                    # print('a_slice:', a_slice.shape)\n",
    "                    # print('dZ:', dZ[:, h:h+1, w:w+1, np.newaxis, c].shape)\n",
    "                    # s = a_slice * dZ[:, h:h+1, w:w+1, np.newaxis, c]\n",
    "                    # print('s shape:', s.shape)\n",
    "\n",
    "                    dW[:,:,:,output_channel_index] += np.sum(activation_slice * dZ[:, row:row+1, col:col+1, np.newaxis, output_channel_index], axis=0)\n",
    "\n",
    "                    db[:,:,:,output_channel_index] += np.sum(dZ[:, row:row+1, col:col+1, output_channel_index], axis=(0,1,2))\n",
    "\n",
    "        # unpad the dActivation_padded\n",
    "        if self.padding_size != 0:\n",
    "            dActivation_prev[:, :, :, :] = dActivation_prev_padded[:,\n",
    "                                       self.padding_size:-self.padding_size,\n",
    "                                       self.padding_size:-self.padding_size,\n",
    "                                       :]\n",
    "\n",
    "        assert dActivation_prev.shape == (mini_batch_size, self.h_prev, self.w_prev, self.num_channel_prev)\n",
    "\n",
    "        assert dW.shape == (self.filter_size, self.filter_size, self.num_channel_prev, self.num_out_channel)\n",
    "\n",
    "        assert db.shape == (1,1,1,self.num_out_channel)\n",
    "\n",
    "        self.update_CNN_parameters(dW, db, learning_rate=learning_rate)\n",
    "\n",
    "        return dActivation_prev\n",
    "\n",
    "    def initialize_gradients(self, mini_batch_size):\n",
    "        dActivation_prev = np.zeros((mini_batch_size, self.h_prev, self.w_prev, self.num_channel_prev))\n",
    "\n",
    "        dW = np.zeros((self.filter_size, self.filter_size, self.num_channel_prev, self.num_out_channel))\n",
    "\n",
    "        db = np.zeros((1,1,1, self.num_out_channel))\n",
    "\n",
    "        return dActivation_prev, dW, db\n",
    "\n",
    "\n",
    "    def update_CNN_parameters(self, dW : np.array, db: np.array, learning_rate: float):\n",
    "        self.W = self.W - learning_rate * dW\n",
    "        self.b = self.b - learning_rate * db\n",
    "\n",
    "    def print_layer_dimensions(self):\n",
    "        print(f'Output Tensor Dimensions: {self.output_tensor.shape}')\n",
    "        print(f'Weight Dimension: {self.W.shape}')\n",
    "        print(f'Bias Dimension: {self.b.shape}')\n",
    "\n",
    "    def get_output_dimension(self) -> Tuple:\n",
    "        return self.h_new, self.w_new, self.num_out_channel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "outputs": [],
   "source": [
    "class Flatten:\n",
    "    # class variable\n",
    "    layer_num = 1\n",
    "\n",
    "    def __init__(self):\n",
    "        self.input_dim = None\n",
    "        self.output_tensor = None\n",
    "        self.output_dim = None\n",
    "        self.h_prev, self.w_prev, self.num_channel_prev = None, None, None\n",
    "        self.layer_name = 'Flatten__' + str(self.layer_num)\n",
    "        self.layer_num += 1\n",
    "        self.is_trainable = False\n",
    "        self.shape_input_tensor = None\n",
    "\n",
    "    def initialize_flatten_layer_dimensions(self, prev_layer_output_dim):\n",
    "        \"\"\"\n",
    "        :param prev_layer_output_dim: prev layer output of shape (new_h, new_w, new_channel)\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.h_prev, self.w_prev, self.num_channel_prev  = prev_layer_output_dim\n",
    "        self.output_dim = self.h_prev * self.w_prev * self.num_channel_prev\n",
    "\n",
    "    def forward(self, Z_prev: np.array) -> np.array:\n",
    "        \"\"\"\n",
    "\n",
    "        :param Z_prev: Z_prev of shape (1/batch size, prev_h, prev_w, prev_color_channel)\n",
    "        :return: tensor of shape (1/batch size, prev_h * prev_w * prev_color_channel)\n",
    "        \"\"\"\n",
    "\n",
    "        self.shape_input_tensor = Z_prev.shape\n",
    "        self.output_tensor = Z_prev.reshape(Z_prev.shape[0], Z_prev.shape[1] * Z_prev.shape[2] * Z_prev.shape[3])\n",
    "\n",
    "        assert self.output_tensor.shape[1] == self.output_dim\n",
    "\n",
    "    def forward_wob(self, Z_prev: np.array) -> np.array:\n",
    "        self.output_tensor = Z_prev.reshape(1, Z_prev.shape[0] * Z_prev.shape[1] * Z_prev.shape[2])\n",
    "\n",
    "    def get_output_dimension(self):\n",
    "        return self.output_dim\n",
    "\n",
    "    def get_output_tensor(self) -> np.array:\n",
    "        return self.output_tensor\n",
    "\n",
    "    def backward(self, dZ: np.array):\n",
    "        \"\"\"\n",
    "\n",
    "        :param dZ: dZ shape: (batch size/1, prev_h * prev_w * prev_color_channel)\n",
    "        :return: tensor of shape: (batch size/1, prev_h, prev_w, prev_color_channel\n",
    "        \"\"\"\n",
    "\n",
    "        assert (dZ.shape[1], dZ.shape[2], dZ.shape[3]) == (self.shape_input_tensor)\n",
    "\n",
    "        dZ_reshaped = dZ.reshape(self.shape_input_tensor)\n",
    "\n",
    "        return dZ_reshaped\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "outputs": [],
   "source": [
    "class MaxPool:\n",
    "    # class variable\n",
    "    layer_num = 1\n",
    "\n",
    "    def __init__(self, filter_size, stride):\n",
    "        self.filter_size = filter_size\n",
    "        self.stride = stride\n",
    "        self.h_prev, self.w_prev, self.num_channel_prev = None, None, None\n",
    "        self.h_new, self.w_new, self.num_out_channel = None, None, None\n",
    "        self.output_tensor = None\n",
    "        self.cache = {}\n",
    "        self.layer_name = 'MaxPool__'+ str(self.layer_num)\n",
    "        self.layer_num += 1\n",
    "        self.is_trainable = False\n",
    "        self.training_mode = False\n",
    "        self.activation_prev_cached = None\n",
    "\n",
    "    def toggle_training_mode(self):\n",
    "        if not self.training_mode:\n",
    "            self.training_mode = True\n",
    "        else:\n",
    "            self.training_mode = False\n",
    "\n",
    "    def initialize_max_pool_params(self, prev_layer_output_dim):\n",
    "        \"\"\"\n",
    "        Initializes output dimensions with the dimension of the previous layers\n",
    "        :param prev_layer_output_dim: output dimension of the layer immediately before this layer\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.h_prev, self.w_prev , self.num_channel_prev = prev_layer_output_dim\n",
    "        self.h_new = int((self.h_prev - self.filter_size) / self.stride + 1)\n",
    "        self.w_new = int((self.w_prev - self.filter_size) / self.stride + 1)\n",
    "        self.num_out_channel = self.num_channel_prev\n",
    "\n",
    "    def forward_wob(self, Z_prev, is_training):\n",
    "\n",
    "        #print('prev z shape in maxpool:', Z_prev.shape)\n",
    "        assert Z_prev.shape == (self.h_prev, self.w_prev, self.num_channel_prev)\n",
    "        #print('here')\n",
    "        Z_prev = np.array(Z_prev, copy=True)\n",
    "        #print('there')\n",
    "        self.output_tensor = np.zeros((self.h_new, self.w_new, self.num_out_channel))\n",
    "\n",
    "        #print('Going for max pooling')\n",
    "        # Apply convolution operation over this zero padded previous activation\n",
    "        for row in range(self.h_new):\n",
    "\n",
    "            row_start = row * self.stride\n",
    "\n",
    "            for col in range(self.w_new):\n",
    "\n",
    "                col_start = col *  self.stride\n",
    "\n",
    "                for output_channel_index in range(self.num_out_channel):\n",
    "\n",
    "                    Z_prev_windowed = Z_prev[\n",
    "                                      row_start : row_start + self.filter_size,\n",
    "                                      col_start : col_start + self.filter_size,\n",
    "                                      output_channel_index\n",
    "                                      ]\n",
    "\n",
    "                    self.output_tensor[row, col, output_channel_index] = Utility.get_max_pool_window(Z_prev_windowed)\n",
    "\n",
    "        assert self.output_tensor.shape == (self.h_new, self.w_new, self.num_out_channel)\n",
    "        if is_training:\n",
    "            pass\n",
    "        print('Max pool forward done')\n",
    "\n",
    "    def forward(self, Z_prev):\n",
    "\n",
    "        batch_size = Z_prev.shape[0]\n",
    "        assert Z_prev.shape == (batch_size, self.h_prev, self.w_prev, self.num_channel_prev)\n",
    "\n",
    "        self.activation_prev_cached = Z_prev\n",
    "\n",
    "        self.output_tensor = np.zeros((batch_size, self.h_new, self.w_new, self.num_out_channel))\n",
    "        Z_prev = np.array(Z_prev, copy=True)\n",
    "\n",
    "        # Apply convolution operation over this zero padded previous activation\n",
    "        for row in range(self.h_new):\n",
    "\n",
    "            row_start = row * self.stride\n",
    "\n",
    "            for col in range(self.w_new):\n",
    "\n",
    "                col_start = col *  self.stride\n",
    "\n",
    "                for output_channel_index in range(self.num_out_channel):\n",
    "\n",
    "                    Z_prev_windowed = Z_prev[:,\n",
    "                                    row_start : row_start + self.filter_size,\n",
    "                                    col_start : col_start + self.filter_size,\n",
    "                                    output_channel_index\n",
    "                                      ]\n",
    "\n",
    "                    self.output_tensor[:, row, col, output_channel_index] = Utility.get_max_pool_window_over_batch(Z_prev_windowed)\n",
    "\n",
    "        assert self.output_tensor.shape == (batch_size, self.h_new, self.w_new, self.num_out_channel)\n",
    "        if self.training_mode:\n",
    "            pass\n",
    "\n",
    "    def forward_(self, Z_prev):\n",
    "\n",
    "        batch_size = Z_prev.shape[0]\n",
    "        assert Z_prev.shape == (batch_size, self.h_prev, self.w_prev, self.num_channel_prev)\n",
    "        self.activation_prev_cached = Z_prev\n",
    "\n",
    "        self.output_tensor = np.zeros((batch_size, self.h_new, self.w_new, self.num_out_channel))\n",
    "        Z_prev = np.array(Z_prev, copy=True)\n",
    "\n",
    "        # Apply convolution operation over this zero padded previous activation\n",
    "        for row in range(self.h_new):\n",
    "\n",
    "            row_start = row * self.stride\n",
    "\n",
    "            for col in range(self.w_new):\n",
    "\n",
    "                col_start = col *  self.stride\n",
    "\n",
    "\n",
    "                Z_prev_windowed = Z_prev[:,\n",
    "                                    row_start : row_start + self.filter_size,\n",
    "                                    col_start : col_start + self.filter_size,\n",
    "                                      :\n",
    "                                    ]\n",
    "\n",
    "                self.output_tensor[:, row, col, :] = Utility.get_max_pool_window_over_batch(Z_prev_windowed)\n",
    "\n",
    "        assert self.output_tensor.shape == (batch_size, self.h_new, self.w_new, self.num_out_channel)\n",
    "        if self.training_mode:\n",
    "            pass\n",
    "\n",
    "    def print_layer_dimensions(self):\n",
    "        print(f'Output Tensor Dimensions: {self.output_tensor.shape}')\n",
    "\n",
    "    def get_output_tensor(self):\n",
    "        return self.output_tensor\n",
    "\n",
    "    def backward(self, dActivation_pool: np.array):\n",
    "\n",
    "        dActivation_pool_prev = np.zeros(self.activation_prev_cached.shape)\n",
    "        #print(self.activation_prev_cached.shape)\n",
    "        mini_batch_size = self.activation_prev_cached.shape[0]\n",
    "        print('mini batch size:', mini_batch_size)\n",
    "\n",
    "        for batch_index in range(mini_batch_size):\n",
    "            # first select a fixed tensor from the batch\n",
    "            activation_prev = self.activation_prev_cached[batch_index, :, :, :]\n",
    "\n",
    "            # print('dA shape:', dA.shape)\n",
    "            # print('h:', self.h_new)\n",
    "            # print('w:', self.w_new)\n",
    "            # print('c:', self.num_out_channel)\n",
    "\n",
    "            for row in range(self.h_new):\n",
    "                row_start = row * self.stride\n",
    "                for col in range(self.w_new):\n",
    "                    col_start = col * self.stride\n",
    "                    for output_channel_index in range(self.num_out_channel):\n",
    "                        activation_prev_window = activation_prev[\n",
    "                            row_start: row_start + self.filter_size,\n",
    "                            col_start: col_start + self.filter_size,\n",
    "                            output_channel_index\n",
    "                        ]\n",
    "\n",
    "                        print('before the mask:', activation_prev_window)\n",
    "                        mask = Utility.get_mask_from_tensor(activation_prev_window)\n",
    "                        print('the mashk:', mask)\n",
    "                        # print(dActivation_pool.shape)\n",
    "                        # print('batch index: ', batch_index)\n",
    "                        # print('row: ', row)\n",
    "                        # print('col: ', col)\n",
    "                        # print('output channel index: ', output_channel_index)\n",
    "                        s = dActivation_pool[batch_index, row, col, output_channel_index]\n",
    "                        dActivation_pool_prev[\n",
    "                        batch_index,\n",
    "                        row: row + self.filter_size,\n",
    "                        col: col + self.filter_size,\n",
    "                        output_channel_index\n",
    "                        ] += mask * dActivation_pool[batch_index, row, col, output_channel_index]\n",
    "\n",
    "                        print(f'for batch {batch_index} {dActivation_pool_prev[batch_index,row: row + self.filter_size,col: col + self.filter_size,output_channel_index]}')\n",
    "\n",
    "        # print(dActivation_pool.shape)\n",
    "        print(self.activation_prev_cached.shape)\n",
    "        assert dActivation_pool_prev.shape == self.activation_prev_cached.shape\n",
    "        return dActivation_pool_prev\n",
    "\n",
    "        return dActivation_pool\n",
    "\n",
    "    def backward_(self, dActivation_pool: np.array):\n",
    "\n",
    "        dActivation_pool_prev = np.zeros(self.activation_prev_cached.shape)\n",
    "        print(self.activation_prev_cached.shape)\n",
    "        mini_batch_size = self.activation_prev_cached.shape[0]\n",
    "        print('mini batch size:', mini_batch_size)\n",
    "\n",
    "        for row in range(self.h_new):\n",
    "            row_start = row * self.stride\n",
    "            for col in range(self.w_new):\n",
    "                col_start = col * self.stride\n",
    "                for output_channel_index in range(self.num_out_channel):\n",
    "                    activation_prev_window = self.activation_prev_cached[\n",
    "                                                :,\n",
    "                                                row_start: row_start + self.filter_size,\n",
    "                                                col_start: col_start + self.filter_size,\n",
    "                                                output_channel_index\n",
    "                                                 ]\n",
    "                    print('before the mask:', activation_prev_window)\n",
    "                    mask = Utility.get_mask_from_tensor_batch(activation_prev_window)\n",
    "                    print('the mask:', mask)\n",
    "\n",
    "                    dActivation_pool_prev[\n",
    "                        :,\n",
    "                        row: row + self.filter_size,\n",
    "                        col: col + self.filter_size,\n",
    "                        output_channel_index\n",
    "                    ] += dActivation_pool[:, np.newaxis, np.newaxis, row, col, output_channel_index] * mask\n",
    "\n",
    "                    print('vectorizing: ', dActivation_pool_prev[\n",
    "                    :,\n",
    "                    row: row + self.filter_size,\n",
    "                    col: col + self.filter_size,\n",
    "                    output_channel_index\n",
    "                    ])\n",
    "\n",
    "\n",
    "        assert dActivation_pool_prev.shape == self.activation_prev_cached.shape\n",
    "        return dActivation_pool_prev\n",
    "\n",
    "    def get_output_dimension(self):\n",
    "        return self.h_new, self.w_new, self.num_out_channel"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "outputs": [],
   "source": [
    "class DenseLayer:\n",
    "\n",
    "    # class variable\n",
    "    layer_num = 1\n",
    "\n",
    "    def __init__(self, num_units):\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "        self.dW = None\n",
    "        self.db = None\n",
    "        self.num_units = num_units\n",
    "        self.A_prev_layer_cached = None\n",
    "        self.output_tensor = None\n",
    "        self.layer_name = 'Dense__' + str(self.layer_num)\n",
    "        self.layer_num += 1\n",
    "        self.is_trainable = True\n",
    "        self.training_mode = False\n",
    "\n",
    "    def toggle_training_mode(self):\n",
    "        if not self.training_mode:\n",
    "            self.training_mode = True\n",
    "        else:\n",
    "            self.training_mode = False\n",
    "\n",
    "\n",
    "    def initialize_dense_layer_weights_biases(self, prev_layer_output_dim):\n",
    "        self.W = np.random.randn(self.num_units, prev_layer_output_dim) * np.sqrt(2/prev_layer_output_dim)\n",
    "        self.b = np.zeros((self.num_units, 1)) # will be broadcast to (hidden_units, batch_size) before addition\n",
    "\n",
    "        print('W dense: ', self.W)\n",
    "\n",
    "    def forward(self, A_prev_layer):\n",
    "        \"\"\"\n",
    "        :param A_prev_layer: tensor of shape (batch, prev_flattened_shape)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        print('starting of dense: ', A_prev_layer)\n",
    "        print(f'A_prev_layer shape: {A_prev_layer.shape}')\n",
    "        assert A_prev_layer.shape[1] == self.W.shape[1]\n",
    "\n",
    "        self.A_prev_layer_cached = A_prev_layer\n",
    "\n",
    "        A_prev_layer = np.array(A_prev_layer, copy=True)\n",
    "        A_prev_layer_reshaped = A_prev_layer.T\n",
    "\n",
    "        Z = np.dot(self.W, A_prev_layer_reshaped) + self.b\n",
    "\n",
    "        self.output_tensor = Z.T # converting to (batch_size, num_units)\n",
    "        print('dense layer output: ',Z)\n",
    "\n",
    "        # assert the output tensor shape should be (num_hidden_units, batch size)\n",
    "        assert self.output_tensor.shape == (A_prev_layer_reshaped.shape[1], self.num_units)\n",
    "\n",
    "        if self.training_mode:\n",
    "            pass\n",
    "\n",
    "    def get_output_tensor(self):\n",
    "        return self.output_tensor\n",
    "\n",
    "    def backward(self, dZ : np.array, learning_rate):\n",
    "        \"\"\"\n",
    "\n",
    "        :param dZ: the gradient of loss with respect to this layers output Z. dZ = dL/dZ. Shape: (batch size , num_units)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # for softmax activation after this, the value of dZ = y_pred - y\n",
    "        A_prev_layer = self.A_prev_layer_cached\n",
    "        mini_batch_size = dZ.shape[0]\n",
    "        dW = (1/mini_batch_size) * np.dot(dZ.T, A_prev_layer) # (num unit, 1) * (1, x) --> (num unit, x)\n",
    "        db = (1/mini_batch_size) * np.sum(dZ.T, axis=1, keepdims=True)\n",
    "        dA_prev_layer = np.dot(dZ, self.W)\n",
    "\n",
    "        print(dW.shape)\n",
    "        print(db.shape)\n",
    "        print(self.b.shape)\n",
    "        assert dW.shape == self.W.shape\n",
    "        assert db.shape == self.b.shape\n",
    "        assert dA_prev_layer.shape == A_prev_layer.shape\n",
    "\n",
    "        self.update_parameters(dW, db, learning_rate=learning_rate)\n",
    "        return dA_prev_layer\n",
    "\n",
    "    def get_output_dimension(self):\n",
    "        return self.num_units\n",
    "\n",
    "    def print_layer_dimensions(self):\n",
    "        print(f'Output Tensor Dimensions: {self.output_tensor.shape}')\n",
    "        print(f'Weight Dimension: {self.W.shape}')\n",
    "        print(f'Bias Dimension: {self.b.shape}')\n",
    "\n",
    "    def update_parameters(self, dW: np.array, db: np.array, learning_rate: float):\n",
    "        self.W = self.W - learning_rate * dW\n",
    "        self.b = self.b - learning_rate * db\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "outputs": [],
   "source": [
    "class Utility:\n",
    "\n",
    "    @staticmethod\n",
    "    def one_hot_encode(y_true):\n",
    "        # Define the One-hot Encoder\n",
    "        ohe = preprocessing.OneHotEncoder()\n",
    "        ohe.fit(y_true)\n",
    "        y_true = ohe.transform(y_true).toarray()\n",
    "        return y_true\n",
    "\n",
    "    @staticmethod\n",
    "    def zero_pad(tensor, pad_size):\n",
    "        \"\"\"\n",
    "        :param tensor: tensor of shape (batch_size, h, w, num_channel)\n",
    "        :return: padded tensor of shape (h + 2 * pad_size, w + 2 * pad_size, num_channel)\n",
    "        \"\"\"\n",
    "        return np.pad(tensor, ((0,0), (pad_size, pad_size), (pad_size, pad_size), (0,0)), mode='constant', constant_values=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def zero_pad_without_batch(tensor, pad_size):\n",
    "        \"\"\"\n",
    "        :param tensor: tensor of shape (h, w, num_channel)\n",
    "        :return: padded tensor of shape (h + 2 * pad_size, w + 2 * pad_size, num_channel)\n",
    "        \"\"\"\n",
    "        return np.pad(tensor, ((pad_size, pad_size), (pad_size, pad_size), (0,0)), mode='constant', constant_values=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def convolve_single_step(Z_prev_windowed, W, b):\n",
    "        \"\"\"\n",
    "        :param Z_prev_windowed: window of shape (F, F, num_channel_Z_prev)\n",
    "        :param W: kernel/filter/weight of shape (F, F, num_channel_Z_prev)\n",
    "        :param b: bias term of shape (1, 1, 1)\n",
    "        :return: scaler convolved value\n",
    "        \"\"\"\n",
    "        return np.multiply(Z_prev_windowed, W).sum() + float(b)\n",
    "\n",
    "    @staticmethod\n",
    "    def convolve_single_step_over_batch(tensor, W, b):\n",
    "        # print('slice shape:', tensor.shape)\n",
    "        # print('W shape:', W.shape)\n",
    "        # print('b shape:', b.shape)\n",
    "        return np.sum(tensor * W, axis=(1,2,3)) + b\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def get_max_pool_window(Z_prev_windowed):\n",
    "        return Z_prev_windowed.max()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_max_pool_window_over_batch(Z_prev_windowed: np.array):\n",
    "        #print('In max pool', Z_prev_windowed.shape)\n",
    "        #print(np.max(Z_prev_windowed, axis=(1,2)))\n",
    "        return np.max(Z_prev_windowed, axis=(1,2))\n",
    "\n",
    "    @staticmethod\n",
    "    def create_mini_batches(X: np.array, Y: np.array, mini_batch_size: int):\n",
    "        total_data = X.shape[0]\n",
    "        for index in range(0, total_data, mini_batch_size):\n",
    "            start_index = index\n",
    "            end_index = min(start_index + mini_batch_size, total_data)\n",
    "            yield X[start_index: end_index,...], Y[start_index: end_index, ...]\n",
    "\n",
    "    @staticmethod\n",
    "    def get_mask_from_tensor(tensor: np.array):\n",
    "        return tensor == tensor.max()\n",
    "\n",
    "    @staticmethod\n",
    "    def get_mask_from_tensor_batch(tensor: np.array):\n",
    "        return tensor == tensor.max(axis=(1,2)).reshape(tensor.shape[0], 1, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 1)\n",
      "(256, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "a = np.random.randn(256,28,28)\n",
    "print(a.max(axis=(1,2)).reshape(-1,1).shape)\n",
    "s = a == a.max(axis=(1,2)).reshape(a.shape[0], 1, 1 )\n",
    "print(s.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "outputs": [],
   "source": [
    "class ReLUActivation:\n",
    "\n",
    "    # class variable\n",
    "    layer_num = 1\n",
    "    def __init__(self):\n",
    "        self.layer_name = 'ReLU__' + str(self.layer_num)\n",
    "        self.layer_num += 1\n",
    "        self.input_tensor_dimension = None\n",
    "        self.output_tensor = None\n",
    "        self.is_trainable = False\n",
    "\n",
    "    def set_input_tensor_dimension(self, prev_layer_tensor_dimension):\n",
    "        self.input_tensor_dimension = prev_layer_tensor_dimension\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        print('Relu Input Tensor Shape: ', tensor.shape)\n",
    "        # print('in relu:', tensor)\n",
    "        # print('After: ', np.maximum(tensor, 0))\n",
    "        self.output_tensor = np.maximum(tensor, 0)\n",
    "\n",
    "    def get_output_dimension(self):\n",
    "        return self.input_tensor_dimension\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(dA: np.array):\n",
    "        return np.where(dA > 0, 1, 0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "outputs": [],
   "source": [
    "@dataclass(unsafe_hash=True)\n",
    "class InputLayer:\n",
    "    \"\"\"\n",
    "    Class for saving input dimension\n",
    "    \"\"\"\n",
    "    input_dimension: np.array\n",
    "    is_trainable: bool\n",
    "    layer_name: str"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "outputs": [],
   "source": [
    "class SoftmaxActivation:\n",
    "\n",
    "    # class variable\n",
    "    layer_num = 1\n",
    "    def __init__(self):\n",
    "        self.input_tensor_dimension = None\n",
    "        self.layer_name = 'Softmax__' + str(self.layer_num)\n",
    "        self.layer_num += 1\n",
    "        self.output_tensor = None\n",
    "        self.is_trainable = False\n",
    "\n",
    "    def set_input_tensor_dimension(self, prev_layer_tensor_dimension):\n",
    "        self.input_tensor_dimension = prev_layer_tensor_dimension\n",
    "\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        print('softmax tensor shape:', tensor.shape) # expected tensor shape: (1/batch-size, classes)\n",
    "        exponent = np.exp(tensor - np.max(tensor, axis=1, keepdims=True))\n",
    "\n",
    "        print('softmax exp shape',exponent.shape)\n",
    "        summation_along_batch = np.sum(exponent, axis=1, keepdims=True)\n",
    "\n",
    "        print('softmax sum shape:', summation_along_batch.shape)\n",
    "\n",
    "        self.output_tensor = exponent/summation_along_batch\n",
    "\n",
    "        return self.output_tensor\n",
    "\n",
    "\n",
    "    def get_output_dimension(self):\n",
    "        return self.input_tensor_dimension\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(dA: np.array):\n",
    "        \"\"\" passes the backward gradient\n",
    "        :param dA: the gradient wrt to the activation softmax\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return dA"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "outputs": [],
   "source": [
    "class CrossEntropyLoss:\n",
    "    epsilon = 1e-50\n",
    "\n",
    "    def compute_cost(self, y_pred, y):\n",
    "        mini_batch_size = y.shape[0]\n",
    "        clipped_y_pred = np.clip(y_pred, self.epsilon, 1.0)\n",
    "        cross_entropy_loss = -(1/mini_batch_size) * np.sum(np.multiply(y, np.log(clipped_y_pred)))\n",
    "        return cross_entropy_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "outputs": [],
   "source": [
    "class GradientDescent:\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = None # a list of layer object according to input\n",
    "        self.layer_w_gradients = dict() # {'layer name': dw}\n",
    "        self.layer_b_gradients = dict() # {'layer name': db}\n",
    "        self.optimizer = None\n",
    "        self.cost_function = None\n",
    "\n",
    "    def add(self, layer_list):\n",
    "        self.layers = layer_list\n",
    "\n",
    "    def initializer_layer_params(self):\n",
    "        \"\"\"\n",
    "        This method initializes the layers in the model providing the input dimension that the layers expect to get\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        #InputLayer(input_dimension=input_dimension, is_trainable=False, layer_name='Input')\n",
    "\n",
    "        assert self.layers[0].layer_name == 'Input'\n",
    "\n",
    "        for previous_layer, current_layer in zip(self.layers, self.layers[1:]):\n",
    "            prev_output_dim = None\n",
    "            if previous_layer.layer_name == 'Input':\n",
    "                prev_output_dim = previous_layer.input_dimension # H, W, Color Channel\n",
    "            else:\n",
    "                prev_output_dim = previous_layer.get_output_dimension()\n",
    "\n",
    "\n",
    "            if current_layer.layer_name.startswith('Conv2D__'):\n",
    "                current_layer.initialize_output_dimensions(prev_output_dim) # H, W, Color Channel\n",
    "                current_layer.initialize_weights_biases()\n",
    "            elif current_layer.layer_name.startswith('MaxPool__'):\n",
    "                print(prev_output_dim)\n",
    "                current_layer.initialize_max_pool_params(prev_output_dim) # H, W, Color Channel\n",
    "            elif current_layer.layer_name.startswith('Dense__'):\n",
    "                current_layer.initialize_dense_layer_weights_biases(prev_output_dim) # flatten layer dimension\n",
    "            elif current_layer.layer_name.startswith('Flatten__'):\n",
    "                current_layer.initialize_flatten_layer_dimensions(prev_output_dim) # (new_h, new_w, new_channel)\n",
    "            elif current_layer.layer_name.startswith(\"ReLU\") or current_layer.layer_name.startswith(\"Softmax\") :\n",
    "                current_layer.set_input_tensor_dimension(prev_output_dim) # for the activation layers\n",
    "\n",
    "\n",
    "    def compile(self, optimizer, cost_function):\n",
    "        self.optimizer = optimizer\n",
    "        self.cost_function = cost_function\n",
    "\n",
    "    def train(self, training_data, validation_data, epochs=5, learning_rate=0.01, mini_batch_size=32):\n",
    "\n",
    "        X_val, y_Val = validation_data\n",
    "\n",
    "        for e in range(epochs):\n",
    "            # each epoch will run through a training once and update weights\n",
    "            print(f'Running Epoch: {e}')\n",
    "            X_train, Y_train = training_data\n",
    "            total_data = X_train.shape[0]\n",
    "            num_of_mini_batches = total_data//mini_batch_size\n",
    "\n",
    "\n",
    "            # first we create the mini batches and then run training step through it\n",
    "            i = 1\n",
    "            for X, Y in Utility.create_mini_batches(X=X_train, Y=Y_train, mini_batch_size=mini_batch_size):\n",
    "                \"\"\"\n",
    "                X shape --> (mini_batch_size, h, w, color_channel)\n",
    "                Y shape --> (mini_batch_size, num_of_class) (one hot encoded vector)\n",
    "                \"\"\"\n",
    "                Y_pred = self.forward_propagation(X)\n",
    "                print('final input shape', Y_pred.shape)\n",
    "                print('final output:', Y_pred)\n",
    "                self.backward_propagation(Y_pred,Y, learning_rate)\n",
    "                Loss = self.cost_function.compute_cost(Y_pred, Y)\n",
    "                print('Loss is : ', Loss)\n",
    "                print(\"\\rProgress {:1.1%}\".format(i / num_of_mini_batches), end=\"\")\n",
    "                print()\n",
    "                i += 1\n",
    "                break\n",
    "\n",
    "            print()\n",
    "            print(f'Cost After a Epoch {e+1}: {Loss * 100}')\n",
    "\n",
    "            # perform validation step here\n",
    "        print('Finish Training!')\n",
    "\n",
    "    def predict(self):\n",
    "        pass\n",
    "\n",
    "    def forward_propagation(self, X_train) -> np.array:\n",
    "        \"\"\"\n",
    "        Performs a forward pass through the network\n",
    "        :param X_train: nd training tensor (batch_size, h, w, color_channel)\n",
    "        :param is_training: whether we are training or not\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        input = X_train\n",
    "        for layer in self.layers[1:]:\n",
    "            # skipping the input layer\n",
    "            print(f'Forward for layer {layer.layer_name}')\n",
    "            if not layer.layer_name.startswith('ReLU') and not layer.layer_name.startswith('Softmax') and not layer.layer_name.startswith('Flatten'):\n",
    "                layer.toggle_training_mode() # toggling training mode for a layer\n",
    "            layer.forward(input)\n",
    "            input = layer.output_tensor # getting the output tensor of the layer to be the input tensor to the next\n",
    "            #print(f'Output of layer {layer.layer_name}\\n: {input}')\n",
    "\n",
    "            if not layer.layer_name.startswith('ReLU') and not layer.layer_name.startswith('Softmax') and not layer.layer_name.startswith('Flatten'):\n",
    "                layer.toggle_training_mode() # turning the training mode off here\n",
    "\n",
    "\n",
    "        return input\n",
    "\n",
    "    def backward_propagation(self, Y_out, Y, learning_rate):\n",
    "        dA = Y_out - Y\n",
    "        for layer in reversed(self.layers[1:]):\n",
    "            if layer.is_trainable:\n",
    "                dA = layer.backward(dA, learning_rate)\n",
    "            else:\n",
    "                dA = layer.backward(dA)\n",
    "\n",
    "\n",
    "    def update_layer_parameters(self, learning_rate):\n",
    "        pass\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "outputs": [],
   "source": [
    "mnist = MnistDataLoader('./dataset/mnist')\n",
    "mnist.load_mnist()\n",
    "mnist.preprocess_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.])"
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.data[mnist.data_list[1]][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he: paramaters:  140\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10,5,7,4) # h, w, c of previous layer\n",
    "cnn = Convolution2D(num_out_channel=8, filter_size=3, stride=2, padding_size=1)\n",
    "cnn.initialize_output_dimensions((A_prev.shape[1], A_prev.shape[2], A_prev.shape[3]))\n",
    "cnn.initialize_weights_biases()\n",
    "#cnn.print_layer_dimensions()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5, 7, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[[[ 2.08818862e-01,  4.49892798e-01,  4.53119627e-01,\n          -5.48086622e-01, -9.38477514e-01, -9.40816896e-01,\n           2.60863407e-01,  4.09591809e-01],\n         [ 4.24660413e-01, -5.20144968e-01,  2.26799200e-01,\n           5.07732164e-01, -8.76501129e-01, -1.46584834e+00,\n          -9.71281999e-01,  4.09039509e-01],\n         [-7.02462357e-01, -9.47644035e-01, -6.18223763e-01,\n          -2.25046977e-01, -1.91126676e-01, -6.38083346e-02,\n          -6.57634029e-01,  2.63664460e-01],\n         [-1.96984894e-02, -3.52576554e-01, -3.86614502e-01,\n          -2.07147601e-01, -1.19559645e+00,  2.38614404e-01,\n          -2.98361690e-01,  1.23058440e-01]],\n\n        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00]],\n\n        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00]]],\n\n\n       [[[-5.04198400e-01, -6.42540657e-04,  4.49965326e-01,\n          -3.75842768e-01, -1.57214364e-01, -1.03978455e-01,\n           2.92422990e-01,  3.91415615e-01],\n         [ 1.28307374e-02, -4.35124763e-01, -4.28537984e-01,\n          -4.46232252e-01,  6.22631798e-01, -8.82451824e-01,\n          -8.12365510e-01,  1.43868493e-01],\n         [-4.51719629e-01, -4.62462174e-01,  8.75840842e-01,\n          -2.12768789e-01,  1.67766791e-01, -1.45532344e-01,\n          -6.26929764e-01,  5.50818486e-01],\n         [ 5.02210384e-02,  5.62731682e-01, -2.69058861e-01,\n          -1.00449513e-01,  2.60599597e-01, -9.37688217e-01,\n          -1.23149717e-01, -9.43957459e-01]],\n\n        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00]],\n\n        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00]]],\n\n\n       [[[-4.08878078e-01, -2.21740412e-01,  7.30870783e-01,\n          -1.56902392e-01,  8.28146736e-01,  9.17103248e-03,\n          -2.70733190e-01,  1.09864649e+00],\n         [-4.73789359e-01,  2.98005005e-01, -5.51168724e-01,\n           5.29270467e-02,  1.62911967e-02, -2.35305618e-01,\n          -2.61979589e-01, -8.48513968e-01],\n         [-2.29847474e-01,  1.05467844e+00,  9.28319154e-01,\n           2.42275932e-01, -4.26842219e-02,  3.12514996e-01,\n          -1.77849525e-01,  1.36249724e-01],\n         [-5.10593411e-02, -3.26603584e-01,  6.38980597e-01,\n           2.76844844e-01,  9.39885387e-01, -6.72199514e-01,\n           1.19092332e-01, -1.33034770e-01]],\n\n        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00]],\n\n        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00]]],\n\n\n       [[[-1.12381891e-01,  9.87474678e-01,  3.64536459e-01,\n          -2.07983251e-01,  5.56900849e-01,  1.69196802e-01,\n          -6.27523783e-01,  1.96392336e-01],\n         [-2.76935977e-01, -2.58595828e-01,  5.76776415e-01,\n          -1.89966061e-01,  3.20455092e-01, -3.09296035e-01,\n          -5.73144734e-01,  8.75951262e-01],\n         [-1.03163380e+00,  5.06066942e-01,  9.55075267e-01,\n          -4.22326240e-01, -3.78918966e-01, -4.51863670e-02,\n          -6.06282064e-01,  1.80541668e-01],\n         [-1.13739999e-02,  5.37213704e-01,  5.56044537e-02,\n           2.85984391e-01,  4.42181479e-01, -7.44873982e-02,\n           5.38160196e-01, -2.00951551e-01]],\n\n        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00]],\n\n        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00]]],\n\n\n       [[[-2.77696629e-01, -2.98343410e+00, -1.49517999e-01,\n           8.24715300e-01, -3.15574534e-01, -9.97988937e-02,\n          -5.66607043e-01, -8.33646827e-02],\n         [-5.13054487e-01,  1.07536683e+00, -2.09473123e+00,\n          -6.20361174e-01, -1.48659414e-01,  1.29055287e+00,\n          -2.36274157e-01, -5.27080149e-01],\n         [-4.40169442e-02, -3.06814203e-01,  7.23097876e-01,\n           2.54783750e-01,  1.51916822e-01,  1.10344565e+00,\n          -1.94789486e-01,  4.95935855e-01],\n         [-2.12543577e-01, -9.88983171e-01,  9.63270002e-01,\n           2.45140623e-01, -1.53498504e-01, -3.77266843e-01,\n          -3.37741572e-01, -4.51121333e-01]],\n\n        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00]],\n\n        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00]]],\n\n\n       [[[-3.69504681e-01,  1.42879299e+00,  7.35101967e-02,\n          -3.19805987e-01,  5.47208821e-02,  4.28041640e-01,\n           8.41221116e-01, -1.42635497e-01],\n         [-8.09037289e-03, -4.44982133e-01, -5.02513263e-01,\n          -2.53459422e-01, -2.86344727e-01,  2.41203412e-01,\n           5.10919189e-01, -3.70098614e-01],\n         [ 4.36868100e-01, -7.07533724e-01,  3.57555483e-01,\n           8.03311580e-02,  3.76915456e-01,  6.47876326e-02,\n           4.77736475e-01,  7.51494476e-01],\n         [ 1.45517779e-01,  2.56939158e-01, -7.82463125e-01,\n           5.27602125e-01,  4.71929090e-01, -6.00542482e-03,\n           1.54225007e-02, -9.36352678e-01]],\n\n        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00]],\n\n        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00]]],\n\n\n       [[[ 1.13029029e-01, -6.54880138e-01,  3.40846991e-01,\n           2.35127582e-01,  1.88284924e-01, -7.36058247e-01,\n          -3.05000496e-01, -5.66118020e-01],\n         [-8.98982782e-01, -2.07127956e-01, -1.50967365e-01,\n          -8.75904387e-01,  6.74032740e-01,  9.76831841e-02,\n          -2.06528989e-01,  1.33270851e-01],\n         [-1.57715605e-01, -4.93699666e-01, -1.50711604e-01,\n          -3.93520656e-01, -1.34947116e+00, -3.09366478e-01,\n           3.11956694e-01,  7.76015397e-01],\n         [-2.66142777e-01, -6.35398295e-01, -4.34730042e-01,\n           8.32903501e-01, -1.05961880e+00,  7.67780934e-01,\n          -8.65741495e-01, -3.56989597e-01]],\n\n        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00]],\n\n        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00]]],\n\n\n       [[[ 8.93710212e-02, -6.15869618e-02,  2.34037625e-01,\n          -3.27615456e-01,  1.23315079e+00, -2.56303066e-01,\n          -5.94456112e-01, -4.56349147e-02],\n         [ 6.02398232e-01, -1.18665242e+00, -4.70015657e-02,\n           8.55633092e-01, -2.46950798e-02,  9.81808011e-02,\n          -3.19497458e-01, -1.23172235e-01],\n         [ 1.65421072e-01,  1.57910637e+00, -1.62553664e-01,\n          -4.35941698e-02,  3.91715966e-01,  2.61729345e-01,\n          -2.88447774e-01,  5.60057861e-01],\n         [ 5.21401293e-01,  1.84117865e-01, -4.12371383e-01,\n           4.87367844e-01,  7.29049209e-01,  4.46159529e-03,\n           1.32601774e-01, -1.82853710e-02]],\n\n        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00]],\n\n        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00]]],\n\n\n       [[[ 2.36105559e-02, -2.58295356e-02,  5.36331237e-01,\n           9.19875122e-01,  1.80605305e-01,  6.56594990e-01,\n           6.94534102e-01, -4.40777762e-02],\n         [ 8.99769660e-02,  1.06725825e+00, -5.67570090e-01,\n          -8.99587928e-01, -1.08534787e-01, -2.43415958e-02,\n           5.21063574e-01,  4.16188787e-01],\n         [-3.02183235e-02, -1.32816671e+00,  1.27029950e+00,\n           6.35065373e-01,  3.94516014e-01, -2.64580176e-02,\n          -3.27053323e-01,  4.10521542e-01],\n         [ 6.75680934e-02,  6.74568291e-02,  2.63853376e-02,\n           1.06301875e+00, -7.99165981e-04,  5.66045310e-01,\n          -4.20352130e-01, -5.55184591e-01]],\n\n        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00]],\n\n        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00]]],\n\n\n       [[[ 1.24672084e-01,  4.09250039e-01, -2.23670135e-01,\n          -2.31616724e-01,  6.98512774e-01, -5.03294591e-01,\n          -1.43780750e-01,  7.30987005e-02],\n         [ 6.56597025e-01,  2.25777429e-01,  1.08671757e-01,\n           4.40624397e-01,  5.53433007e-01,  9.47014683e-02,\n           6.41983378e-01, -7.94022108e-01],\n         [-1.09932618e-02,  5.08384264e-01, -4.89568139e-01,\n          -1.12110867e-01, -1.30976761e+00,  5.36447759e-02,\n           1.23665985e+00, -1.27856498e-01],\n         [ 1.45503624e-02,  1.01863006e-01,  2.70073229e-01,\n           2.65903316e-01,  5.94683933e-01,  1.06120110e-02,\n          -5.05867888e-01, -1.01467139e-01]],\n\n        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00]],\n\n        [[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00],\n         [ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n           0.00000000e+00,  0.00000000e+00]]]])"
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.forward_(A_prev)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 3, 4, 8)\n",
      "Z'as mean =\n",
      " -0.004269451026202821\n",
      "Z[3,2,1] =\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(cnn.output_tensor.shape)\n",
    "print(\"Z'as mean =\\n\", np.mean(cnn.output_tensor))\n",
    "print(\"Z[3,2,1] =\\n\", cnn.output_tensor[3,2,1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Tensor Dimensions: (2, 3, 3, 3)\n",
      "[[[[1.74481176 0.90159072 1.65980218]\n",
      "   [1.74481176 1.46210794 1.65980218]\n",
      "   [1.74481176 1.6924546  1.65980218]]\n",
      "\n",
      "  [[1.14472371 0.90159072 2.10025514]\n",
      "   [1.14472371 0.90159072 1.65980218]\n",
      "   [1.14472371 1.6924546  1.65980218]]\n",
      "\n",
      "  [[1.13162939 1.51981682 2.18557541]\n",
      "   [1.13162939 1.51981682 2.18557541]\n",
      "   [1.13162939 1.6924546  2.18557541]]]\n",
      "\n",
      "\n",
      " [[[1.19891788 0.84616065 0.82797464]\n",
      "   [0.69803203 0.84616065 1.2245077 ]\n",
      "   [0.69803203 1.12141771 1.2245077 ]]\n",
      "\n",
      "  [[1.96710175 0.84616065 1.27375593]\n",
      "   [1.96710175 0.84616065 1.23616403]\n",
      "   [1.62765075 1.12141771 1.2245077 ]]\n",
      "\n",
      "  [[1.96710175 0.86888616 1.27375593]\n",
      "   [1.96710175 0.86888616 1.23616403]\n",
      "   [1.62765075 1.12141771 0.79280687]]]]\n"
     ]
    }
   ],
   "source": [
    "# Case 1: stride of 1\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(2, 5, 5, 3)\n",
    "maxpool = MaxPool(filter_size=3, stride=1)\n",
    "maxpool.initialize_max_pool_params((A_prev.shape[1],A_prev.shape[2],A_prev.shape[3]))\n",
    "maxpool.forward_(A_prev)\n",
    "maxpool.print_layer_dimensions()\n",
    "print(maxpool.output_tensor)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "(10000,)\n",
      "(10000,)\n",
      "(10000,)\n",
      "(10000,)\n",
      "(10000,)\n",
      "Shape: (50000, 32, 32, 3)\n",
      "Shape: (50000, 1)\n",
      "Shape: (10000, 32, 32, 3)\n",
      "Shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "cifer_dataloader = Cifer10DataLoader('/home/akil/Work/Work/Academics/4-2/ML/Assignment-3/dataset/cifer-10/cifar-10-python/cifar-10-batches-py')\n",
    "cifer_dataloader.concatenate_data()\n",
    "cifer_dataloader.preprocess_data()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0., 0., 0., 0., 0., 0., 1., 0., 0., 0.])"
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifer_dataloader.data['train_labels'][0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "outputs": [
    {
     "data": {
      "text/plain": "(50000, 32, 32, 3)"
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifer_dataloader.data['train_images'].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "outputs": [
    {
     "data": {
      "text/plain": "(50000, 10)"
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cifer_dataloader.data['train_labels'].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "outputs": [],
   "source": [
    "total_data = cifer_dataloader.data['train_images'].shape[0]\n",
    "mini_batch_size = 32\n",
    "p = np.random.permutation(total_data)\n",
    "cifer_dataloader.data['train_images'], cifer_dataloader.data['train_labels'] = cifer_dataloader.data['train_images'][p, :], cifer_dataloader.data['train_labels'][p, :]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "outputs": [],
   "source": [
    "X = cifer_dataloader.data['train_images']\n",
    "Y = cifer_dataloader.data['train_labels']"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "outputs": [
    {
     "data": {
      "text/plain": "(50000, 32, 32, 3)"
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testing The forward pass of the Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "outputs": [],
   "source": [
    "image = mnist.data[mnist.data_list[0]][0]\n",
    "label = mnist.data[mnist.data_list[1]][1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "outputs": [],
   "source": [
    "X_train = mnist.data[mnist.data_list[0]]\n",
    "Y_train = mnist.data[mnist.data_list[1]]\n",
    "X_test = mnist.data[mnist.data_list[0]]\n",
    "Y_test = mnist.data[mnist.data_list[1]]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "cnn = Model()\n",
    "cnn.add(layer_list=[\n",
    "    InputLayer(input_dimension=(mnist.size, mnist.size, mnist.color_channel), is_trainable=False, layer_name='Input'),\n",
    "    Convolution2D(num_out_channel=6, filter_size=5, stride=1, padding_size=2),\n",
    "    ReLUActivation(),\n",
    "    MaxPool(filter_size=2, stride=2),\n",
    "    Convolution2D(num_out_channel=12, filter_size=5, stride=1, padding_size=0),\n",
    "    ReLUActivation(),\n",
    "    MaxPool(filter_size=2, stride=2),\n",
    "    Convolution2D(num_out_channel=100, filter_size=5, stride=1, padding_size=0),\n",
    "    ReLUActivation(),\n",
    "    Flatten(),\n",
    "    DenseLayer(10),\n",
    "    SoftmaxActivation()\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "outputs": [],
   "source": [
    "cnn.compile(optimizer=GradientDescent(), cost_function=CrossEntropyLoss())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he: paramaters:  784\n",
      "(28, 28, 6)\n",
      "he: paramaters:  1176\n",
      "(10, 10, 12)\n",
      "he: paramaters:  300\n",
      "W dense:  [[ 0.23433301 -0.15695256  0.02802117 -0.01465311 -0.00679853 -0.06208694\n",
      "  -0.05473639  0.1448807  -0.05362639 -0.04228363 -0.11115291 -0.03769559\n",
      "   0.41050864 -0.11288882  0.04220027  0.01841743 -0.03964389 -0.10562536\n",
      "  -0.13409854 -0.28108555 -0.01293638 -0.01479885  0.20664262 -0.0431796\n",
      "  -0.24552261 -0.12354633 -0.00998175 -0.0957205   0.15327297  0.17756452\n",
      "   0.00394415 -0.0200549  -0.01319557 -0.12291854  0.06996494  0.39673395\n",
      "  -0.05844328 -0.16063467  0.04137244 -0.24016021 -0.06910572 -0.21096078\n",
      "   0.02113375 -0.0455477  -0.11112349 -0.11058085  0.10271831 -0.01808624\n",
      "  -0.1491749   0.02138677  0.02144036  0.0477756  -0.0468098  -0.06914375\n",
      "   0.09651266  0.3165064   0.03298989 -0.08631199  0.15941969 -0.01278247\n",
      "   0.02321483  0.07827833 -0.03683673 -0.09385001  0.22244925  0.04238086\n",
      "  -0.37604759 -0.03826052 -0.04495972  0.00735161  0.13334194  0.15821356\n",
      "  -0.00456329  0.13195965  0.03327507  0.12007587  0.05472935 -0.05694539\n",
      "  -0.04262108 -0.08217869 -0.42232564  0.10370317  0.02066403  0.05701513\n",
      "  -0.14949681  0.05498358  0.10087137  0.08328992 -0.13898599 -0.0087903\n",
      "   0.11857885  0.15681018  0.16829293  0.21266206 -0.16956749  0.27647182\n",
      "  -0.13538805 -0.08126271 -0.00770479  0.15900465]\n",
      " [ 0.00605127 -0.0795584  -0.22923113 -0.01433783 -0.00449382  0.17837813\n",
      "  -0.13542653  0.11035183  0.03009743 -0.33670508 -0.0138011   0.0918708\n",
      "  -0.33101657 -0.13371743  0.00792214  0.01949504 -0.21414425  0.05735003\n",
      "  -0.11218137 -0.01312658 -0.19391137  0.05425754 -0.02571688 -0.05076037\n",
      "   0.02469925  0.08103429  0.05954181 -0.2076774   0.03690674  0.22248858\n",
      "  -0.05516834  0.12416898  0.22366234 -0.08966218 -0.19286859  0.09199745\n",
      "   0.11714332 -0.16073705 -0.24109581 -0.02041952  0.05371126 -0.11757706\n",
      "   0.04797806 -0.14308796  0.01093867  0.00769574  0.1608035   0.04306774\n",
      "  -0.08507381  0.14688581  0.05205705 -0.11880091 -0.12815412  0.04949908\n",
      "  -0.22128464  0.09523986 -0.07950685  0.13915089  0.25029369  0.08408911\n",
      "  -0.13826594 -0.17401666 -0.01232512  0.03054184  0.14716684 -0.37344242\n",
      "  -0.20630156 -0.20045247 -0.06744153  0.46296206  0.31660337  0.12566092\n",
      "   0.1838416  -0.05301719 -0.04403631  0.18512621 -0.08153685 -0.08979278\n",
      "  -0.11990143 -0.06783375  0.12049906 -0.03186343  0.0190204   0.10551545\n",
      "   0.22924501 -0.13733771 -0.11631297 -0.06799771  0.04071208  0.0470904\n",
      "   0.13193438 -0.1721211  -0.04549364 -0.03108253 -0.05200679 -0.22759622\n",
      "  -0.01403876  0.28690568 -0.12797538  0.01504809]\n",
      " [ 0.13143755 -0.03210506 -0.0626926  -0.20898627 -0.00372024 -0.01074178\n",
      "  -0.09391863 -0.13123921 -0.09959319 -0.30485247 -0.08985164 -0.17910312\n",
      "  -0.35724025 -0.04462388 -0.07380855  0.37068027  0.28082327  0.0923557\n",
      "   0.25995525 -0.02893537  0.10085603  0.21372429 -0.1253364  -0.014963\n",
      "  -0.4284981   0.1088483   0.17527204  0.11430078 -0.03004535 -0.05947873\n",
      "  -0.00289594 -0.03867322  0.08740226 -0.13065767  0.04055525  0.03834415\n",
      "  -0.32318592  0.03390578  0.10601996 -0.11404258  0.16700737 -0.09302439\n",
      "   0.21783706  0.02844384 -0.04419889  0.08630245 -0.13842127 -0.02474472\n",
      "  -0.21228434  0.01278563 -0.03534366  0.02632158  0.08623249 -0.07633551\n",
      "   0.04596291 -0.1921782   0.28100168 -0.23085965 -0.02871346 -0.20945061\n",
      "   0.00348682 -0.04752088  0.23111914  0.22139528  0.23077785  0.2149593\n",
      "  -0.00100584  0.10627078  0.07521374  0.04839694  0.31097698 -0.13691464\n",
      "  -0.21375778  0.20130059  0.37244731  0.13680586 -0.22623119 -0.22832254\n",
      "  -0.05573506 -0.25007881 -0.09659846  0.07360468  0.15951912  0.15640516\n",
      "  -0.09976866 -0.1231342   0.15404759  0.03124618 -0.03631908 -0.04221361\n",
      "   0.06014554  0.21162836  0.01283908  0.22472649 -0.2282236   0.09789231\n",
      "   0.1152805  -0.40791537 -0.16096937  0.03934009]\n",
      " [ 0.20110121 -0.01570264 -0.00169964  0.06304136 -0.03828767 -0.05313021\n",
      "   0.12537144 -0.06406583 -0.05117677 -0.00237549  0.14280524  0.0462158\n",
      "   0.16745861  0.08439094 -0.04744899 -0.25930261  0.19367089 -0.08334141\n",
      "   0.10520446  0.05157987  0.12890058  0.0251831   0.17634879  0.16903414\n",
      "  -0.12467661  0.08706938  0.09618885  0.02095281  0.09392079 -0.1550673\n",
      "   0.07383165  0.00360838  0.02960589  0.11701617 -0.14660041 -0.18193834\n",
      "  -0.23976568 -0.13126411 -0.14664461  0.16207068 -0.03899407  0.03764527\n",
      "  -0.06165083  0.04829581 -0.03961065 -0.26027626 -0.00854622 -0.28085664\n",
      "  -0.0164789   0.00253857 -0.27480149  0.00929446 -0.18782278  0.01113893\n",
      "   0.03489065 -0.02432139  0.1527285  -0.02852003  0.17602948  0.2247397\n",
      "  -0.0423241  -0.04816121  0.17437242 -0.2502237  -0.09031184  0.32806622\n",
      "  -0.01557087  0.10677376 -0.15307258 -0.1170791  -0.03134287 -0.0222843\n",
      "   0.00470898  0.0529353  -0.28942057 -0.02388145  0.0432909   0.21851095\n",
      "  -0.02915189 -0.05730832 -0.19608822 -0.14467401  0.06410998 -0.09334283\n",
      "  -0.21963441 -0.11677598  0.05432213 -0.07436265 -0.0054532  -0.09960719\n",
      "   0.07507642 -0.02844047  0.23655265  0.10505586 -0.12922667 -0.03981726\n",
      "  -0.38541809  0.03413317  0.10413312 -0.01845739]\n",
      " [-0.37118199  0.12067522  0.14249845  0.08648423 -0.11090721  0.10315269\n",
      "   0.0388125  -0.11492959 -0.10126944  0.01771174 -0.10005055  0.18295773\n",
      "   0.11873334 -0.07106077 -0.04990795 -0.13756036 -0.10651608 -0.31647789\n",
      "   0.14458206 -0.17754006 -0.03180535 -0.09686621 -0.07451028  0.02706598\n",
      "  -0.11669443  0.19317909 -0.14809038 -0.05236689 -0.08574212  0.1341117\n",
      "  -0.03681274 -0.08095151  0.09801293 -0.18990718 -0.16795604  0.01434638\n",
      "  -0.10167092 -0.11012148 -0.13004331  0.1802649   0.10947755 -0.04739338\n",
      "  -0.11410505 -0.02349677  0.2223729  -0.25223064 -0.2380167   0.07845683\n",
      "  -0.04500743 -0.09249269 -0.08331295  0.29825487  0.05193207  0.09601951\n",
      "  -0.04532964  0.01032593  0.13467532  0.0550868  -0.07652279  0.11416274\n",
      "  -0.1142763  -0.06094192 -0.09554699  0.07142942  0.22824228 -0.17581716\n",
      "  -0.52296088  0.10322597 -0.06913174 -0.12608769 -0.41232617 -0.07838693\n",
      "   0.12437416 -0.09440814  0.05719484 -0.28761528 -0.17685165 -0.0095257\n",
      "  -0.04591371  0.01825585 -0.20245762  0.07013009 -0.33194728 -0.00323627\n",
      "   0.06015273  0.23522817  0.02110747 -0.24259827 -0.22054609  0.17587656\n",
      "  -0.08515914  0.09653533  0.24164887  0.06307746 -0.12307465 -0.11449475\n",
      "  -0.39001332  0.28429425  0.22570342  0.05430447]\n",
      " [ 0.0469217  -0.2194913  -0.01355622 -0.10801519 -0.19130784  0.04191901\n",
      "   0.05655136 -0.08402542 -0.28041099 -0.1243086   0.16250783  0.01858664\n",
      "   0.16712155  0.18584058  0.08569798 -0.02744754 -0.2435898  -0.26891413\n",
      "   0.14570273 -0.20249429  0.10875794  0.20228222  0.11173351 -0.03248095\n",
      "   0.01784335 -0.01243858  0.12555168 -0.07475092  0.13691881 -0.19238417\n",
      "  -0.05216042  0.16383924  0.17810235 -0.01692925  0.0259012  -0.04474809\n",
      "   0.05929729 -0.12205864 -0.21961238 -0.09714024  0.07893842  0.24856875\n",
      "  -0.05526843  0.00906959 -0.13816798  0.14043598  0.043062    0.04563515\n",
      "  -0.11778193 -0.2243837   0.05216518 -0.4299235  -0.16242331 -0.20490842\n",
      "   0.01068467 -0.05318414  0.21995258 -0.14701171  0.02906309 -0.23585923\n",
      "   0.01231892  0.28439952  0.11798789 -0.07549719  0.07384935 -0.05687937\n",
      "  -0.04080897  0.0150674  -0.0124417   0.05384443  0.10869703  0.10795401\n",
      "   0.16262184  0.44175817 -0.16009115  0.18569235 -0.02288928  0.02324058\n",
      "  -0.10541986  0.19599853  0.03620063 -0.07942547  0.07677708 -0.27846164\n",
      "  -0.32816439 -0.06418462  0.18042556 -0.0987882   0.23081263  0.10871693\n",
      "   0.01468858 -0.0952603   0.1365381  -0.09048948 -0.07125962  0.28060362\n",
      "  -0.12541058 -0.17052718  0.01367997 -0.01481772]\n",
      " [ 0.30560385 -0.15273851  0.11335419  0.13234435  0.00319118 -0.03745578\n",
      "  -0.13604316  0.02614281  0.11913973 -0.04850656  0.09691292  0.1700161\n",
      "   0.0673544  -0.11904913 -0.19228818  0.00779161 -0.07593178  0.13526002\n",
      "   0.00388015  0.08074885 -0.21289822  0.15248476 -0.10526229  0.35793041\n",
      "   0.01689996 -0.04716675  0.30206044 -0.26517752  0.00847042 -0.18117725\n",
      "   0.23193759 -0.07411332  0.12965162 -0.18933711 -0.03794858 -0.00615416\n",
      "   0.09414675 -0.04103973 -0.30498675  0.07921286  0.04453075  0.00857272\n",
      "   0.21970971 -0.15057733  0.07387266 -0.0440388   0.0220877  -0.13441428\n",
      "   0.09778338  0.23357508 -0.16645577 -0.07722686 -0.12476033  0.06169302\n",
      "   0.10151311 -0.03586607  0.14880323 -0.05391992 -0.01923517 -0.06539954\n",
      "   0.08130281 -0.03095394  0.04456719  0.021086   -0.05835679 -0.41381774\n",
      "   0.14656577  0.05651207 -0.27007349 -0.16417343  0.20738818 -0.13137848\n",
      "  -0.07444555  0.12143338 -0.00953065 -0.0884161   0.04294048 -0.23063395\n",
      "  -0.03298697  0.11983394  0.08433259  0.03156858 -0.03304286  0.02112112\n",
      "  -0.01452411  0.10911495 -0.00297823  0.09125817  0.01357638  0.31666539\n",
      "   0.04290199 -0.12350118 -0.13850555 -0.11996905 -0.06853603  0.04418703\n",
      "  -0.00184966 -0.13592194 -0.11456599 -0.07005   ]\n",
      " [ 0.22298033  0.12472659 -0.17294814 -0.01542812 -0.09061857  0.16782782\n",
      "   0.15588527  0.12716652  0.09695     0.13368695  0.22636818  0.0676718\n",
      "   0.04484557 -0.01206786 -0.00548889 -0.15730178  0.0837956  -0.08419051\n",
      "   0.04487176 -0.10329111 -0.13283882 -0.33257972 -0.02658418 -0.08092031\n",
      "  -0.06465999  0.32942176 -0.13826489 -0.06344884 -0.06087668  0.03801021\n",
      "  -0.10283707  0.03568773 -0.10842345  0.13881666 -0.05114357 -0.11584692\n",
      "  -0.17668945  0.0410603   0.06402208 -0.0849015   0.02438777 -0.27797469\n",
      "   0.04092867  0.07743207 -0.18510491  0.19037505  0.05427329  0.12312225\n",
      "   0.14234364  0.01533176  0.02934378 -0.03161162 -0.00157583  0.11916181\n",
      "  -0.05182792 -0.01859607  0.06412299  0.07804224  0.12477997  0.10095636\n",
      "   0.16509539  0.13358839 -0.02153095  0.08971401 -0.02128765 -0.02927671\n",
      "   0.12897943  0.21688149  0.0068293  -0.11629961 -0.11170568 -0.06688641\n",
      "  -0.27880909  0.05670225 -0.19714911 -0.20867166 -0.00355936 -0.20980035\n",
      "   0.05398388 -0.14361152 -0.05033593 -0.14644329  0.00938045  0.07145653\n",
      "  -0.09661139 -0.19740898  0.03445677  0.01960739  0.11412406  0.15205876\n",
      "  -0.24851715 -0.05006503 -0.08888563 -0.02307691 -0.18529478  0.44332488\n",
      "  -0.09046133  0.10669173 -0.01110349  0.13303196]\n",
      " [ 0.0850459  -0.05373376  0.03529112 -0.02473862  0.08194539  0.0320415\n",
      "   0.10176875  0.0251478  -0.22885075  0.225895    0.09727931 -0.14049837\n",
      "  -0.27574118 -0.01705099  0.12598737 -0.15441453 -0.06172542  0.04699498\n",
      "  -0.05477971 -0.15803253 -0.10290721 -0.01439522  0.06640616  0.03793539\n",
      "  -0.02586158 -0.04415213 -0.16695763  0.10093017  0.03259573 -0.13918678\n",
      "  -0.06091682  0.1831212  -0.13803757 -0.11270447  0.02266154  0.12448096\n",
      "   0.02376549 -0.04730314 -0.08374646  0.07287683 -0.06318817 -0.19938594\n",
      "   0.0996746  -0.02911686  0.13026271 -0.13673354 -0.07130347 -0.04023579\n",
      "   0.02565825  0.15233095 -0.40703335  0.19765411  0.01775915  0.00909367\n",
      "  -0.10697807  0.11723511  0.07666091  0.00651892  0.06567077  0.24595272\n",
      "  -0.06588922 -0.10341358  0.03588818  0.10261851 -0.15535429  0.16213073\n",
      "  -0.06626069  0.10538414  0.03338484 -0.23806027 -0.06917932  0.15907605\n",
      "  -0.22028087 -0.05030644  0.38114169 -0.0400039  -0.00594456  0.01022122\n",
      "  -0.07568618  0.1663301  -0.19359037  0.05797928 -0.00925497 -0.00792477\n",
      "  -0.14255134  0.06555975 -0.24448416 -0.1480825  -0.0624177   0.18136061\n",
      "   0.09749421  0.10689971 -0.02066297 -0.21561988  0.16445753 -0.03716288\n",
      "   0.10065562  0.13893828 -0.07339559  0.15447544]\n",
      " [-0.03613241 -0.10358958  0.07040119 -0.00774752 -0.11044349 -0.06012384\n",
      "  -0.06275391  0.09927891 -0.09669931  0.12790121 -0.19195166  0.14157772\n",
      "  -0.20557626  0.11130538  0.23956757  0.00096159  0.14406626  0.17173036\n",
      "   0.10340162  0.21699914 -0.09478475  0.06927971  0.06116806  0.12183686\n",
      "  -0.17948076 -0.32715822 -0.03270395 -0.1148861  -0.09706604  0.0755699\n",
      "   0.01872378 -0.06204243  0.05101415  0.10399782  0.09169044  0.17947807\n",
      "   0.03614471  0.3500945   0.09268644  0.04227481 -0.00546247  0.33123682\n",
      "   0.15660938  0.01700678 -0.09147166  0.18995812  0.11146484 -0.11881834\n",
      "  -0.3506024  -0.18800806 -0.14529084  0.13038504  0.07159624  0.23050712\n",
      "  -0.04087443  0.05186479 -0.10069888  0.13567667  0.06918506 -0.29260811\n",
      "  -0.12066918  0.17069184  0.40972121  0.07435906  0.18652699  0.13856182\n",
      "   0.03373528  0.11866037  0.10035309 -0.10275571 -0.21112915 -0.12480212\n",
      "  -0.02681432  0.28453107 -0.10227301  0.12482941 -0.0522678  -0.14459135\n",
      "   0.01411531  0.147003    0.12028558 -0.04800522 -0.11932151  0.05918664\n",
      "  -0.00485139 -0.12419809 -0.14486713 -0.14950399  0.0355993  -0.10058857\n",
      "  -0.12396151  0.06524821 -0.01857062 -0.12846067 -0.0691321  -0.46797374\n",
      "  -0.11102007  0.18228537  0.16811235  0.15317269]]\n"
     ]
    }
   ],
   "source": [
    "cnn.initializer_layer_params()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Epoch: 0\n",
      "Forward for layer Conv2D__1\n",
      "(2, 28, 28, 1)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (2, 28, 28, 6)\n",
      "Forward for layer MaxPool__1\n",
      "Forward for layer Conv2D__1\n",
      "(2, 14, 14, 6)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (2, 10, 10, 12)\n",
      "Forward for layer MaxPool__1\n",
      "Forward for layer Conv2D__1\n",
      "(2, 5, 5, 12)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (2, 1, 1, 100)\n",
      "Forward for layer Flatten__1\n",
      "Forward for layer Dense__1\n",
      "starting of dense:  [[0.08137326 0.10650468 0.04325873 0.         0.07990617 0.\n",
      "  0.         0.00205095 0.         0.04209543 0.         0.\n",
      "  0.         0.         0.02554314 0.         0.         0.04679587\n",
      "  0.         0.         0.         0.         0.         0.06812715\n",
      "  0.05214487 0.         0.         0.00263254 0.16639248 0.\n",
      "  0.06143025 0.17122859 0.05173468 0.09646144 0.09360312 0.05091651\n",
      "  0.1362361  0.0862849  0.         0.11561884 0.         0.\n",
      "  0.09643892 0.10419204 0.         0.         0.         0.02636713\n",
      "  0.15701317 0.00933859 0.0485219  0.         0.11421747 0.\n",
      "  0.07825466 0.         0.         0.         0.09702144 0.05832248\n",
      "  0.         0.         0.         0.         0.         0.06000328\n",
      "  0.04056581 0.00782963 0.04542473 0.         0.         0.01175496\n",
      "  0.         0.18416796 0.10565936 0.00314629 0.         0.\n",
      "  0.07514686 0.08428152 0.19585266 0.14753186 0.         0.\n",
      "  0.12425455 0.         0.         0.         0.10173814 0.01360023\n",
      "  0.16567405 0.08733282 0.         0.13017551 0.01987176 0.06297233\n",
      "  0.         0.12877218 0.         0.        ]\n",
      " [0.10473249 0.0668097  0.05683469 0.         0.04569379 0.\n",
      "  0.0983382  0.00887722 0.         0.02064864 0.         0.\n",
      "  0.         0.         0.07316553 0.01530135 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.00396476 0.         0.         0.01666844 0.12867083 0.\n",
      "  0.03899917 0.22008624 0.0918106  0.08394624 0.15929107 0.\n",
      "  0.10324439 0.05972727 0.         0.17352563 0.         0.\n",
      "  0.16411409 0.14008517 0.         0.         0.         0.00713311\n",
      "  0.11156514 0.         0.02620403 0.         0.12810904 0.04416866\n",
      "  0.0771204  0.         0.         0.         0.08907159 0.\n",
      "  0.         0.         0.         0.         0.         0.06437378\n",
      "  0.02469296 0.03592317 0.02193264 0.01732722 0.01308696 0.00769528\n",
      "  0.         0.15216612 0.05148422 0.         0.02912218 0.\n",
      "  0.12694301 0.         0.20992142 0.1915081  0.         0.\n",
      "  0.08296273 0.         0.         0.         0.09799822 0.01723501\n",
      "  0.12950791 0.03196516 0.         0.15418795 0.03137868 0.\n",
      "  0.02328792 0.09296656 0.         0.        ]]\n",
      "A_prev_layer shape: (2, 100)\n",
      "dense layer output:  [[-0.07060638 -0.08327725]\n",
      " [-0.02607899 -0.03561596]\n",
      " [-0.06188306  0.00251247]\n",
      " [-0.09735659 -0.06169531]\n",
      " [-0.18444952 -0.18958337]\n",
      " [-0.03536342 -0.00809197]\n",
      " [ 0.02929779 -0.00123892]\n",
      " [-0.03594391 -0.0185678 ]\n",
      " [ 0.07815394  0.04529829]\n",
      " [ 0.03626775  0.08661674]]\n",
      "Forward for layer Softmax__1\n",
      "softmax tensor shape: (2, 10)\n",
      "softmax exp shape (2, 10)\n",
      "softmax sum shape: (2, 1)\n",
      "final input shape (2, 10)\n",
      "final output: [[0.09643487 0.1008259  0.09727978 0.09388941 0.08605828 0.09989412\n",
      "  0.1065668  0.09983615 0.11190252 0.10731216]\n",
      " [0.09423139 0.09883133 0.10267237 0.09628719 0.08472809 0.10158934\n",
      "  0.10228793 0.10053066 0.10716062 0.11168108]]\n",
      "Loss is :  2.3328232049283186\n",
      "Progress 0.0%\n",
      "\n",
      "Cost After a Epoch 1: 233.28232049283187\n",
      "Running Epoch: 1\n",
      "Forward for layer Conv2D__1\n",
      "(2, 28, 28, 1)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (2, 28, 28, 6)\n",
      "Forward for layer MaxPool__1\n",
      "Forward for layer Conv2D__1\n",
      "(2, 14, 14, 6)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (2, 10, 10, 12)\n",
      "Forward for layer MaxPool__1\n",
      "Forward for layer Conv2D__1\n",
      "(2, 5, 5, 12)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (2, 1, 1, 100)\n",
      "Forward for layer Flatten__1\n",
      "Forward for layer Dense__1\n",
      "starting of dense:  [[0.08137326 0.10650468 0.04325873 0.         0.07990617 0.\n",
      "  0.         0.00205095 0.         0.04209543 0.         0.\n",
      "  0.         0.         0.02554314 0.         0.         0.04679587\n",
      "  0.         0.         0.         0.         0.         0.06812715\n",
      "  0.05214487 0.         0.         0.00263254 0.16639248 0.\n",
      "  0.06143025 0.17122859 0.05173468 0.09646144 0.09360312 0.05091651\n",
      "  0.1362361  0.0862849  0.         0.11561884 0.         0.\n",
      "  0.09643892 0.10419204 0.         0.         0.         0.02636713\n",
      "  0.15701317 0.00933859 0.0485219  0.         0.11421747 0.\n",
      "  0.07825466 0.         0.         0.         0.09702144 0.05832248\n",
      "  0.         0.         0.         0.         0.         0.06000328\n",
      "  0.04056581 0.00782963 0.04542473 0.         0.         0.01175496\n",
      "  0.         0.18416796 0.10565936 0.00314629 0.         0.\n",
      "  0.07514686 0.08428152 0.19585266 0.14753186 0.         0.\n",
      "  0.12425455 0.         0.         0.         0.10173814 0.01360023\n",
      "  0.16567405 0.08733282 0.         0.13017551 0.01987176 0.06297233\n",
      "  0.         0.12877218 0.         0.        ]\n",
      " [0.10473249 0.0668097  0.05683469 0.         0.04569379 0.\n",
      "  0.0983382  0.00887722 0.         0.02064864 0.         0.\n",
      "  0.         0.         0.07316553 0.01530135 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.00396476 0.         0.         0.01666844 0.12867083 0.\n",
      "  0.03899917 0.22008624 0.0918106  0.08394624 0.15929107 0.\n",
      "  0.10324439 0.05972727 0.         0.17352563 0.         0.\n",
      "  0.16411409 0.14008517 0.         0.         0.         0.00713311\n",
      "  0.11156514 0.         0.02620403 0.         0.12810904 0.04416866\n",
      "  0.0771204  0.         0.         0.         0.08907159 0.\n",
      "  0.         0.         0.         0.         0.         0.06437378\n",
      "  0.02469296 0.03592317 0.02193264 0.01732722 0.01308696 0.00769528\n",
      "  0.         0.15216612 0.05148422 0.         0.02912218 0.\n",
      "  0.12694301 0.         0.20992142 0.1915081  0.         0.\n",
      "  0.08296273 0.         0.         0.         0.09799822 0.01723501\n",
      "  0.12950791 0.03196516 0.         0.15418795 0.03137868 0.\n",
      "  0.02328792 0.09296656 0.         0.        ]]\n",
      "A_prev_layer shape: (2, 100)\n",
      "dense layer output:  [[-0.07060638 -0.08327725]\n",
      " [-0.02607899 -0.03561596]\n",
      " [-0.06188306  0.00251247]\n",
      " [-0.09735659 -0.06169531]\n",
      " [-0.18444952 -0.18958337]\n",
      " [-0.03536342 -0.00809197]\n",
      " [ 0.02929779 -0.00123892]\n",
      " [-0.03594391 -0.0185678 ]\n",
      " [ 0.07815394  0.04529829]\n",
      " [ 0.03626775  0.08661674]]\n",
      "Forward for layer Softmax__1\n",
      "softmax tensor shape: (2, 10)\n",
      "softmax exp shape (2, 10)\n",
      "softmax sum shape: (2, 1)\n",
      "final input shape (2, 10)\n",
      "final output: [[0.09643487 0.1008259  0.09727978 0.09388941 0.08605828 0.09989412\n",
      "  0.1065668  0.09983615 0.11190252 0.10731216]\n",
      " [0.09423139 0.09883133 0.10267237 0.09628719 0.08472809 0.10158934\n",
      "  0.10228793 0.10053066 0.10716062 0.11168108]]\n",
      "Loss is :  2.3328232049283186\n",
      "Progress 0.0%\n",
      "\n",
      "Cost After a Epoch 2: 233.28232049283187\n",
      "Running Epoch: 2\n",
      "Forward for layer Conv2D__1\n",
      "(2, 28, 28, 1)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (2, 28, 28, 6)\n",
      "Forward for layer MaxPool__1\n",
      "Forward for layer Conv2D__1\n",
      "(2, 14, 14, 6)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (2, 10, 10, 12)\n",
      "Forward for layer MaxPool__1\n",
      "Forward for layer Conv2D__1\n",
      "(2, 5, 5, 12)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (2, 1, 1, 100)\n",
      "Forward for layer Flatten__1\n",
      "Forward for layer Dense__1\n",
      "starting of dense:  [[0.08137326 0.10650468 0.04325873 0.         0.07990617 0.\n",
      "  0.         0.00205095 0.         0.04209543 0.         0.\n",
      "  0.         0.         0.02554314 0.         0.         0.04679587\n",
      "  0.         0.         0.         0.         0.         0.06812715\n",
      "  0.05214487 0.         0.         0.00263254 0.16639248 0.\n",
      "  0.06143025 0.17122859 0.05173468 0.09646144 0.09360312 0.05091651\n",
      "  0.1362361  0.0862849  0.         0.11561884 0.         0.\n",
      "  0.09643892 0.10419204 0.         0.         0.         0.02636713\n",
      "  0.15701317 0.00933859 0.0485219  0.         0.11421747 0.\n",
      "  0.07825466 0.         0.         0.         0.09702144 0.05832248\n",
      "  0.         0.         0.         0.         0.         0.06000328\n",
      "  0.04056581 0.00782963 0.04542473 0.         0.         0.01175496\n",
      "  0.         0.18416796 0.10565936 0.00314629 0.         0.\n",
      "  0.07514686 0.08428152 0.19585266 0.14753186 0.         0.\n",
      "  0.12425455 0.         0.         0.         0.10173814 0.01360023\n",
      "  0.16567405 0.08733282 0.         0.13017551 0.01987176 0.06297233\n",
      "  0.         0.12877218 0.         0.        ]\n",
      " [0.10473249 0.0668097  0.05683469 0.         0.04569379 0.\n",
      "  0.0983382  0.00887722 0.         0.02064864 0.         0.\n",
      "  0.         0.         0.07316553 0.01530135 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.00396476 0.         0.         0.01666844 0.12867083 0.\n",
      "  0.03899917 0.22008624 0.0918106  0.08394624 0.15929107 0.\n",
      "  0.10324439 0.05972727 0.         0.17352563 0.         0.\n",
      "  0.16411409 0.14008517 0.         0.         0.         0.00713311\n",
      "  0.11156514 0.         0.02620403 0.         0.12810904 0.04416866\n",
      "  0.0771204  0.         0.         0.         0.08907159 0.\n",
      "  0.         0.         0.         0.         0.         0.06437378\n",
      "  0.02469296 0.03592317 0.02193264 0.01732722 0.01308696 0.00769528\n",
      "  0.         0.15216612 0.05148422 0.         0.02912218 0.\n",
      "  0.12694301 0.         0.20992142 0.1915081  0.         0.\n",
      "  0.08296273 0.         0.         0.         0.09799822 0.01723501\n",
      "  0.12950791 0.03196516 0.         0.15418795 0.03137868 0.\n",
      "  0.02328792 0.09296656 0.         0.        ]]\n",
      "A_prev_layer shape: (2, 100)\n",
      "dense layer output:  [[-0.07060638 -0.08327725]\n",
      " [-0.02607899 -0.03561596]\n",
      " [-0.06188306  0.00251247]\n",
      " [-0.09735659 -0.06169531]\n",
      " [-0.18444952 -0.18958337]\n",
      " [-0.03536342 -0.00809197]\n",
      " [ 0.02929779 -0.00123892]\n",
      " [-0.03594391 -0.0185678 ]\n",
      " [ 0.07815394  0.04529829]\n",
      " [ 0.03626775  0.08661674]]\n",
      "Forward for layer Softmax__1\n",
      "softmax tensor shape: (2, 10)\n",
      "softmax exp shape (2, 10)\n",
      "softmax sum shape: (2, 1)\n",
      "final input shape (2, 10)\n",
      "final output: [[0.09643487 0.1008259  0.09727978 0.09388941 0.08605828 0.09989412\n",
      "  0.1065668  0.09983615 0.11190252 0.10731216]\n",
      " [0.09423139 0.09883133 0.10267237 0.09628719 0.08472809 0.10158934\n",
      "  0.10228793 0.10053066 0.10716062 0.11168108]]\n",
      "Loss is :  2.3328232049283186\n",
      "Progress 0.0%\n",
      "\n",
      "Cost After a Epoch 3: 233.28232049283187\n",
      "Running Epoch: 3\n",
      "Forward for layer Conv2D__1\n",
      "(2, 28, 28, 1)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (2, 28, 28, 6)\n",
      "Forward for layer MaxPool__1\n",
      "Forward for layer Conv2D__1\n",
      "(2, 14, 14, 6)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (2, 10, 10, 12)\n",
      "Forward for layer MaxPool__1\n",
      "Forward for layer Conv2D__1\n",
      "(2, 5, 5, 12)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (2, 1, 1, 100)\n",
      "Forward for layer Flatten__1\n",
      "Forward for layer Dense__1\n",
      "starting of dense:  [[0.08137326 0.10650468 0.04325873 0.         0.07990617 0.\n",
      "  0.         0.00205095 0.         0.04209543 0.         0.\n",
      "  0.         0.         0.02554314 0.         0.         0.04679587\n",
      "  0.         0.         0.         0.         0.         0.06812715\n",
      "  0.05214487 0.         0.         0.00263254 0.16639248 0.\n",
      "  0.06143025 0.17122859 0.05173468 0.09646144 0.09360312 0.05091651\n",
      "  0.1362361  0.0862849  0.         0.11561884 0.         0.\n",
      "  0.09643892 0.10419204 0.         0.         0.         0.02636713\n",
      "  0.15701317 0.00933859 0.0485219  0.         0.11421747 0.\n",
      "  0.07825466 0.         0.         0.         0.09702144 0.05832248\n",
      "  0.         0.         0.         0.         0.         0.06000328\n",
      "  0.04056581 0.00782963 0.04542473 0.         0.         0.01175496\n",
      "  0.         0.18416796 0.10565936 0.00314629 0.         0.\n",
      "  0.07514686 0.08428152 0.19585266 0.14753186 0.         0.\n",
      "  0.12425455 0.         0.         0.         0.10173814 0.01360023\n",
      "  0.16567405 0.08733282 0.         0.13017551 0.01987176 0.06297233\n",
      "  0.         0.12877218 0.         0.        ]\n",
      " [0.10473249 0.0668097  0.05683469 0.         0.04569379 0.\n",
      "  0.0983382  0.00887722 0.         0.02064864 0.         0.\n",
      "  0.         0.         0.07316553 0.01530135 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.00396476 0.         0.         0.01666844 0.12867083 0.\n",
      "  0.03899917 0.22008624 0.0918106  0.08394624 0.15929107 0.\n",
      "  0.10324439 0.05972727 0.         0.17352563 0.         0.\n",
      "  0.16411409 0.14008517 0.         0.         0.         0.00713311\n",
      "  0.11156514 0.         0.02620403 0.         0.12810904 0.04416866\n",
      "  0.0771204  0.         0.         0.         0.08907159 0.\n",
      "  0.         0.         0.         0.         0.         0.06437378\n",
      "  0.02469296 0.03592317 0.02193264 0.01732722 0.01308696 0.00769528\n",
      "  0.         0.15216612 0.05148422 0.         0.02912218 0.\n",
      "  0.12694301 0.         0.20992142 0.1915081  0.         0.\n",
      "  0.08296273 0.         0.         0.         0.09799822 0.01723501\n",
      "  0.12950791 0.03196516 0.         0.15418795 0.03137868 0.\n",
      "  0.02328792 0.09296656 0.         0.        ]]\n",
      "A_prev_layer shape: (2, 100)\n",
      "dense layer output:  [[-0.07060638 -0.08327725]\n",
      " [-0.02607899 -0.03561596]\n",
      " [-0.06188306  0.00251247]\n",
      " [-0.09735659 -0.06169531]\n",
      " [-0.18444952 -0.18958337]\n",
      " [-0.03536342 -0.00809197]\n",
      " [ 0.02929779 -0.00123892]\n",
      " [-0.03594391 -0.0185678 ]\n",
      " [ 0.07815394  0.04529829]\n",
      " [ 0.03626775  0.08661674]]\n",
      "Forward for layer Softmax__1\n",
      "softmax tensor shape: (2, 10)\n",
      "softmax exp shape (2, 10)\n",
      "softmax sum shape: (2, 1)\n",
      "final input shape (2, 10)\n",
      "final output: [[0.09643487 0.1008259  0.09727978 0.09388941 0.08605828 0.09989412\n",
      "  0.1065668  0.09983615 0.11190252 0.10731216]\n",
      " [0.09423139 0.09883133 0.10267237 0.09628719 0.08472809 0.10158934\n",
      "  0.10228793 0.10053066 0.10716062 0.11168108]]\n",
      "Loss is :  2.3328232049283186\n",
      "Progress 0.0%\n",
      "\n",
      "Cost After a Epoch 4: 233.28232049283187\n",
      "Running Epoch: 4\n",
      "Forward for layer Conv2D__1\n",
      "(2, 28, 28, 1)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (2, 28, 28, 6)\n",
      "Forward for layer MaxPool__1\n",
      "Forward for layer Conv2D__1\n",
      "(2, 14, 14, 6)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (2, 10, 10, 12)\n",
      "Forward for layer MaxPool__1\n",
      "Forward for layer Conv2D__1\n",
      "(2, 5, 5, 12)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (2, 1, 1, 100)\n",
      "Forward for layer Flatten__1\n",
      "Forward for layer Dense__1\n",
      "starting of dense:  [[0.08137326 0.10650468 0.04325873 0.         0.07990617 0.\n",
      "  0.         0.00205095 0.         0.04209543 0.         0.\n",
      "  0.         0.         0.02554314 0.         0.         0.04679587\n",
      "  0.         0.         0.         0.         0.         0.06812715\n",
      "  0.05214487 0.         0.         0.00263254 0.16639248 0.\n",
      "  0.06143025 0.17122859 0.05173468 0.09646144 0.09360312 0.05091651\n",
      "  0.1362361  0.0862849  0.         0.11561884 0.         0.\n",
      "  0.09643892 0.10419204 0.         0.         0.         0.02636713\n",
      "  0.15701317 0.00933859 0.0485219  0.         0.11421747 0.\n",
      "  0.07825466 0.         0.         0.         0.09702144 0.05832248\n",
      "  0.         0.         0.         0.         0.         0.06000328\n",
      "  0.04056581 0.00782963 0.04542473 0.         0.         0.01175496\n",
      "  0.         0.18416796 0.10565936 0.00314629 0.         0.\n",
      "  0.07514686 0.08428152 0.19585266 0.14753186 0.         0.\n",
      "  0.12425455 0.         0.         0.         0.10173814 0.01360023\n",
      "  0.16567405 0.08733282 0.         0.13017551 0.01987176 0.06297233\n",
      "  0.         0.12877218 0.         0.        ]\n",
      " [0.10473249 0.0668097  0.05683469 0.         0.04569379 0.\n",
      "  0.0983382  0.00887722 0.         0.02064864 0.         0.\n",
      "  0.         0.         0.07316553 0.01530135 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.00396476 0.         0.         0.01666844 0.12867083 0.\n",
      "  0.03899917 0.22008624 0.0918106  0.08394624 0.15929107 0.\n",
      "  0.10324439 0.05972727 0.         0.17352563 0.         0.\n",
      "  0.16411409 0.14008517 0.         0.         0.         0.00713311\n",
      "  0.11156514 0.         0.02620403 0.         0.12810904 0.04416866\n",
      "  0.0771204  0.         0.         0.         0.08907159 0.\n",
      "  0.         0.         0.         0.         0.         0.06437378\n",
      "  0.02469296 0.03592317 0.02193264 0.01732722 0.01308696 0.00769528\n",
      "  0.         0.15216612 0.05148422 0.         0.02912218 0.\n",
      "  0.12694301 0.         0.20992142 0.1915081  0.         0.\n",
      "  0.08296273 0.         0.         0.         0.09799822 0.01723501\n",
      "  0.12950791 0.03196516 0.         0.15418795 0.03137868 0.\n",
      "  0.02328792 0.09296656 0.         0.        ]]\n",
      "A_prev_layer shape: (2, 100)\n",
      "dense layer output:  [[-0.07060638 -0.08327725]\n",
      " [-0.02607899 -0.03561596]\n",
      " [-0.06188306  0.00251247]\n",
      " [-0.09735659 -0.06169531]\n",
      " [-0.18444952 -0.18958337]\n",
      " [-0.03536342 -0.00809197]\n",
      " [ 0.02929779 -0.00123892]\n",
      " [-0.03594391 -0.0185678 ]\n",
      " [ 0.07815394  0.04529829]\n",
      " [ 0.03626775  0.08661674]]\n",
      "Forward for layer Softmax__1\n",
      "softmax tensor shape: (2, 10)\n",
      "softmax exp shape (2, 10)\n",
      "softmax sum shape: (2, 1)\n",
      "final input shape (2, 10)\n",
      "final output: [[0.09643487 0.1008259  0.09727978 0.09388941 0.08605828 0.09989412\n",
      "  0.1065668  0.09983615 0.11190252 0.10731216]\n",
      " [0.09423139 0.09883133 0.10267237 0.09628719 0.08472809 0.10158934\n",
      "  0.10228793 0.10053066 0.10716062 0.11168108]]\n",
      "Loss is :  2.3328232049283186\n",
      "Progress 0.0%\n",
      "\n",
      "Cost After a Epoch 5: 233.28232049283187\n",
      "Finish Training!\n"
     ]
    }
   ],
   "source": [
    "cnn.train(training_data=(X_train, Y_train), validation_data=(None, None),mini_batch_size=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "outputs": [
    {
     "data": {
      "text/plain": "(4, 3)"
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense = DenseLayer(num_units=4)\n",
    "dense.W = np.array( [[ 1.4401747,   0.72498046, -0.05727674],\n",
    "                     [-1.15246919, -0.39990891,  0.44136903],\n",
    "                     [ 1.14171484, -1.41891945,  0.73059128],\n",
    "                     [ 0.60664542, -0.08249916, -1.05893566]])\n",
    "dense.W.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "outputs": [
    {
     "data": {
      "text/plain": "(1, 3)"
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([ [0.83351854,  -0.55429203,   0.0702855 ]])\n",
    "a.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "outputs": [
    {
     "data": {
      "text/plain": "(4, 1)"
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense.b = np.array( [[-0.64243089], [0.51146315], [-0.17120088], [1.10775354]])\n",
    "dense.b.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_prev_layer shape: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "dense.forward(a)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 3)\n",
      "(4, 1)\n",
      "(4, 1)\n",
      "[[ 0.12678218 -0.08431048  0.01069076]\n",
      " [-0.16374732  0.1088924  -0.01380781]\n",
      " [ 1.34887082 -0.89700266  0.11374199]\n",
      " [ 1.32088062 -0.8783891   0.11138175]]\n",
      "[[ 0.15210481]\n",
      " [-0.19645312]\n",
      " [ 1.61828532]\n",
      " [ 1.58470455]]\n"
     ]
    }
   ],
   "source": [
    "dz = dense.output_tensor\n",
    "dense.backward(dz)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "outputs": [],
   "source": [
    "s = np.array([[-0.39692968,  1.78981015, -0.54303206,  0.14530002,  1.42375341]])\n",
    "s = s.reshape(1, -1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax tensor shape: (1, 5)\n",
      "softmax exp shape (1, 5)\n",
      "softmax sum shape: (1, 1)\n",
      "[[0.05357302 0.47712828 0.0462908  0.09213688 0.33087103]]\n"
     ]
    }
   ],
   "source": [
    "soft = SoftmaxActivation()\n",
    "print(soft.forward(s))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he: paramaters:  784\n",
      "(28, 28, 6)\n",
      "he: paramaters:  1176\n",
      "(10, 10, 12)\n",
      "he: paramaters:  300\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "X = np.random.randn(1,28,28,1)\n",
    "Y = np.random.randn(1,10)\n",
    "cnn = Model()\n",
    "np.random.seed(1)\n",
    "\n",
    "cnn = Model()\n",
    "cnn.add(layer_list=[\n",
    "    InputLayer(input_dimension=(mnist.size, mnist.size, mnist.color_channel), is_trainable=False, layer_name='Input'),\n",
    "    Convolution2D(num_out_channel=6, filter_size=5, stride=1, padding_size=2),\n",
    "    ReLUActivation(),\n",
    "    MaxPool(filter_size=2, stride=2),\n",
    "    Convolution2D(num_out_channel=12, filter_size=5, stride=1, padding_size=0),\n",
    "    ReLUActivation(),\n",
    "    MaxPool(filter_size=2, stride=2),\n",
    "    Convolution2D(num_out_channel=100, filter_size=5, stride=1, padding_size=0),\n",
    "    ReLUActivation(),\n",
    "    Flatten(),\n",
    "    DenseLayer(10),\n",
    "    SoftmaxActivation()\n",
    "])\n",
    "cnn.compile(optimizer=GradientDescent(), cost_function=CrossEntropyLoss())\n",
    "cnn.initializer_layer_params()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Epoch: 0\n",
      "Forward for layer Conv2D__1\n",
      "(1, 28, 28, 1)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (1, 28, 28, 6)\n",
      "Forward for layer MaxPool__1\n",
      "Forward for layer Conv2D__1\n",
      "(1, 14, 14, 6)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (1, 10, 10, 12)\n",
      "Forward for layer MaxPool__1\n",
      "Forward for layer Conv2D__1\n",
      "(1, 5, 5, 12)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (1, 1, 1, 100)\n",
      "Forward for layer Flatten__1\n",
      "Forward for layer Dense__1\n",
      "A_prev_layer shape: (1, 100)\n",
      "Forward for layer Softmax__1\n",
      "softmax tensor shape: (1, 10)\n",
      "softmax exp shape (1, 10)\n",
      "softmax sum shape: (1, 1)\n",
      "final input shape (1, 10)\n",
      "final output: [[0.09049115 0.06765897 0.1529529  0.08609433 0.04750961 0.10952845\n",
      "  0.10272049 0.08826287 0.12400152 0.13077972]]\n",
      "Loss is :  -6.485603025596568\n",
      "Progress 100.0%\n",
      "\n",
      "Cost After a Epoch 1: -648.5603025596567\n",
      "Running Epoch: 1\n",
      "Forward for layer Conv2D__1\n",
      "(1, 28, 28, 1)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (1, 28, 28, 6)\n",
      "Forward for layer MaxPool__1\n",
      "Forward for layer Conv2D__1\n",
      "(1, 14, 14, 6)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (1, 10, 10, 12)\n",
      "Forward for layer MaxPool__1\n",
      "Forward for layer Conv2D__1\n",
      "(1, 5, 5, 12)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (1, 1, 1, 100)\n",
      "Forward for layer Flatten__1\n",
      "Forward for layer Dense__1\n",
      "A_prev_layer shape: (1, 100)\n",
      "Forward for layer Softmax__1\n",
      "softmax tensor shape: (1, 10)\n",
      "softmax exp shape (1, 10)\n",
      "softmax sum shape: (1, 1)\n",
      "final input shape (1, 10)\n",
      "final output: [[0.09049115 0.06765897 0.1529529  0.08609433 0.04750961 0.10952845\n",
      "  0.10272049 0.08826287 0.12400152 0.13077972]]\n",
      "Loss is :  -6.485603025596568\n",
      "Progress 100.0%\n",
      "\n",
      "Cost After a Epoch 2: -648.5603025596567\n",
      "Running Epoch: 2\n",
      "Forward for layer Conv2D__1\n",
      "(1, 28, 28, 1)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (1, 28, 28, 6)\n",
      "Forward for layer MaxPool__1\n",
      "Forward for layer Conv2D__1\n",
      "(1, 14, 14, 6)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (1, 10, 10, 12)\n",
      "Forward for layer MaxPool__1\n",
      "Forward for layer Conv2D__1\n",
      "(1, 5, 5, 12)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (1, 1, 1, 100)\n",
      "Forward for layer Flatten__1\n",
      "Forward for layer Dense__1\n",
      "A_prev_layer shape: (1, 100)\n",
      "Forward for layer Softmax__1\n",
      "softmax tensor shape: (1, 10)\n",
      "softmax exp shape (1, 10)\n",
      "softmax sum shape: (1, 1)\n",
      "final input shape (1, 10)\n",
      "final output: [[0.09049115 0.06765897 0.1529529  0.08609433 0.04750961 0.10952845\n",
      "  0.10272049 0.08826287 0.12400152 0.13077972]]\n",
      "Loss is :  -6.485603025596568\n",
      "Progress 100.0%\n",
      "\n",
      "Cost After a Epoch 3: -648.5603025596567\n",
      "Running Epoch: 3\n",
      "Forward for layer Conv2D__1\n",
      "(1, 28, 28, 1)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (1, 28, 28, 6)\n",
      "Forward for layer MaxPool__1\n",
      "Forward for layer Conv2D__1\n",
      "(1, 14, 14, 6)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (1, 10, 10, 12)\n",
      "Forward for layer MaxPool__1\n",
      "Forward for layer Conv2D__1\n",
      "(1, 5, 5, 12)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (1, 1, 1, 100)\n",
      "Forward for layer Flatten__1\n",
      "Forward for layer Dense__1\n",
      "A_prev_layer shape: (1, 100)\n",
      "Forward for layer Softmax__1\n",
      "softmax tensor shape: (1, 10)\n",
      "softmax exp shape (1, 10)\n",
      "softmax sum shape: (1, 1)\n",
      "final input shape (1, 10)\n",
      "final output: [[0.09049115 0.06765897 0.1529529  0.08609433 0.04750961 0.10952845\n",
      "  0.10272049 0.08826287 0.12400152 0.13077972]]\n",
      "Loss is :  -6.485603025596568\n",
      "Progress 100.0%\n",
      "\n",
      "Cost After a Epoch 4: -648.5603025596567\n",
      "Running Epoch: 4\n",
      "Forward for layer Conv2D__1\n",
      "(1, 28, 28, 1)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (1, 28, 28, 6)\n",
      "Forward for layer MaxPool__1\n",
      "Forward for layer Conv2D__1\n",
      "(1, 14, 14, 6)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (1, 10, 10, 12)\n",
      "Forward for layer MaxPool__1\n",
      "Forward for layer Conv2D__1\n",
      "(1, 5, 5, 12)\n",
      "Forward for layer ReLU__1\n",
      "Relu Input Tensor Shape:  (1, 1, 1, 100)\n",
      "Forward for layer Flatten__1\n",
      "Forward for layer Dense__1\n",
      "A_prev_layer shape: (1, 100)\n",
      "Forward for layer Softmax__1\n",
      "softmax tensor shape: (1, 10)\n",
      "softmax exp shape (1, 10)\n",
      "softmax sum shape: (1, 1)\n",
      "final input shape (1, 10)\n",
      "final output: [[0.09049115 0.06765897 0.1529529  0.08609433 0.04750961 0.10952845\n",
      "  0.10272049 0.08826287 0.12400152 0.13077972]]\n",
      "Loss is :  -6.485603025596568\n",
      "Progress 100.0%\n",
      "\n",
      "Cost After a Epoch 5: -648.5603025596567\n",
      "Finish Training!\n"
     ]
    }
   ],
   "source": [
    "cnn.train(training_data=(X, Y), validation_data=(None, None),mini_batch_size=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conv_forward' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_102397/2488773407.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      7\u001B[0m hparameters = {\"pad\" : 2,\n\u001B[1;32m      8\u001B[0m                \"stride\": 2}\n\u001B[0;32m----> 9\u001B[0;31m \u001B[0mZ\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcache_conv\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mconv_forward\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mA_prev\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mW\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mb\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mhparameters\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     10\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;31m# Test conv_backward\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'conv_forward' is not defined"
     ]
    }
   ],
   "source": [
    "# We'll run conv_forward to initialize the 'Z' and 'cache_conv\",\n",
    "# which we'll use to test the conv_backward function\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10,4,4,3)\n",
    "W = np.random.randn(2,2,3,8)\n",
    "b = np.random.randn(1,1,1,8)\n",
    "hparameters = {\"pad\" : 2,\n",
    "               \"stride\": 2}\n",
    "Z, cache_conv = conv_forward(A_prev, W, b, hparameters)\n",
    "\n",
    "# Test conv_backward\n",
    "dA, dW, db = conv_backward_(Z, cache_conv)\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(10,4,4,3) # h, w, c of previous layer\n",
    "cnn = Convolution2D(num_out_channel=8, filter_size=2, stride=2, padding_size=2)\n",
    "cnn.initialize_output_dimensions((A_prev.shape[1], A_prev.shape[2], A_prev.shape[3]))\n",
    "cnn.initialize_weights_biases()\n",
    "cnn.forward(A_prev)\n",
    "Z = cnn.get_output_tensor()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Test conv_backward\n",
    "dA, dW, db = cnn.backward(Z)\n",
    "print(\"dA_mean =\", np.mean(dA))\n",
    "print(\"dW_mean =\", np.mean(dW))\n",
    "print(\"db_mean =\", np.mean(db))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Case 1: stride of 1\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "maxpool = MaxPool(filter_size=2, stride=1)\n",
    "maxpool.initialize_max_pool_params((A_prev.shape[1],A_prev.shape[2],A_prev.shape[3]))\n",
    "maxpool.forward(A_prev)\n",
    "A = maxpool.get_output_tensor()\n",
    "print('shape of A',A.shape)\n",
    "\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "print(dA[0,0,0,0])\n",
    "out = maxpool.backward(dA)\n",
    "print(\"mode = max\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', out[1,1])\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Case 1: stride of 1\n",
    "np.random.seed(1)\n",
    "A_prev = np.random.randn(5, 5, 3, 2)\n",
    "maxpool = MaxPool(filter_size=2, stride=1)\n",
    "maxpool.initialize_max_pool_params((A_prev.shape[1],A_prev.shape[2],A_prev.shape[3]))\n",
    "maxpool.forward(A_prev)\n",
    "A = maxpool.get_output_tensor()\n",
    "print('shape of A',A.shape)\n",
    "\n",
    "dA = np.random.randn(5, 4, 2, 2)\n",
    "print(dA[0,0,0,0])\n",
    "out = maxpool.backward_(dA)\n",
    "\n",
    "print(out.shape)\n",
    "print(\"mode = max\")\n",
    "print('mean of dA = ', np.mean(dA))\n",
    "print('dA_prev[1,1] = ', out[1,1])\n",
    "print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}